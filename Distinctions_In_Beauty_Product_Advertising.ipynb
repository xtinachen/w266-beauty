{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistinctionsInBeautyProductAdvertising.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xtinachen/w266-beauty/blob/main/Distinctions_In_Beauty_Product_Advertising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQgqN95qhnhY"
      },
      "source": [
        "# Distinctions in Beauty Product Advertising \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtvKkW-lCwOg"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek56pScfDjPo"
      },
      "source": [
        "# All of the data is in a shared Google Drive folder. Mount drive for access. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd2GrAd9S4g5"
      },
      "source": [
        "# Install textstat to analyze text complexity.\n",
        "!pip install textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGskEMjVeqkj"
      },
      "source": [
        "# Install Transformers for Embeddings.\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvzNqc7gLpYU"
      },
      "source": [
        "# General Loading and Processing.\n",
        "from google.colab import drive\n",
        "import pandas as pd \n",
        "import spacy\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from scipy.stats import zscore\n",
        "from scipy import stats\n",
        "import random\n",
        "\n",
        "# Text Processing.\n",
        "import textstat\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Modeling.\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "import transformers as transformer\n",
        "import torch\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "from tqdm import tqdm\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c2vEmycDQbG"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C5xBlxGawes"
      },
      "source": [
        "# Truncate size of output.\n",
        "pd.set_option('max_colwidth', 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7_8qGo_4TU5"
      },
      "source": [
        "### Lotion Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZgOIU3FDndb"
      },
      "source": [
        "lotions_path = '/content/gdrive/MyDrive/266/final/data/hair_scraped_latest_cl.csv'\n",
        "lotion_df = pd.read_csv(lotions_path) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oGMnUKz4VWH"
      },
      "source": [
        "### Hair Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrDI9nG4UwV"
      },
      "source": [
        "hair_path = '/content/gdrive/MyDrive/266/final/data/hair_scraped_latest_cl.csv'\n",
        "hair_df = pd.read_csv(hair_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guQUtk0L8Ke1"
      },
      "source": [
        "### All Junglescout Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A7IZZJB8GyY"
      },
      "source": [
        "raw_junglescout_path = '/content/gdrive/MyDrive/266/final/data/junglescout/all_raw_junglescout_categorized.csv'\n",
        "all_products_df = pd.read_csv(raw_junglescout_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3avrjozltSU"
      },
      "source": [
        "## Build a Corpus of Marketing Terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S15K9zijmMU1"
      },
      "source": [
        "For extracting marketing terms, we tokenized titles and tagged them with their parts of speech using the nltk word_tokenize and pos_tag functions. After extracting all adjectives, we filtered out what we felt were purely marketing terms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRdyyDGAmCO4"
      },
      "source": [
        "adjective_tags = [\"JJ\", \"JJR\", \"JJS\"] \n",
        "adjectives = set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5x-qcZDmFJn"
      },
      "source": [
        "# Tokenize data and extract adjectives. \n",
        "for product_title in lotion_df[\"product_title_cl\"]:\n",
        "  words_and_tags = nltk.pos_tag(word_tokenize(product_title))\n",
        "  for word, tag in words_and_tags:\n",
        "    if tag in adjective_tags:\n",
        "      adjectives.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouRUih2ikdMK"
      },
      "source": [
        "# Lotion Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YNxyiaQMDPo"
      },
      "source": [
        "## Data Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoZOpOCVMCfK"
      },
      "source": [
        "# Reduce dataset to only columns we're interested in. \n",
        "lotion_df = lotion_df[[\"product_id\", \"product_title_cl\", 'title_cl', 'about_cl', 'description_cl', \"price_cl\", 'price_unit', 'ingredients', \"category\", \"full_text\", \"title\"]]\n",
        "# Remove all products where the unit price is NAN\n",
        "lotion_df = lotion_df[lotion_df['price_unit'].notna()]\n",
        "# Reset the index to ensure linearity. \n",
        "lotion_df = lotion_df.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qex40AXWmroa",
        "outputId": "d0796d58-a21c-4c0c-e6ed-7d344fda8f26"
      },
      "source": [
        "lotion_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2487, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUqQhFh5gMJO"
      },
      "source": [
        "Remove Outliers: [Source](https://www.kite.com/python/answers/how-to-remove-outliers-from-a-pandas-dataframe-in-python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RANiRL4kf2av"
      },
      "source": [
        "# Remove Outliers \n",
        "z_scores = stats.zscore(lotion_df[\"price_unit\"].values)\n",
        "abs_z_scores = np.abs(z_scores).reshape(-1,1)\n",
        "filtered_entries = (abs_z_scores < 1.7).all(axis=1)\n",
        "lotion_df = lotion_df[filtered_entries]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktbmpOH0MTgh",
        "outputId": "db07f976-b1f5-472c-f949-c86525dbfbd7"
      },
      "source": [
        "# Number of unique product_ids.  \n",
        "len(lotion_df.product_id.unique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqdoZgZAMT83",
        "outputId": "ce8a3b5f-0aee-4ad5-c57e-961b38e8e2e7"
      },
      "source": [
        "# Examine the sub-categories in the lotion dataset. \n",
        "lotion_df[\"category\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shampoo        1496\n",
              "conditioner     968\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "PPGGLAMsMiTm",
        "outputId": "0ef1097a-444a-460b-abc2-8d314d792023"
      },
      "source": [
        "# Examine the distribution of price per unit (ounces).\n",
        "plt.hist(lotion_df['price_unit'], range=[0,40])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yizcVVo_Mszk",
        "outputId": "a547100d-5f73-4b64-be34-48b92ba54ef1"
      },
      "source": [
        "lotion_df[\"price_unit\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2464.000000\n",
              "mean        2.489824\n",
              "std         2.857140\n",
              "min         0.017240\n",
              "25%         1.081667\n",
              "50%         1.720714\n",
              "75%         2.833885\n",
              "max        32.950000\n",
              "Name: price_unit, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfdDt_U8MzaD",
        "outputId": "9febe9ce-6762-47b7-d720-d966b40d3e7a"
      },
      "source": [
        "# Value to split products into lower and higher priced lotion products\n",
        "cutoff_lotion = lotion_df[\"price_unit\"].median()\n",
        "cutoff_lotion"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7207142857142856"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlnjlNqAk34t"
      },
      "source": [
        "## Freedman and Jurafsky Analysis (Scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pb8cjs-Cvnw"
      },
      "source": [
        "### Falutin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Wle7D1F2_z"
      },
      "source": [
        "Flesch Reading Score and Grade Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlDg-c7sDmjb"
      },
      "source": [
        "# The flesch kincaid grade indicates the grade that a student would be able to read\n",
        "# the text. For example, a 9.3 means a ninth grader could read the text. \n",
        "lotion_df[\"flesch_kincaid_grade\"] = lotion_df[\"full_text\"].apply(textstat.flesch_kincaid_grade)\n",
        "\n",
        "# The flesch read ease score has no lower bound, but has an upper bound of 121.22. \n",
        "# The lower the score is, the harder the text is to read. \n",
        "lotion_df[\"reading_ease\"] = lotion_df[\"full_text\"].apply(textstat.flesch_reading_ease)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGTpiKXgF-wT"
      },
      "source": [
        "Number of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_-YSIgzF83S"
      },
      "source": [
        "# Count the number of words present in the text (does not include punctuation by default).\n",
        "lotion_df[\"lexicon_count\"] = lotion_df[\"full_text\"].apply(textstat.lexicon_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX_0YRXKGc8j"
      },
      "source": [
        "Word Commonality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXQZj3x6G7L1"
      },
      "source": [
        "# Raw marketing terms (hand curated).\n",
        "marketing_term_corpus = [\"absorbing\",\"active\",\"advanced\",\"all natural\",\"aloe\",\"aluminum\",\"anti aging\",\n",
        "     \"antioxidant\",\"antiwrinkle\",\"attractant\",\"authentic\",\"best\",\"botanic\",\"bpa\",\n",
        "     \"certified\",\"classic\",\"clean\",\"cleanser\",\"clear\",\"clinically\",\"coconut\",\n",
        "     \"collagen\",\"comedogenic\",\"cosmeceutical\",\"creamy\",\"curvy\",\"deep\",\"dermatologist\",\n",
        "     \"double\",\"dry\",\"dual\",\"elite\",\"essential\",\"essential oil\",\"esthetic\",\"extra\",\n",
        "     \"facial\",\"fair\",\"fda\",\"finish\",\"fragrance\",\"fragrant\",\"free\",\"fullest\",\"gentle\",\n",
        "     \"glow\",\"gluten\",\"glycolic\",\"green\",\"healthiest\",\"healthy\",\"herbal\",\"hygienic\",\n",
        "     \"hypoallergenic\",\"iluminate\",\"incredible\",\"intense\",\"intensive\",\"keeper\",\"kosher\",\n",
        "     \"lavish\",\"light\",\"liquid\",\"lucious\",\"lux\",\"maximum\",\"microbiotic\",\"miracle\",\n",
        "     \"multifunctional\",\"natural\",\"neutral\",\"nongreasy\",\"norwegian\",\"odorfree\",\"oilcontrol\",\n",
        "     \"oilfree\",\"olive\",\"organic\",\"original\",\"pabafree\",\"paraben\",\"parabenfree\",\n",
        "     \"phytoactive\",\"poppy\",\"premium\",\"protective\",\"pure\",\"purest\",\"purpose\",\"rapid\",\n",
        "     \"raw\",\"resistant\",\"restorative\",\"rich\",\"royale\",\"safe\",\"select\",\"sensitive\",\n",
        "     \"shave\",\"shea\",\"sheamoisture\",\"silicon\",\"silicone\",\"silk\",\"skin\",\"smart\",\"soft\",\n",
        "     \"solar\",\"soothe\",\"soothing\",\"spotlight\",\"stress\",\"sublime\",\"suntan\",\"tan\",\n",
        "     \"texturized\",\"therapy\",\"touch\",\"transformative\",\"tropical\",\"ultimate\",\"unisex\",\n",
        "     \"unprocessed\",\"uplift\",\"urban\",\"uv\",\"uva\",\"uvb\",\"vegan\",\"virgin\",\"vitae\",\n",
        "     \"vitamin\",\"youthful\",\"zen\"]\n",
        "\n",
        "# Lemmatized.\n",
        "marketing_term_corpus_cl = ['absorb', 'activ', 'advanc', 'natur', 'alo', 'aluminum', \n",
        "                            'anti age', 'antioxid', 'antiwrinkl', 'attract', 'authent', \n",
        "                            'best', 'botan', 'bpa', 'certifi', 'classic', 'clean', 'cleanser', \n",
        "                            'clear', 'clinic', 'coconut', 'collagen', 'comedogen', 'cosmeceut', \n",
        "                            'creami', 'curvi', 'deep', 'dermatologist', 'doubl', 'dri', 'dual', \n",
        "                            'elit', 'essenti', 'essenti oil', 'esthet', 'extra', 'facial', 'fair', \n",
        "                            'fda', 'finish', 'fragranc', 'fragrant', 'free', 'fullest', 'gentl', \n",
        "                            'glow', 'gluten', 'glycol', 'green', 'healthiest', 'healthi', 'herbal', \n",
        "                            'hygien', 'hypoallergen', 'ilumin', 'incred', 'intens', 'intens', \n",
        "                            'keeper', 'kosher', 'lavish', 'light', 'liquid', 'luciou', 'lux', \n",
        "                            'maximum', 'microbiot', 'miracl', 'multifunct', 'natur', 'neutral', \n",
        "                            'nongreasi', 'norwegian', 'odorfre', 'oilcontrol', 'oilfre', 'oliv', \n",
        "                            'organ', 'origin', 'pabafre', 'paraben', 'parabenfre', 'phytoact', \n",
        "                            'poppi', 'premium', 'protect', 'pure', 'purest', 'purpos', 'rapid', \n",
        "                            'raw', 'resist', 'restor', 'rich', 'royal', 'safe', 'select', \n",
        "                            'sensit', 'shave', 'shea', 'sheamoistur', 'silicon', 'silicon', \n",
        "                            'silk', 'skin', 'smart', 'soft', 'solar', 'sooth', 'sooth', \n",
        "                            'spotlight', 'stress', 'sublim', 'suntan', 'tan', 'textur', 'therapi', \n",
        "                            'touch', 'transform', 'tropic', 'ultim', 'unisex', 'unprocess', 'uplift', \n",
        "                            'urban', 'uv', 'uva', 'uvb', 'vegan', 'virgin', 'vita', 'vitamin', 'youth', 'zen']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBieaVlHHIJ_"
      },
      "source": [
        "# Count the occurances of marketing terms in the text. \n",
        "def count_marketing_terms(text):\n",
        "  count = 0\n",
        "  for word in text.split(\" \"):\n",
        "    if word in marketing_term_corpus_cl: \n",
        "      count += 1 \n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7TU77wsSi52"
      },
      "source": [
        "# Raw count of marketing terms present. \n",
        "lotion_df[\"marketing_term_count_raw\"] = lotion_df[\"full_text\"].apply(count_marketing_terms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "iusqycGbSqlC",
        "outputId": "b6bad0b7-aba3-4d5d-af26-695acf0a0366"
      },
      "source": [
        "# Examine falutin variables (as a whole).\n",
        "lotion_df[[\"flesch_kincaid_grade\",\"reading_ease\", \"lexicon_count\",\"marketing_term_count_raw\" ]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flesch_kincaid_grade</th>\n",
              "      <th>reading_ease</th>\n",
              "      <th>lexicon_count</th>\n",
              "      <th>marketing_term_count_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2464.000000</td>\n",
              "      <td>2464.000000</td>\n",
              "      <td>2464.000000</td>\n",
              "      <td>2464.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>33.324107</td>\n",
              "      <td>-2.362658</td>\n",
              "      <td>79.426136</td>\n",
              "      <td>8.640828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>27.288936</td>\n",
              "      <td>73.259133</td>\n",
              "      <td>69.118892</td>\n",
              "      <td>8.756718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>-634.630000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>15.800000</td>\n",
              "      <td>-23.260000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>25.100000</td>\n",
              "      <td>17.340000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>40.400000</td>\n",
              "      <td>45.090000</td>\n",
              "      <td>98.250000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>276.700000</td>\n",
              "      <td>104.640000</td>\n",
              "      <td>704.000000</td>\n",
              "      <td>83.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       flesch_kincaid_grade  ...  marketing_term_count_raw\n",
              "count           2464.000000  ...               2464.000000\n",
              "mean              33.324107  ...                  8.640828\n",
              "std               27.288936  ...                  8.756718\n",
              "min                0.900000  ...                  0.000000\n",
              "25%               15.800000  ...                  3.000000\n",
              "50%               25.100000  ...                  6.000000\n",
              "75%               40.400000  ...                 12.000000\n",
              "max              276.700000  ...                 83.000000\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkHI15A7TIXW"
      },
      "source": [
        "# Split dataset based on the median of the price. \n",
        "lower_priced_products = lotion_df[lotion_df['price_unit'] <= cutoff_lotion]\n",
        "higher_priced_products = lotion_df[lotion_df['price_unit'] > cutoff_lotion]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Nj_omcQkTvcH",
        "outputId": "8421eca1-77ae-42c5-bba4-902e8433b0cc"
      },
      "source": [
        "# Examine distribution of falutin variables for lower priced products. \n",
        "lower_priced_products[[\"flesch_kincaid_grade\",\"reading_ease\", \"lexicon_count\",\"marketing_term_count_raw\" ]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flesch_kincaid_grade</th>\n",
              "      <th>reading_ease</th>\n",
              "      <th>lexicon_count</th>\n",
              "      <th>marketing_term_count_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>34.793344</td>\n",
              "      <td>-6.378782</td>\n",
              "      <td>83.088474</td>\n",
              "      <td>9.103896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>27.358890</td>\n",
              "      <td>73.396034</td>\n",
              "      <td>69.307268</td>\n",
              "      <td>8.947300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.900000</td>\n",
              "      <td>-379.870000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>16.600000</td>\n",
              "      <td>-28.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>26.300000</td>\n",
              "      <td>14.980000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>42.300000</td>\n",
              "      <td>41.370000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>178.800000</td>\n",
              "      <td>97.540000</td>\n",
              "      <td>453.000000</td>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       flesch_kincaid_grade  ...  marketing_term_count_raw\n",
              "count           1232.000000  ...               1232.000000\n",
              "mean              34.793344  ...                  9.103896\n",
              "std               27.358890  ...                  8.947300\n",
              "min                2.900000  ...                  0.000000\n",
              "25%               16.600000  ...                  3.000000\n",
              "50%               26.300000  ...                  7.000000\n",
              "75%               42.300000  ...                 12.000000\n",
              "max              178.800000  ...                 55.000000\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "GfSVnOi_TyeE",
        "outputId": "1f224d5f-dbba-4395-c307-7e272cdfba87"
      },
      "source": [
        "# Examine distribution of falutin variabels for higher priced products. \n",
        "higher_priced_products[[\"flesch_kincaid_grade\",\"reading_ease\", \"lexicon_count\",\"marketing_term_count_raw\" ]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flesch_kincaid_grade</th>\n",
              "      <th>reading_ease</th>\n",
              "      <th>lexicon_count</th>\n",
              "      <th>marketing_term_count_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>31.854870</td>\n",
              "      <td>1.653466</td>\n",
              "      <td>75.763799</td>\n",
              "      <td>8.177760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>27.150457</td>\n",
              "      <td>72.930778</td>\n",
              "      <td>68.763208</td>\n",
              "      <td>8.540441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>-634.630000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>14.600000</td>\n",
              "      <td>-18.870000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>23.600000</td>\n",
              "      <td>20.050000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>5.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>38.875000</td>\n",
              "      <td>48.132500</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>11.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>276.700000</td>\n",
              "      <td>104.640000</td>\n",
              "      <td>704.000000</td>\n",
              "      <td>83.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       flesch_kincaid_grade  ...  marketing_term_count_raw\n",
              "count           1232.000000  ...               1232.000000\n",
              "mean              31.854870  ...                  8.177760\n",
              "std               27.150457  ...                  8.540441\n",
              "min                0.900000  ...                  0.000000\n",
              "25%               14.600000  ...                  2.000000\n",
              "50%               23.600000  ...                  5.500000\n",
              "75%               38.875000  ...                 11.000000\n",
              "max              276.700000  ...                 83.000000\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75cDW4coCzxW"
      },
      "source": [
        "### Distinction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKxQM43oweoD"
      },
      "source": [
        "#### Unique and Comparator Words: Baseline\n",
        "\n",
        "As a baseline anlaysis, we hand-curated a list of words that indicate \"uniqueness\". This feature will just indicate the number of unique words present in the title."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa8IuGWgm0gN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76172369-88d5-4acf-ea28-80374b0864b3"
      },
      "source": [
        "unique_and_comparator_words = [\"unique\", \"different\", \"most\", \"distinct\", \"special\", \"single\", \"only\", \"new\", 'premium', 'ultra',\"more\", \"less\", \"most\", \"better\", \"lower\", \"higher\", \"far\", \"farther\", \"little\", \"simpler\", \"least\"]\n",
        "# most and only got removed due to stop words, add back in? \n",
        "unique_and_comparator_words_cl = ['uniqu', 'differ', 'distinct', 'special', 'singl', 'new', 'premium', 'ultra','less', 'better', 'lower', 'higher', 'far', 'farther', 'littl', 'simpler', 'least'] \n",
        "def count_unique_and_comparator_words(text):\n",
        "  count = 0\n",
        "  for word in unique_and_comparator_words_cl:\n",
        "    if word in text.lower().split(' '):\n",
        "      count += 1\n",
        "  return count \n",
        "\n",
        "# lotion_df[\"unique_word_count_basic\"] = lotion_df[\"product_title_cl\"].apply(count_unique_and_comparator_words)\n",
        "lotion_df[\"unique_word_count_basic\"] = lotion_df[\"full_text\"].apply(count_unique_and_comparator_words)\n",
        "lotion_df[\"unique_word_count_basic\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2464.000000\n",
              "mean        0.313718\n",
              "std         0.614646\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max         5.000000\n",
              "Name: unique_word_count_basic, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smfzJm01wpX8"
      },
      "source": [
        "This method provided next to no signal. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB-H6Strvl4b"
      },
      "source": [
        "#### Unique and Comparator Words: Improvement\n",
        "\n",
        "We tried two different things to improve this signal. \n",
        "\n",
        "**1. Unigrams, Bigrams, Trigrams Using a Custom Word2Vec Model**\n",
        "\n",
        "First, we extracted unigrams, bigrams and trigrams. We combined the bigrams and trigrams by underscores to create unique unigrams capturing the two or three words. Then we created our own embeddings using a custom Word2Vec model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COdulyx0ozpY"
      },
      "source": [
        "# Get unigrams, bigrams and trigrams of product title.\n",
        "def connect_by_underscore(ngram_list):\n",
        "  new_list = []\n",
        "  for ngram in ngram_list:\n",
        "    new_list.append(\"_\".join(ngram))\n",
        "  return new_list\n",
        "    \n",
        "# Extract unigrams, bigrams and trigrams. \n",
        "# Think we should remove prod from unigram/bigram/trigram.\n",
        "lotion_df[\"unigrams\"] = lotion_df[\"full_text\"].apply(lambda x: np.array(str.split(x)))\n",
        "lotion_df[\"bigrams\"] = lotion_df[\"unigrams\"].apply(nltk.bigrams).apply(list).apply(connect_by_underscore)\n",
        "lotion_df[\"trigrams\"] = lotion_df[\"unigrams\"].apply(nltk.trigrams).apply(list).apply(connect_by_underscore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYOLR_Z-BTjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e5b8eb-277d-4d7b-e3e9-c601d0b2046b"
      },
      "source": [
        "# Examine Unigrams\n",
        "lotion_df[\"unigrams\"][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [silicon, mix, silicon, mix, bambu, extract, nutrit, prod, oz, ounc, nutrit, prod, bamboo, extract, regener, nourish, vitamin, e, c, f, carthamu, oil, almond, oil, improv, health, beauti, hair, hair, type, includ, natur, process, hair, follow, silicon, mix, bambu, treatment, good, result, work, great, brittl, dull, hair, free, residu, improv, silicon, mix, bambu, treatment, ', s, effici]\n",
              "1                                                                                                                                                                   [clay, esth, prod, ex, oz, refil, improv, health, scalp, hair, keep, scalp, healthi, remov, dirt, excess, oil, sweat, caus, clog, pore, increa, moistur, soft, loosen, tension, caus, scalp, relax, leav, hair, moistur, soft, healthi]\n",
              "2                                                                                                                      [healthi, sexi, hair, triwheat, leav, prod, oz, power, combin, soy, cocoa, work, synergist, perfectli, care, hair, prod, nourish, moistur, mild, protein, reconstructor, safe, use, colortr, hair, truli, amaz, prod, reconstruct, moistur, combat, environment, stress, type, hair]\n",
              "3                                                                                                                [rainbow, research, henna, hair, color, prod, persian, copper, oz, red, vendor, rainbow, research, rainbow, research, henna, hair, color, prod, persian, copper, red, copper, oz, make, we, color, persian, copper, rainbow, research, henna, hair, color, prod, persian, copper, oz, red]\n",
              "4                                                                                                                                                                                             [avalon, organ, strengthen, peppermint, prod, prod, set, ounc, wheat, protein, alo, vitamin, rosemari, essenti, oil, strengthen, thicken, thin, limp, lifeless, hair, creat, full, volum, healthylook, shine]\n",
              "Name: unigrams, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DLMEbEM726Z",
        "outputId": "7e83c9f9-7e80-462f-cc55-9f8fc3403a8e"
      },
      "source": [
        "# Create a single vocabulary out of all unigrams, bigrams and trigrams.\n",
        "vocab_unigrams_bigrams_trigrams = list(itertools.chain.from_iterable(list([lotion_df[\"unigrams\"].values,lotion_df[\"bigrams\"].values,lotion_df[\"trigrams\"].values])))\n",
        "vocab_unigrams_bigrams_trigrams = list(itertools.chain.from_iterable(vocab_unigrams_bigrams_trigrams))\n",
        "print(vocab_unigrams_bigrams_trigrams[-5:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['free_prod_tec', 'prod_tec_itali', 'tec_itali_unisex', 'itali_unisex_oz', 'unisex_oz_shampoo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl6QHGIOxh7j"
      },
      "source": [
        "# Create word vectors for the vocabulary using Word2Vec.\n",
        "custom_model = gensim.models.Word2Vec([vocab_unigrams_bigrams_trigrams])\n",
        "custom_model.save('custom.embedding')\n",
        "custom_model = gensim.models.Word2Vec.load('custom.embedding')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcT58F-5PrUc"
      },
      "source": [
        "# base_word_list: terms to match to (ex. unigue_words_cl).\n",
        "# model: Word2Vec model.\n",
        "def extract_similar_words(base_word_list, model):\n",
        "  counts = {product_id: 0 for product_id in lotion_df[\"product_id\"]}\n",
        "  other_unique_and_comparator_words = set()\n",
        "  # Guarantee base words are in the models vocabulary.\n",
        "  official_base_word_list = [word for word in base_word_list if word in model.wv.vocab]\n",
        "  # For each word in the base_word_list, get the word vector.\n",
        "  for base_word in official_base_word_list:\n",
        "    # For each product_title.\n",
        "    for product_title, product_id in zip(lotion_df[\"full_text\"], lotion_df[\"product_id\"]): \n",
        "      # For each word.\n",
        "      for word in str(product_title).split(\" \"):\n",
        "        # Check if its in the dictionary (to avoid key errors).\n",
        "        if word in model.wv.vocab:\n",
        "          # If word in vocab and word_vector similarity above x, add word to list \n",
        "          # and increase count of similar words.\n",
        "          if model.similarity(base_word,word) >= 0.5:\n",
        "            other_unique_and_comparator_words.add(word)\n",
        "            counts[product_id] += 1\n",
        "\n",
        "\n",
        "  return counts, other_unique_and_comparator_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esoLPLa7yOku",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "b0506b1c-da4d-4fed-8bd0-817b1e1a3aa0"
      },
      "source": [
        "# Find the list and count of marketing terms in the product text that has \n",
        "# a high cosine similarity to the unique/custom word list. \n",
        "counts_custom, other_unique_and_comparator_words_custom = extract_similar_words(unique_and_comparator_words_cl, custom_model)\n",
        "plt.hist(counts_custom.values())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh40_4mjy73O"
      },
      "source": [
        "# Merge the counts back into the main df using a join on product_id.\n",
        "lotion_df = lotion_df.merge(pd.DataFrame(list(counts_custom.items()),columns = ['product_id','mktg_embedding_counts_custom']), how=\"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT1bSzdWzQB-",
        "outputId": "45344eaa-5738-4984-cd00-60c59f890371"
      },
      "source": [
        "# Examine the distribution of the number of marketing terms present \n",
        "# (based on cosine similarity).\n",
        "lotion_df['mktg_embedding_counts_custom'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2464.000000\n",
              "mean        0.409497\n",
              "std         0.896562\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max        12.000000\n",
              "Name: mktg_embedding_counts_custom, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDhuzgM6yzhR",
        "outputId": "819fcd41-4c26-40fa-9020-47610cf9e40d"
      },
      "source": [
        "# Examine words in the text that had a high cosine similarity to the unique/comparator word list.\n",
        "list(other_unique_and_comparator_words_custom)[:min(len(other_unique_and_comparator_words_custom), 25)] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['premium',\n",
              " 'littl',\n",
              " 'new',\n",
              " 'ultra',\n",
              " 'differ',\n",
              " 'singl',\n",
              " 'less',\n",
              " 'least',\n",
              " 'special',\n",
              " 'far',\n",
              " 'uniqu']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yTHlfl7PiJD"
      },
      "source": [
        "# Split products based on the median price to analyze any significant differences \n",
        "# in the distribution of the number marketing terms present. \n",
        "lower_priced_products = lotion_df[lotion_df['price_unit'] <= cutoff_lotion]\n",
        "higher_priced_products = lotion_df[lotion_df['price_unit'] > cutoff_lotion]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJHqJmUbQdNn",
        "outputId": "57744762-a9d7-40af-d94d-90bf781a95a6"
      },
      "source": [
        "# Examine distribution of the number of marketing terms in the text for lower \n",
        "# priced products.\n",
        "lower_priced_products[\"mktg_embedding_counts_custom\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1232.000000\n",
              "mean        0.371753\n",
              "std         0.822973\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max         7.000000\n",
              "Name: mktg_embedding_counts_custom, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "t7AcLOnGQMHM",
        "outputId": "ee9a6a23-37c8-4de6-94b7-47ec08212d1b"
      },
      "source": [
        "plt.hist(lower_priced_products[\"mktg_embedding_counts_custom\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XieD0SHdQnX-",
        "outputId": "75645046-4ac4-4ecd-9286-260477ccca99"
      },
      "source": [
        "# Examine distribution of the number of marketing terms in the text for higher \n",
        "# priced products. \n",
        "higher_priced_products[\"mktg_embedding_counts_custom\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1232.000000\n",
              "mean        0.447240\n",
              "std         0.963413\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         1.000000\n",
              "max        12.000000\n",
              "Name: mktg_embedding_counts_custom, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "E7BdtG5uQYoN",
        "outputId": "4b876b8e-594d-43cd-9ed6-7e6e71c14e71"
      },
      "source": [
        "plt.hist(higher_priced_products[\"mktg_embedding_counts_custom\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOreDOW2O_2"
      },
      "source": [
        "**2. Pre-trained Glove Embeddings on Unigrams**\n",
        "\n",
        "The next thing we tried to do to improve on the baseline was to use only unigrams and pre-trained word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU1dGcKpxy5V",
        "outputId": "69e4781b-a846-4704-fc80-b811c69e1b93"
      },
      "source": [
        "# Only use unigrams, since bigrams and trigrams joined on '_' won't be in vocab. \n",
        "vocab_unigrams = list(itertools.chain.from_iterable(lotion_df[\"unigrams\"]))\n",
        "print(vocab_unigrams[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['silicon', 'mix', 'silicon', 'mix', 'bambu', 'extract', 'nutrit', 'prod', 'oz', 'ounc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j1gXc6ctw41"
      },
      "source": [
        "# Download pre-trained word vectors.\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSsyn-NP0drq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2eb3e2-1b8e-4db9-dcb4-2f34965a2ded"
      },
      "source": [
        "# Extract words and the count of those words in each text using cosine similarity to\n",
        "# the hand curated unique/comparator word list. \n",
        "counts_glove, other_unique_and_comparator_words_glove = extract_similar_words(unique_and_comparator_words_cl, glove_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrmKgUyiqY--"
      },
      "source": [
        "# Merge the counts back into the main df using a join on product_id.\n",
        "lotion_df = lotion_df.merge(pd.DataFrame(list(counts_glove.items()),columns = ['product_id','mktg_embedding_counts_glov']), how=\"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqUlcC3eqY_A",
        "outputId": "6b2c6d39-dcc6-462a-fbc2-ab986cc4aa90"
      },
      "source": [
        "# Examine the distribution of the number of marketing terms present \n",
        "# (based on cosine similarity).\n",
        "lotion_df['mktg_embedding_counts_glov'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2464.000000\n",
              "mean        9.353896\n",
              "std        12.983349\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         5.000000\n",
              "75%        12.000000\n",
              "max       108.000000\n",
              "Name: mktg_embedding_counts_glov, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i06znA_AqY_B",
        "outputId": "76935317-f055-45b9-ba14-41a09364e7bb"
      },
      "source": [
        "# Examine words in the text that had a high cosine similarity to the unique/comparator word list.\n",
        "list(other_unique_and_comparator_words_glove)[:min(len(other_unique_and_comparator_words_glove), 25)] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lot',\n",
              " 'kind',\n",
              " 'two',\n",
              " 'around',\n",
              " 'half',\n",
              " 'five',\n",
              " 'differ',\n",
              " 'can',\n",
              " 'sleeker',\n",
              " 'special',\n",
              " 'help',\n",
              " 'play',\n",
              " 'job',\n",
              " 'so',\n",
              " 'bring',\n",
              " 'dead',\n",
              " 'current',\n",
              " 'go',\n",
              " 'last',\n",
              " 'million',\n",
              " 'rather',\n",
              " 'nine',\n",
              " 'give',\n",
              " 'often',\n",
              " 'come']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M69E5_ipqY_C"
      },
      "source": [
        "# Split products based on the median price to analyze any significant differences \n",
        "# in the distribution of the number marketing terms present. \n",
        "lower_priced_products = lotion_df[lotion_df['price_unit'] <= cutoff_lotion]\n",
        "higher_priced_products = lotion_df[lotion_df['price_unit'] > cutoff_lotion]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSfrraU7qY_C",
        "outputId": "50dfcb62-84d0-438b-906f-fbd4785bc853"
      },
      "source": [
        "# Examine distribution of the number of marketing terms in the text for lower \n",
        "# priced products.\n",
        "lower_priced_products[\"mktg_embedding_counts_glov\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1232.000000\n",
              "mean        9.920455\n",
              "std        13.303927\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         5.000000\n",
              "75%        12.000000\n",
              "max       102.000000\n",
              "Name: mktg_embedding_counts_glov, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "ljWQjq9EqY_C",
        "outputId": "5c076832-9f74-4106-9d9e-b09eee244ff4"
      },
      "source": [
        "plt.hist(lower_priced_products[\"mktg_embedding_counts_glov\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vst6K2kqY_D",
        "outputId": "8e077b61-8304-435e-a0fa-5230d729c194"
      },
      "source": [
        "# Examine distribution of the number of marketing terms in the text for higher \n",
        "# priced products. \n",
        "higher_priced_products[\"mktg_embedding_counts_glov\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1232.000000\n",
              "mean        8.787338\n",
              "std        12.634662\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         4.000000\n",
              "75%        12.000000\n",
              "max       108.000000\n",
              "Name: mktg_embedding_counts_glov, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "NcpFgsxWqY_D",
        "outputId": "058e755b-184a-4460-a3f6-03fe48f3eb35"
      },
      "source": [
        "plt.hist(higher_priced_products[\"mktg_embedding_counts_glov\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN4ElEQVR4nO3cb6ied33H8fdnja22QtM/odQk7GQYlCK4luAqHTJaB7YV0wcqHTKDBPKkm9UKGrcHsmctiFVhFEqji0P8s1jWoOJwaWXsgZmJSm0bXWOtJiG1R9dWp4gWv3tw/4LHmNNzTs45Ob2/e7/g5lz/7nP9rl7h3fv8zn3uVBWSpF7+aK0HIElaecZdkhoy7pLUkHGXpIaMuyQ1tG6tBwBw+eWX18zMzFoPQ5KmyuHDh39SVRvOtO9FEfeZmRkOHTq01sOQpKmS5Ifz7XNaRpIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhp6UfyF6nLM7P7Smp37yTtvXrNzS9IL8ZW7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqaFFxT3Je5M8muSRJJ9J8tIkW5IcTHI0yeeSnD+OvWCsHx37Z1bzAiRJf2jBuCfZCLwb2FZVrwHOA24F7gLurqpXAs8AO8dTdgLPjO13j+MkSefQYqdl1gEvS7IOuBA4CVwP7Bv79wK3jOXtY52x/4YkWZnhSpIWY8G4V9UJ4MPAj5hE/TngMPBsVT0/DjsObBzLG4Fj47nPj+MvO/37JtmV5FCSQ7Ozs8u9DknSHIuZlrmEyavxLcArgIuANy33xFV1b1Vtq6ptGzZsWO63kyTNsZhpmTcCP6iq2ar6DXA/cB2wfkzTAGwCTozlE8BmgLH/YuCnKzpqSdILWkzcfwRcm+TCMXd+A/AY8BDw1nHMDuCBsbx/rDP2P1hVtXJDliQtZDFz7geZ/GL0m8B3xnPuBT4A3JHkKJM59T3jKXuAy8b2O4DdqzBuSdILWLfwIVBVHwI+dNrmJ4DXneHYXwFvW/7QJElny79QlaSGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJamhRcU+yPsm+JN9NciTJ65NcmuSrSR4fXy8ZxybJx5McTfJwkmtW9xIkSadb7Cv3jwFfqapXA68FjgC7gQNVtRU4MNYBbgS2jscu4J4VHbEkaUELxj3JxcAbgD0AVfXrqnoW2A7sHYftBW4Zy9uBT9XE14H1Sa5c8ZFLkua1mFfuW4BZ4JNJvpXkviQXAVdU1clxzFPAFWN5I3BszvOPj22/J8muJIeSHJqdnT37K5Ak/YHFxH0dcA1wT1VdDfyC303BAFBVBdRSTlxV91bVtqratmHDhqU8VZK0gMXE/ThwvKoOjvV9TGL/41PTLePr02P/CWDznOdvGtskSefIgnGvqqeAY0leNTbdADwG7Ad2jG07gAfG8n7gneNdM9cCz82ZvpEknQPrFnnc3wKfTnI+8ATwLib/Y/h8kp3AD4G3j2O/DNwEHAV+OY6VJJ1Di4p7VX0b2HaGXTec4dgCblvmuCRJy+BfqEpSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDW06LgnOS/Jt5J8caxvSXIwydEkn0ty/th+wVg/OvbPrM7QJUnzWcor99uBI3PW7wLurqpXAs8AO8f2ncAzY/vd4zhJ0jm0qLgn2QTcDNw31gNcD+wbh+wFbhnL28c6Y/8N43hJ0jmy2FfuHwXeD/x2rF8GPFtVz4/148DGsbwROAYw9j83jpcknSMLxj3Jm4Gnq+rwSp44ya4kh5Icmp2dXclvLUn/7y3mlft1wFuSPAl8lsl0zMeA9UnWjWM2ASfG8glgM8DYfzHw09O/aVXdW1Xbqmrbhg0blnURkqTft2Dcq+qDVbWpqmaAW4EHq+odwEPAW8dhO4AHxvL+sc7Y/2BV1YqOWpL0gpbzPvcPAHckOcpkTn3P2L4HuGxsvwPYvbwhSpKWat3Ch/xOVX0N+NpYfgJ43RmO+RXwthUYmyTpLPkXqpLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqaMG4J9mc5KEkjyV5NMntY/ulSb6a5PHx9ZKxPUk+nuRokoeTXLPaFyFJ+n2LeeX+PPC+qroKuBa4LclVwG7gQFVtBQ6MdYAbga3jsQu4Z8VHLUl6QQvGvapOVtU3x/LPgSPARmA7sHccthe4ZSxvBz5VE18H1ie5csVHLkma15Lm3JPMAFcDB4Erqurk2PUUcMVY3ggcm/O042Pb6d9rV5JDSQ7Nzs4ucdiSpBey6LgneTnwBeA9VfWzufuqqoBayomr6t6q2lZV2zZs2LCUp0qSFrBuMQcleQmTsH+6qu4fm3+c5MqqOjmmXZ4e208Am+c8fdPY1s7M7i+tyXmfvPPmNTmvpOmxmHfLBNgDHKmqj8zZtR/YMZZ3AA/M2f7O8a6Za4Hn5kzfSJLOgcW8cr8O+GvgO0m+Pbb9HXAn8PkkO4EfAm8f+74M3AQcBX4JvGtFRyxJWtCCca+q/wQyz+4bznB8Abctc1ySpGXwL1QlqSHjLkkNGXdJasi4S1JDxl2SGjLuktSQcZekhoy7JDVk3CWpIeMuSQ0Zd0lqyLhLUkPGXZIaMu6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ8Zdkhoy7pLUkHGXpIbWrfUAtHQzu7+0Zud+8s6b1+zckhbPV+6S1JBxl6SGjLskNWTcJakh4y5JDRl3SWrIuEtSQ77PXUuyVu+x9/310tL4yl2SGjLuktSQcZekhpxz11Rwrl9aGl+5S1JDqxL3JG9K8r0kR5PsXo1zSJLmt+LTMknOA/4R+EvgOPCNJPur6rGVPpe02vx4ZU2r1Zhzfx1wtKqeAEjyWWA7YNylJfD3DOdOx/+Jr0bcNwLH5qwfB/7s9IOS7AJ2jdX/TfK9szzf5cBPzvK506L7NXp9LyK566yeNlXXeBZW7frO8r/3KX883441e7dMVd0L3Lvc75PkUFVtW4EhvWh1v0avb/p1v8ZpvL7V+IXqCWDznPVNY5sk6RxZjbh/A9iaZEuS84Fbgf2rcB5J0jxWfFqmqp5P8jfAvwHnAZ+oqkdX+jxzLHtqZwp0v0avb/p1v8apu75U1VqPQZK0wvwLVUlqyLhLUkNTHfduH3OQZHOSh5I8luTRJLeP7Zcm+WqSx8fXS9Z6rMuR5Lwk30ryxbG+JcnBcR8/N34RP7WSrE+yL8l3kxxJ8vpO9zDJe8e/z0eSfCbJS6f9Hib5RJKnkzwyZ9sZ71kmPj6u9eEk16zdyOc3tXGf8zEHNwJXAX+V5Kq1HdWyPQ+8r6quAq4FbhvXtBs4UFVbgQNjfZrdDhyZs34XcHdVvRJ4Bti5JqNaOR8DvlJVrwZey+RaW9zDJBuBdwPbquo1TN40cSvTfw//CXjTadvmu2c3AlvHYxdwzzka45JMbdyZ8zEHVfVr4NTHHEytqjpZVd8cyz9nEoWNTK5r7zhsL3DL2oxw+ZJsAm4G7hvrAa4H9o1Dpv36LgbeAOwBqKpfV9WzNLqHTN5l97Ik64ALgZNM+T2sqv8A/ue0zfPds+3Ap2ri68D6JFeem5Eu3jTH/Uwfc7Bxjcay4pLMAFcDB4Erqurk2PUUcMUaDWslfBR4P/DbsX4Z8GxVPT/Wp/0+bgFmgU+Oqaf7klxEk3tYVSeADwM/YhL154DD9LqHp8x3z6aiPdMc97aSvBz4AvCeqvrZ3H01ee/qVL5/Ncmbgaer6vBaj2UVrQOuAe6pqquBX3DaFMyU38NLmLxy3QK8AriIP5zOaGca79k0x73lxxwkeQmTsH+6qu4fm3986se+8fXptRrfMl0HvCXJk0ym0a5nMj+9fvyID9N/H48Dx6vq4FjfxyT2Xe7hG4EfVNVsVf0GuJ/Jfe10D0+Z755NRXumOe7tPuZgzD/vAY5U1Ufm7NoP7BjLO4AHzvXYVkJVfbCqNlXVDJP79WBVvQN4CHjrOGxqrw+gqp4CjiV51dh0A5OPu25xD5lMx1yb5MLx7/XU9bW5h3PMd8/2A+8c75q5FnhuzvTNi0dVTe0DuAn4b+D7wN+v9XhW4Hr+nMmPfg8D3x6Pm5jMSx8AHgf+Hbh0rce6Atf6F8AXx/KfAP8FHAX+Bbhgrce3zGv7U+DQuI//ClzS6R4C/wB8F3gE+Gfggmm/h8BnmPwO4TdMfvraOd89A8LknXrfB77D5J1Da34Npz/8+AFJamiap2UkSfMw7pLUkHGXpIaMuyQ1ZNwlqSHjLkkNGXdJauj/AKLFNdBol4HzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWJ5pl1yQCDB"
      },
      "source": [
        "#### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcmn5V8GczwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a397f01b-60bb-4ecd-e527-c93b8e0c53e3"
      },
      "source": [
        "# Extract sentiment scores for each text using nltk SentimentIntensityAnalyzer.\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "sentiment_df = lotion_df[\"full_text\"].apply(sentiment_analyzer.polarity_scores).to_frame()\n",
        "sentiment_df  = sentiment_df[\"full_text\"].apply(pd.Series)\n",
        "# Add results to the main dataframe. \n",
        "lotion_df[\"pos\"] = sentiment_df[\"pos\"]\n",
        "lotion_df[\"neg\"] = sentiment_df[\"neg\"]\n",
        "lotion_df[\"neu\"] = sentiment_df[\"neu\"]\n",
        "lotion_df[\"compound\"] = sentiment_df[\"compound\"]\n",
        "# Examine values\n",
        "lotion_df[[\"pos\",\"neg\",\"neu\",\"compound\"]].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.204</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.752</td>\n",
              "      <td>0.8689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.075</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.803</td>\n",
              "      <td>-0.2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.141</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.739</td>\n",
              "      <td>0.2263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.228</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.7003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pos    neg    neu  compound\n",
              "0  0.204  0.044  0.752    0.8689\n",
              "1  0.075  0.122  0.803   -0.2023\n",
              "2  0.141  0.120  0.739    0.2263\n",
              "3  0.000  0.000  1.000    0.0000\n",
              "4  0.228  0.000  0.772    0.7003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wb80Ts61CHt"
      },
      "source": [
        "# Split products based on price. \n",
        "lower_priced_products = lotion_df[lotion_df[\"price_unit\"] <= cutoff_lotion] \n",
        "higher_priced_products = lotion_df[lotion_df[\"price_unit\"] > cutoff_lotion]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "US0Po8j91FzB",
        "outputId": "bf07e92c-a467-45d2-9bd3-1ffdfc8e527b"
      },
      "source": [
        "# Examine sentiment scores for the lower priced products.\n",
        "lower_priced_products[[\"pos\",\"neg\",\"neu\",\"compound\"]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.182294</td>\n",
              "      <td>0.018390</td>\n",
              "      <td>0.799310</td>\n",
              "      <td>0.714098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.102369</td>\n",
              "      <td>0.033779</td>\n",
              "      <td>0.105961</td>\n",
              "      <td>0.376015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.406000</td>\n",
              "      <td>-0.840200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>0.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.193000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.790500</td>\n",
              "      <td>0.896800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.249000</td>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.866250</td>\n",
              "      <td>0.969000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.594000</td>\n",
              "      <td>0.292000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               pos          neg          neu     compound\n",
              "count  1232.000000  1232.000000  1232.000000  1232.000000\n",
              "mean      0.182294     0.018390     0.799310     0.714098\n",
              "std       0.102369     0.033779     0.105961     0.376015\n",
              "min       0.000000     0.000000     0.406000    -0.840200\n",
              "25%       0.115000     0.000000     0.730000     0.612400\n",
              "50%       0.193000     0.000000     0.790500     0.896800\n",
              "75%       0.249000     0.028000     0.866250     0.969000\n",
              "max       0.594000     0.292000     1.000000     0.998700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Zoz9H4om1F6V",
        "outputId": "021e4320-4ece-464b-a48c-97d3af477970"
      },
      "source": [
        "# Examine sentiment scores for the higher priced products. \n",
        "higher_priced_products[[\"pos\",\"neg\",\"neu\",\"compound\"]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.00000</td>\n",
              "      <td>1232.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.169657</td>\n",
              "      <td>0.017519</td>\n",
              "      <td>0.81283</td>\n",
              "      <td>0.663473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.105497</td>\n",
              "      <td>0.036884</td>\n",
              "      <td>0.11027</td>\n",
              "      <td>0.393575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.44000</td>\n",
              "      <td>-0.855500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.73575</td>\n",
              "      <td>0.493900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.173000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.80800</td>\n",
              "      <td>0.848100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.245000</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>0.88500</td>\n",
              "      <td>0.957100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.329000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.999400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               pos          neg         neu     compound\n",
              "count  1232.000000  1232.000000  1232.00000  1232.000000\n",
              "mean      0.169657     0.017519     0.81283     0.663473\n",
              "std       0.105497     0.036884     0.11027     0.393575\n",
              "min       0.000000     0.000000     0.44000    -0.855500\n",
              "25%       0.100000     0.000000     0.73575     0.493900\n",
              "50%       0.173000     0.000000     0.80800     0.848100\n",
              "75%       0.245000     0.023000     0.88500     0.957100\n",
              "max       0.560000     0.329000     1.00000     0.999400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHrFGHSiDEFI"
      },
      "source": [
        "### Authenticity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0jhVK4xnVq5"
      },
      "source": [
        "#### Term Frequency - Inverse Document Frequency  \n",
        "TFIDF gives a score for each token/word in 1 document of documents cominbed into one. For authenticy, we want the words and its tfidf over the whole lower price \"document\" and higher price \"document\" (lower_priced_products, higher_priced_products)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUtKtm_ln2WK"
      },
      "source": [
        "# Create TFIDF function\n",
        "def tfidf(column, col_name):\n",
        "  tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
        "  column = column.dropna().tolist()\n",
        "  text = [' '.join(column)] # makes into a list with one item\n",
        "    #np.unique(np.hstack(column)).tolist()\n",
        "  print(text)\n",
        "  tfIdf = tfIdfVectorizer.fit_transform(text)\n",
        "  # tfIdf[#] makes it only look at that row - need to fix\n",
        "  df_name = pd.DataFrame(tfIdf.T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[col_name])\n",
        "  df_name = df_name.sort_values(col_name, ascending=False)\n",
        "  return df_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "280m0pQBZND9"
      },
      "source": [
        "#  Create TFIDF data for lower priced lotions: titles, about, and description\n",
        "lower_title_tf = tfidf(lower_priced_products.title_cl, col_name='l_title_tfidf')\n",
        "lower_about_tf = tfidf(lower_priced_products.about_cl, col_name='l_about_tfidf')\n",
        "lower_desc_tf = tfidf(lower_priced_products.description_cl, col_name='l_desc_tfidf')\n",
        "\n",
        "#  Create TFIDF data for higher priced lotions: titles, about, and description\n",
        "higher_title_tf = tfidf(higher_priced_products.title_cl, col_name='h_title_tfidf')\n",
        "higher_about_tf = tfidf(higher_priced_products.about_cl, col_name='h_about_tfidf')\n",
        "higher_desc_tf = tfidf(higher_priced_products.description_cl, col_name='h_desc_tfidf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbijZT7lqHtD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "outputId": "217aa1e4-0540-46b7-b0e8-6e85b65e72d6"
      },
      "source": [
        "# Create TFIDF dataframe for lower priced products\n",
        "lower_tf = lower_title_tf.join(lower_about_tf, how='outer')\n",
        "lower_tf = lower_tf.join(lower_desc_tf, how='outer')\n",
        "lower_tf['word'] = lower_tf.index\n",
        "lower_tf.sort_values(by=['l_desc_tfidf'], ascending=False).head(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>l_title_tfidf</th>\n",
              "      <th>l_about_tfidf</th>\n",
              "      <th>l_desc_tfidf</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hair</th>\n",
              "      <td>0.243264</td>\n",
              "      <td>0.748543</td>\n",
              "      <td>0.706335</td>\n",
              "      <td>hair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prod</th>\n",
              "      <td>0.779860</td>\n",
              "      <td>0.341208</td>\n",
              "      <td>0.400664</td>\n",
              "      <td>prod</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>moistur</th>\n",
              "      <td>0.100680</td>\n",
              "      <td>0.162522</td>\n",
              "      <td>0.164803</td>\n",
              "      <td>moistur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>natur</th>\n",
              "      <td>0.069115</td>\n",
              "      <td>0.110209</td>\n",
              "      <td>0.142115</td>\n",
              "      <td>natur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oil</th>\n",
              "      <td>0.089795</td>\n",
              "      <td>0.133427</td>\n",
              "      <td>0.126657</td>\n",
              "      <td>oil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>color</th>\n",
              "      <td>0.069115</td>\n",
              "      <td>0.114912</td>\n",
              "      <td>0.123665</td>\n",
              "      <td>color</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product</th>\n",
              "      <td>0.005986</td>\n",
              "      <td>0.116675</td>\n",
              "      <td>0.122917</td>\n",
              "      <td>product</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>use</th>\n",
              "      <td>0.002177</td>\n",
              "      <td>0.104332</td>\n",
              "      <td>0.105713</td>\n",
              "      <td>use</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>help</th>\n",
              "      <td>0.004898</td>\n",
              "      <td>0.104919</td>\n",
              "      <td>0.101475</td>\n",
              "      <td>help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clean</th>\n",
              "      <td>0.019592</td>\n",
              "      <td>0.106683</td>\n",
              "      <td>0.095242</td>\n",
              "      <td>clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>scalp</th>\n",
              "      <td>0.030476</td>\n",
              "      <td>0.089049</td>\n",
              "      <td>0.091253</td>\n",
              "      <td>scalp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>formula</th>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.076412</td>\n",
              "      <td>0.089757</td>\n",
              "      <td>formula</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smooth</th>\n",
              "      <td>0.041360</td>\n",
              "      <td>0.083759</td>\n",
              "      <td>0.084022</td>\n",
              "      <td>smooth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shampoo</th>\n",
              "      <td>0.005442</td>\n",
              "      <td>0.031740</td>\n",
              "      <td>0.083773</td>\n",
              "      <td>shampoo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dri</th>\n",
              "      <td>0.039728</td>\n",
              "      <td>0.079939</td>\n",
              "      <td>0.078288</td>\n",
              "      <td>dri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz</th>\n",
              "      <td>0.394012</td>\n",
              "      <td>0.050843</td>\n",
              "      <td>0.076293</td>\n",
              "      <td>oz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shine</th>\n",
              "      <td>0.027755</td>\n",
              "      <td>0.079057</td>\n",
              "      <td>0.073800</td>\n",
              "      <td>shine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nourish</th>\n",
              "      <td>0.029932</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.070559</td>\n",
              "      <td>nourish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>damag</th>\n",
              "      <td>0.017959</td>\n",
              "      <td>0.079351</td>\n",
              "      <td>0.068813</td>\n",
              "      <td>damag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leav</th>\n",
              "      <td>0.013605</td>\n",
              "      <td>0.073179</td>\n",
              "      <td>0.068315</td>\n",
              "      <td>leav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condition</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034973</td>\n",
              "      <td>0.062830</td>\n",
              "      <td>condition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soft</th>\n",
              "      <td>0.008707</td>\n",
              "      <td>0.075530</td>\n",
              "      <td>0.062580</td>\n",
              "      <td>soft</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodi</th>\n",
              "      <td>0.038639</td>\n",
              "      <td>0.044378</td>\n",
              "      <td>0.060835</td>\n",
              "      <td>bodi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condit</th>\n",
              "      <td>0.019048</td>\n",
              "      <td>0.059660</td>\n",
              "      <td>0.060586</td>\n",
              "      <td>condit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>protect</th>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.067595</td>\n",
              "      <td>0.059588</td>\n",
              "      <td>protect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>style</th>\n",
              "      <td>0.004898</td>\n",
              "      <td>0.029389</td>\n",
              "      <td>0.058342</td>\n",
              "      <td>style</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ingredi</th>\n",
              "      <td>0.011429</td>\n",
              "      <td>0.068477</td>\n",
              "      <td>0.056597</td>\n",
              "      <td>ingredi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>care</th>\n",
              "      <td>0.033197</td>\n",
              "      <td>0.042908</td>\n",
              "      <td>0.056347</td>\n",
              "      <td>care</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>0.004898</td>\n",
              "      <td>0.075824</td>\n",
              "      <td>0.055599</td>\n",
              "      <td>make</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>size</th>\n",
              "      <td>0.005442</td>\n",
              "      <td>0.009698</td>\n",
              "      <td>0.050862</td>\n",
              "      <td>size</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           l_title_tfidf  l_about_tfidf  l_desc_tfidf       word\n",
              "hair            0.243264       0.748543      0.706335       hair\n",
              "prod            0.779860       0.341208      0.400664       prod\n",
              "moistur         0.100680       0.162522      0.164803    moistur\n",
              "natur           0.069115       0.110209      0.142115      natur\n",
              "oil             0.089795       0.133427      0.126657        oil\n",
              "color           0.069115       0.114912      0.123665      color\n",
              "product         0.005986       0.116675      0.122917    product\n",
              "use             0.002177       0.104332      0.105713        use\n",
              "help            0.004898       0.104919      0.101475       help\n",
              "clean           0.019592       0.106683      0.095242      clean\n",
              "scalp           0.030476       0.089049      0.091253      scalp\n",
              "formula         0.016871       0.076412      0.089757    formula\n",
              "smooth          0.041360       0.083759      0.084022     smooth\n",
              "shampoo         0.005442       0.031740      0.083773    shampoo\n",
              "dri             0.039728       0.079939      0.078288        dri\n",
              "oz              0.394012       0.050843      0.076293         oz\n",
              "shine           0.027755       0.079057      0.073800      shine\n",
              "nourish         0.029932       0.077881      0.070559    nourish\n",
              "damag           0.017959       0.079351      0.068813      damag\n",
              "leav            0.013605       0.073179      0.068315       leav\n",
              "condition            NaN       0.034973      0.062830  condition\n",
              "soft            0.008707       0.075530      0.062580       soft\n",
              "bodi            0.038639       0.044378      0.060835       bodi\n",
              "condit          0.019048       0.059660      0.060586     condit\n",
              "protect         0.016871       0.067595      0.059588    protect\n",
              "style           0.004898       0.029389      0.058342      style\n",
              "ingredi         0.011429       0.068477      0.056597    ingredi\n",
              "care            0.033197       0.042908      0.056347       care\n",
              "make            0.004898       0.075824      0.055599       make\n",
              "size            0.005442       0.009698      0.050862       size"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnc720Y7Qljr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "outputId": "1313ff47-de27-4399-cb22-ea0004860d0a"
      },
      "source": [
        "# Create TFIDF dataframe for higher priced products\n",
        "higher_tf = higher_title_tf.join(higher_about_tf, how='outer')\n",
        "higher_tf = higher_tf.join(higher_desc_tf, how='outer')\n",
        "higher_tf['word'] = higher_tf.index\n",
        "higher_tf.sort_values(by=['h_desc_tfidf'], ascending=False).head(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>h_title_tfidf</th>\n",
              "      <th>h_about_tfidf</th>\n",
              "      <th>h_desc_tfidf</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hair</th>\n",
              "      <td>0.245107</td>\n",
              "      <td>0.761475</td>\n",
              "      <td>0.752358</td>\n",
              "      <td>hair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prod</th>\n",
              "      <td>0.758765</td>\n",
              "      <td>0.362374</td>\n",
              "      <td>0.368224</td>\n",
              "      <td>prod</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>color</th>\n",
              "      <td>0.074598</td>\n",
              "      <td>0.130687</td>\n",
              "      <td>0.136663</td>\n",
              "      <td>color</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>moistur</th>\n",
              "      <td>0.054350</td>\n",
              "      <td>0.117833</td>\n",
              "      <td>0.124162</td>\n",
              "      <td>moistur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>natur</th>\n",
              "      <td>0.058080</td>\n",
              "      <td>0.127627</td>\n",
              "      <td>0.121036</td>\n",
              "      <td>natur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oil</th>\n",
              "      <td>0.060211</td>\n",
              "      <td>0.136808</td>\n",
              "      <td>0.113649</td>\n",
              "      <td>oil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product</th>\n",
              "      <td>0.009058</td>\n",
              "      <td>0.079881</td>\n",
              "      <td>0.105410</td>\n",
              "      <td>product</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>use</th>\n",
              "      <td>0.002131</td>\n",
              "      <td>0.111099</td>\n",
              "      <td>0.104841</td>\n",
              "      <td>use</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>help</th>\n",
              "      <td>0.004263</td>\n",
              "      <td>0.097633</td>\n",
              "      <td>0.101432</td>\n",
              "      <td>help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>scalp</th>\n",
              "      <td>0.043693</td>\n",
              "      <td>0.092736</td>\n",
              "      <td>0.088931</td>\n",
              "      <td>scalp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clean</th>\n",
              "      <td>0.028773</td>\n",
              "      <td>0.089981</td>\n",
              "      <td>0.088646</td>\n",
              "      <td>clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dri</th>\n",
              "      <td>0.058080</td>\n",
              "      <td>0.083860</td>\n",
              "      <td>0.084100</td>\n",
              "      <td>dri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smooth</th>\n",
              "      <td>0.031970</td>\n",
              "      <td>0.063048</td>\n",
              "      <td>0.082396</td>\n",
              "      <td>smooth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz</th>\n",
              "      <td>0.474228</td>\n",
              "      <td>0.079575</td>\n",
              "      <td>0.075009</td>\n",
              "      <td>oz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leav</th>\n",
              "      <td>0.018649</td>\n",
              "      <td>0.067027</td>\n",
              "      <td>0.069042</td>\n",
              "      <td>leav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shine</th>\n",
              "      <td>0.021314</td>\n",
              "      <td>0.060294</td>\n",
              "      <td>0.067905</td>\n",
              "      <td>shine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>0.009058</td>\n",
              "      <td>0.069475</td>\n",
              "      <td>0.066769</td>\n",
              "      <td>make</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>protect</th>\n",
              "      <td>0.019715</td>\n",
              "      <td>0.067945</td>\n",
              "      <td>0.066201</td>\n",
              "      <td>protect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>damag</th>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.077739</td>\n",
              "      <td>0.066201</td>\n",
              "      <td>damag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shampoo</th>\n",
              "      <td>0.006927</td>\n",
              "      <td>0.037033</td>\n",
              "      <td>0.064780</td>\n",
              "      <td>shampoo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract</th>\n",
              "      <td>0.006394</td>\n",
              "      <td>0.038563</td>\n",
              "      <td>0.062791</td>\n",
              "      <td>extract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condit</th>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.053866</td>\n",
              "      <td>0.060802</td>\n",
              "      <td>condit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ingredi</th>\n",
              "      <td>0.005328</td>\n",
              "      <td>0.059988</td>\n",
              "      <td>0.060234</td>\n",
              "      <td>ingredi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condition</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.033054</td>\n",
              "      <td>0.055688</td>\n",
              "      <td>condition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>style</th>\n",
              "      <td>0.007993</td>\n",
              "      <td>0.028157</td>\n",
              "      <td>0.054836</td>\n",
              "      <td>style</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>formula</th>\n",
              "      <td>0.013854</td>\n",
              "      <td>0.063354</td>\n",
              "      <td>0.054552</td>\n",
              "      <td>formula</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soft</th>\n",
              "      <td>0.008525</td>\n",
              "      <td>0.050806</td>\n",
              "      <td>0.052847</td>\n",
              "      <td>soft</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>care</th>\n",
              "      <td>0.030905</td>\n",
              "      <td>0.048969</td>\n",
              "      <td>0.052847</td>\n",
              "      <td>care</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>protein</th>\n",
              "      <td>0.010124</td>\n",
              "      <td>0.036727</td>\n",
              "      <td>0.045744</td>\n",
              "      <td>protein</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nourish</th>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.062130</td>\n",
              "      <td>0.044891</td>\n",
              "      <td>nourish</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           h_title_tfidf  h_about_tfidf  h_desc_tfidf       word\n",
              "hair            0.245107       0.761475      0.752358       hair\n",
              "prod            0.758765       0.362374      0.368224       prod\n",
              "color           0.074598       0.130687      0.136663      color\n",
              "moistur         0.054350       0.117833      0.124162    moistur\n",
              "natur           0.058080       0.127627      0.121036      natur\n",
              "oil             0.060211       0.136808      0.113649        oil\n",
              "product         0.009058       0.079881      0.105410    product\n",
              "use             0.002131       0.111099      0.104841        use\n",
              "help            0.004263       0.097633      0.101432       help\n",
              "scalp           0.043693       0.092736      0.088931      scalp\n",
              "clean           0.028773       0.089981      0.088646      clean\n",
              "dri             0.058080       0.083860      0.084100        dri\n",
              "smooth          0.031970       0.063048      0.082396     smooth\n",
              "oz              0.474228       0.079575      0.075009         oz\n",
              "leav            0.018649       0.067027      0.069042       leav\n",
              "shine           0.021314       0.060294      0.067905      shine\n",
              "make            0.009058       0.069475      0.066769       make\n",
              "protect         0.019715       0.067945      0.066201    protect\n",
              "damag           0.020781       0.077739      0.066201      damag\n",
              "shampoo         0.006927       0.037033      0.064780    shampoo\n",
              "extract         0.006394       0.038563      0.062791    extract\n",
              "condit          0.020781       0.053866      0.060802     condit\n",
              "ingredi         0.005328       0.059988      0.060234    ingredi\n",
              "condition            NaN       0.033054      0.055688  condition\n",
              "style           0.007993       0.028157      0.054836      style\n",
              "formula         0.013854       0.063354      0.054552    formula\n",
              "soft            0.008525       0.050806      0.052847       soft\n",
              "care            0.030905       0.048969      0.052847       care\n",
              "protein         0.010124       0.036727      0.045744    protein\n",
              "nourish         0.020781       0.062130      0.044891    nourish"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZgWyDoWdMO_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "230c6462-f6a1-41b8-df93-609e641ac709"
      },
      "source": [
        "# Create joined TFIDF dataframe\n",
        "tfidf_joined = lower_tf.merge(higher_tf, how='outer', on ='word')\n",
        "tfidf_joined = tfidf_joined[['word', 'l_title_tfidf', 'h_title_tfidf',\t'l_about_tfidf', 'h_about_tfidf', 'l_desc_tfidf', 'h_desc_tfidf']]\n",
        "tfidf_joined.sort_values(by='l_title_tfidf', ascending=False) # sort to see, change by accordingly "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>l_title_tfidf</th>\n",
              "      <th>h_title_tfidf</th>\n",
              "      <th>l_about_tfidf</th>\n",
              "      <th>h_about_tfidf</th>\n",
              "      <th>l_desc_tfidf</th>\n",
              "      <th>h_desc_tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3580</th>\n",
              "      <td>prod</td>\n",
              "      <td>0.779860</td>\n",
              "      <td>0.758765</td>\n",
              "      <td>0.341208</td>\n",
              "      <td>0.362374</td>\n",
              "      <td>0.400664</td>\n",
              "      <td>0.368224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3275</th>\n",
              "      <td>oz</td>\n",
              "      <td>0.394012</td>\n",
              "      <td>0.474228</td>\n",
              "      <td>0.050843</td>\n",
              "      <td>0.079575</td>\n",
              "      <td>0.076293</td>\n",
              "      <td>0.075009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3246</th>\n",
              "      <td>ounc</td>\n",
              "      <td>0.250883</td>\n",
              "      <td>0.209406</td>\n",
              "      <td>0.023805</td>\n",
              "      <td>0.031830</td>\n",
              "      <td>0.040391</td>\n",
              "      <td>0.034095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022</th>\n",
              "      <td>hair</td>\n",
              "      <td>0.243264</td>\n",
              "      <td>0.245107</td>\n",
              "      <td>0.748543</td>\n",
              "      <td>0.761475</td>\n",
              "      <td>0.706335</td>\n",
              "      <td>0.752358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1705</th>\n",
              "      <td>fl</td>\n",
              "      <td>0.115918</td>\n",
              "      <td>0.098576</td>\n",
              "      <td>0.008229</td>\n",
              "      <td>0.009488</td>\n",
              "      <td>0.019447</td>\n",
              "      <td>0.011081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7717</th>\n",
              "      <td>zingib</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7718</th>\n",
              "      <td>zip</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7719</th>\n",
              "      <td>zipper</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7720</th>\n",
              "      <td>zizyphu</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7721</th>\n",
              "      <td>zogic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7722 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         word  l_title_tfidf  ...  l_desc_tfidf  h_desc_tfidf\n",
              "3580     prod       0.779860  ...      0.400664      0.368224\n",
              "3275       oz       0.394012  ...      0.076293      0.075009\n",
              "3246     ounc       0.250883  ...      0.040391      0.034095\n",
              "2022     hair       0.243264  ...      0.706335      0.752358\n",
              "1705       fl       0.115918  ...      0.019447      0.011081\n",
              "...       ...            ...  ...           ...           ...\n",
              "7717   zingib            NaN  ...           NaN      0.000284\n",
              "7718      zip            NaN  ...           NaN           NaN\n",
              "7719   zipper            NaN  ...           NaN           NaN\n",
              "7720  zizyphu            NaN  ...           NaN      0.000284\n",
              "7721    zogic            NaN  ...           NaN           NaN\n",
              "\n",
              "[7722 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tza3fbMzgMPh"
      },
      "source": [
        "### Health\n",
        "\n",
        "\n",
        "*   Ingredients from Amazon Products \n",
        "*   Toxicity Levels from California Chemicals in Cosmetics Database \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4AdI3DdpHtU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "8696663b-0f62-4c00-b725-43d602958529"
      },
      "source": [
        "# Check if ingredient exists in our data\n",
        "lotion_df[lotion_df['ingredients'].str.contains('retinol', na=False)][[\"product_title_cl\",\"ingredients\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_title_cl</th>\n",
              "      <th>ingredients</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [product_title_cl, ingredients]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlp8QK0ckVfT",
        "outputId": "8139bd94-79a6-4df3-b368-0fba5390bbba"
      },
      "source": [
        "lotion_df.ingredients.unique()[0:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan,\n",
              "       'Lawsonia inermis (red henna), indigofereae (black henna) and cassia obovata (neutral henna).',\n",
              "       'Honey, Oat, Vitamin E'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAtjCDR4jvPl"
      },
      "source": [
        "toxic_chem_li = ['titanium dioxide',\n",
        " 'distillates',\n",
        " 'estragole',\n",
        " 'cocamide diethanolamine',\n",
        " 'toluene',\n",
        " 'chromium',\n",
        " 'retinol',\n",
        " 'retinol/retinyl esters',\n",
        " 'vitamin a',\n",
        " 'vitamin a palmitate',\n",
        " 'butylated hydroxyanisole',\n",
        " 'coffea arabica extract',\n",
        " 'lauramide diethanolamine',\n",
        " 'silica',\n",
        " 'carbon black',\n",
        " 'genistein',\n",
        " 'progesterone',\n",
        " '2,4-hexadienal',\n",
        " 'methyleugenol',\n",
        " 'carbon-black extracts',\n",
        " 'retinyl palmitate',\n",
        " 'o-phenylphenol',\n",
        " 'acrylamide',\n",
        " 'formaldehyde',\n",
        " 'ginkgo biloba extract',\n",
        " 'mica',\n",
        " 'ethylene glycol',\n",
        " 'acetic acid',\n",
        " 'ethyl acrylate',\n",
        " 'trade secret',\n",
        " 'methanol',\n",
        " 'mineral oils',\n",
        " 'diethanolamine',\n",
        " 'tea-lauryl sulfate',\n",
        " 'retinyl acetate',\n",
        " 'lead acetate',\n",
        " 'talc',\n",
        " 'triethanolamine',\n",
        " 'o-phenylenediamine and its salts',\n",
        " 'safrole',\n",
        " 'styrene',\n",
        " 'acetaldehyde',\n",
        " 'cocamide dea',\n",
        " '1,4-dioxane',\n",
        " 'arsenic',\n",
        " 'dichloroacetic acid',\n",
        " 'ethylene oxide',\n",
        " 'lead',\n",
        " 'dichloromethane',\n",
        " 'benzene',\n",
        " 'benzyl chloride',\n",
        " 'n-nitrosodimethylamine',\n",
        " 'propylene oxide',\n",
        " 'methyl chloride',\n",
        " 'cadmium and cadmium compounds',\n",
        " 'n-methylpyrrolidone',\n",
        " 'di-n-butyl phthalate',\n",
        " 'coal tars',\n",
        " 'all-trans retinoic acid',\n",
        " 'quinoline and its strong acid salts',\n",
        " 'methylene glycol',\n",
        " 'benzophenone',\n",
        " 'cocamide',\n",
        " 'lauramide dea',\n",
        " 'aloe vera',\n",
        " 'musk xylene',\n",
        " 'aspirin',\n",
        " 'coal tar',\n",
        " 'benzophenone-3',\n",
        " 'quartz',\n",
        " 'talc containing asbestiform fibers',\n",
        " 'sodium bromate',\n",
        " 'phenacetin',\n",
        " 'mercury and mercury compounds',\n",
        " 'p-aminodiphenylamine',\n",
        " 'permethrin',\n",
        " 'acetylsalicylic acid',\n",
        " 'coal tar extract',\n",
        " 'selenium sulfide',\n",
        " 'oil orange ss',\n",
        " 'spironolactone',\n",
        " 'nickel',\n",
        " 'caffeic acid',\n",
        " 'cocamide mea',\n",
        " 'cosmetic talc',\n",
        " 'c.i. acid red 114',\n",
        " 'caffeine',\n",
        " 'benzophenone-4',\n",
        " 'ethanol in alcoholic beverages',\n",
        " 'coffee extract',\n",
        " 'retinol palmitate',\n",
        " 'coffee bean extract',\n",
        " 'propylene glycol mono-t-butyl ether',\n",
        " 'avobenzone',\n",
        " 'coal tar solution',\n",
        " 'pulegone',\n",
        " 'beta-myrcene',\n",
        " '2,2-bis(bromomethyl)-1,3-propanediol',\n",
        " 'benzo[a]pyrene',\n",
        " 'benz[a]anthracene',\n",
        " 'extract of coffee bean',\n",
        " 'goldenseal root powder',\n",
        " 'isopropyl alcohol manufacture using strong acids',\n",
        " '2-propyleneacrolein',\n",
        " 'n,n-dimethyl-p-toluidine',\n",
        " 'formaldehyde solution',\n",
        " 'n-nitrosodiethanolamine',\n",
        " 'benzophenone-2',\n",
        " 'vinyl acetate',\n",
        " 'trichloroacetic acid',\n",
        " 'phenacemide',\n",
        " 'polygeenan',\n",
        " 'diethanolamides of the fatty acids of coconut oil',\n",
        " 'bisphenol a',\n",
        " 'hydrous magnesium silicate']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ0IYbsplcv9"
      },
      "source": [
        "# If ingredients contain toxic ingredient, label how many, else 0. \n",
        "def check_toxic(ing):\n",
        "  counter = 0\n",
        "  for i in toxic_chem_li:\n",
        "    if i in ing.lower().split(', '): \n",
        "      counter += 1\n",
        "  \n",
        "  return counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWaz56fldQg1"
      },
      "source": [
        "# If a product  contain toxic ingredients, create a list of them joined on |.\n",
        "def check_toxic_ingredient(ing):\n",
        "  li = []\n",
        "  for i in toxic_chem_li:\n",
        "    if i in ing.lower().split(', '): \n",
        "      li.append(i)\n",
        "  return '|'.join(li)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBigTc7cnZnw"
      },
      "source": [
        "# Retrieve count of toxic ingredients.\n",
        "lotion_df['toxic_count'] = lotion_df['ingredients'].apply(lambda x: check_toxic(x) if pd.notnull(x) else None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zxoAZtznuPs"
      },
      "source": [
        "# Retrieve list of toxic ingredients. \n",
        "lotion_df['toxic_ing'] = lotion_df['ingredients'].apply(lambda x: check_toxic_ingredient(x) if pd.notnull(x) else None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DET-8tQ5UW1f"
      },
      "source": [
        "# Split products based on price. \n",
        "lower_priced_products = lotion_df[lotion_df[\"price_unit\"] <= cutoff_lotion] \n",
        "higher_priced_products = lotion_df[lotion_df[\"price_unit\"] > cutoff_lotion]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5DB-C7EUW1l",
        "outputId": "0522c4c4-3bc9-496d-81f3-4b22bf2b539f"
      },
      "source": [
        "# Examine distribution of toxic ingredients for lower priced products. \n",
        "lower_priced_products[\"toxic_count\"].describe() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    508.000000\n",
              "mean       0.250000\n",
              "std        0.542567\n",
              "min        0.000000\n",
              "25%        0.000000\n",
              "50%        0.000000\n",
              "75%        0.000000\n",
              "max        4.000000\n",
              "Name: toxic_count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvZj22zvUW1m",
        "outputId": "9225861e-511d-4d58-9429-3c05b01edb06"
      },
      "source": [
        "# Examine distribution of toxic ingredients for higher  priced products.\n",
        "higher_priced_products[\"toxic_count\"].describe() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    453.000000\n",
              "mean       0.253863\n",
              "std        0.523361\n",
              "min        0.000000\n",
              "25%        0.000000\n",
              "50%        0.000000\n",
              "75%        0.000000\n",
              "max        2.000000\n",
              "Name: toxic_count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpmzB0BRiLqg"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1xKtRj4N5C-"
      },
      "source": [
        "# Truncate text to a max length of 100.\n",
        "def truncate_or_fill(text):\n",
        "  if len(text.split()) >= 100:\n",
        "    return ' '.join(text.split()[:100])\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MeaGukzN52N"
      },
      "source": [
        "# Truncate all text to a max length of 100\n",
        "lotion_df[\"full_text_trunc\"] = lotion_df[\"full_text\"].apply(truncate_or_fill)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNZfFpAUt6Ct"
      },
      "source": [
        "#### Neural Network (Definition) \n",
        "\n",
        "Used for the baseline and the improvement.[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ude3smsft4Lx"
      },
      "source": [
        "# Run a 3 layer Dense Neural Network with Dropout and Relu activation.\n",
        "# Examine results interms of Mean Squared Error. \n",
        "def run_model(input_dims, train_features, train_labels, test_features, test_labels, epochs_):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add( tf.keras.layers.Dense(64, activation='relu',input_shape=input_dims))\n",
        "  model.add( tf.keras.layers.Dropout(0.1))\n",
        "  model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "  model.add( tf.keras.layers.Dense(1))\n",
        "\n",
        "  loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['mse'])  \n",
        "  \n",
        "  history = model.fit(train_features,train_labels, epochs=epochs_)\n",
        "  results = model.evaluate(test_features, test_labels)\n",
        "  print(\"Mean Absolute Error, Mean Squared Error \", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lwmf4rlu1ta"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pibiS-rHu1Cq"
      },
      "source": [
        "# Split data into train and test.\n",
        "random_df = pd.DataFrame(np.random.randn(lotion_df[\"full_text_trunc\"].shape[0]))\n",
        "mask = np.random.rand(len(random_df)) < 0.8\n",
        "train_features = lotion_df[mask]\n",
        "train_labels = lotion_df[\"price_unit\"][mask].fillna(0)\n",
        "test_features = lotion_df[~mask]\n",
        "test_labels = lotion_df[\"price_unit\"][~mask].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSU-95OG2WBF",
        "outputId": "57deb9e6-0c8e-4415-8322-332d73d67bbe"
      },
      "source": [
        "print(train_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_features.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1977, 29)\n",
            "(1977,)\n",
            "(487, 29)\n",
            "(487,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pip84JV2o4NT"
      },
      "source": [
        "### Baseline\n",
        "Run a Neural Network with features computed during the Freedman and Jurafsky Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAdIhDHkuIQ-"
      },
      "source": [
        "# Scale a dataframe column\n",
        "def scale_data(data):\n",
        "  min_max_scaler = preprocessing.StandardScaler()\n",
        "  data_scaled = min_max_scaler.fit_transform(data)\n",
        "  return pd.DataFrame(data_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxePsa64I4DZ"
      },
      "source": [
        "lotion_df[\"marketing_term_count_scaled\"] = scale_data((lotion_df[\"marketing_term_count_raw\"]+1).values.reshape(-1,1))\n",
        "lotion_df[\"flesch_kincaid_grade_scaled\"] = scale_data((lotion_df[\"flesch_kincaid_grade\"]).values.reshape(-1,1))\n",
        "lotion_df[\"reading_ease_scaled\"] = scale_data((lotion_df[\"reading_ease\"]).values.reshape(-1,1))\n",
        "lotion_df[\"lexicon_count_scaled\"] = scale_data((lotion_df[\"lexicon_count\"]).values.reshape(-1,1))\n",
        "lotion_df[\"mktg_embedding_counts_glov_scaled\"] = scale_data((lotion_df[\"mktg_embedding_counts_glov\"]).values.reshape(-1,1))\n",
        "lotion_df[\"compound_scaled\"] = scale_data((lotion_df[\"compound\"]).values.reshape(-1,1))\n",
        "lotion_df[\"toxic_count_scaled\"] = scale_data((lotion_df[\"toxic_count\"]).values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDxr73smxb8-"
      },
      "source": [
        "# Scaled Data\n",
        "# Mean Absolute Error, Mean Squared Error  [6.7269392013549805, 275.2937316894531]\n",
        "train = lotion_df[[\"marketing_term_count_scaled\" , \"flesch_kincaid_grade_scaled\" , \"reading_ease_scaled\" , \"lexicon_count_scaled\" , \"mktg_embedding_counts_glov_scaled\" , \"compound_scaled\",\"toxic_count_scaled\"]][mask]\n",
        "test = lotion_df[[\"marketing_term_count_scaled\" , \"flesch_kincaid_grade_scaled\" , \"reading_ease_scaled\" , \"lexicon_count_scaled\" , \"mktg_embedding_counts_glov_scaled\" , \"compound_scaled\",\"toxic_count_scaled\"]][~mask]\n",
        "\n",
        "# Results of using Unscaled Features. \n",
        "# Mean Absolute Error, Mean Squared Error  [6.72703218460083, 275.4627685546875]\n",
        "train = lotion_df[[\"marketing_term_count_raw\" , \"flesch_kincaid_grade\" , \"reading_ease\" , \"lexicon_count\" , \"mktg_embedding_counts_glov\" , \"compound\",\"toxic_count\"]][mask]\n",
        "test = lotion_df[[\"marketing_term_count_raw\" , \"flesch_kincaid_grade\" , \"reading_ease\" , \"lexicon_count\" , \"mktg_embedding_counts_glov\" , \"compound\",\"toxic_count\"]][~mask]\n",
        "\n",
        "# Scaled and neu instead of compount\n",
        "# Mean Absolute Error, Mean Squared Error  [6.727107048034668, 275.5451965332031]\n",
        "train = lotion_df[[ \"marketing_term_count_scaled\" ,\"flesch_kincaid_grade_scaled\" , \"reading_ease_scaled\" , \"lexicon_count_scaled\" , \"mktg_embedding_counts_glov_scaled\",\"toxic_count_scaled\" , \"neu\"]][mask]\n",
        "test = lotion_df[[\"marketing_term_count_scaled\" ,\"flesch_kincaid_grade_scaled\" , \"reading_ease_scaled\" , \"lexicon_count_scaled\" , \"mktg_embedding_counts_glov_scaled\" , \"neu\",\"toxic_count_scaled\"]][~mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0uJ5ERqyCSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f9bb47-9f2c-42fd-bfe7-3bd3ecd38cb0"
      },
      "source": [
        "run_model((train.shape[1],), train, train_labels, test, test_labels, 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 2.4400 - mse: 15.2492\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 2.0277 - mse: 15.4984\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5025 - mse: 10.2329\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4219 - mse: 9.0435\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4706 - mse: 10.2324\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4448 - mse: 9.0356\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4956 - mse: 10.8878\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3961 - mse: 9.0128\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3438 - mse: 7.5662\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4611 - mse: 9.1680\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4442 - mse: 9.8015\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4508 - mse: 10.2798\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4497 - mse: 9.2334\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4174 - mse: 9.4074\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4293 - mse: 9.1697\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3535 - mse: 6.9096\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5512 - mse: 11.2690\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4218 - mse: 9.3271\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4023 - mse: 8.9426\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4263 - mse: 9.4382\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3676 - mse: 7.9975\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4136 - mse: 8.5667\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4596 - mse: 9.6926\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4332 - mse: 10.1137\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4092 - mse: 9.1718\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4950 - mse: 9.3064\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3602 - mse: 8.1427\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3592 - mse: 8.5809\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4864 - mse: 10.4705\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3269 - mse: 7.4602\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4439 - mse: 8.7473\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4324 - mse: 8.5275\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3509 - mse: 8.8636\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4175 - mse: 9.6339\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4119 - mse: 8.2795\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3995 - mse: 8.6959\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4017 - mse: 9.1382\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4422 - mse: 9.0520\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3597 - mse: 8.0561\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4726 - mse: 9.4474\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4546 - mse: 9.4990\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3992 - mse: 8.9290\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3701 - mse: 7.7600\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3559 - mse: 7.7184\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4629 - mse: 9.5735\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4493 - mse: 9.3106\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5181 - mse: 10.4057\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2957 - mse: 6.7437\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3817 - mse: 8.3321\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3894 - mse: 8.3453\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4263 - mse: 9.0414\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4101 - mse: 8.6923\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3151 - mse: 7.4025\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 6.9131\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3584 - mse: 7.6326\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3316 - mse: 7.8406\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4253 - mse: 9.0659\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4071 - mse: 8.5174\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4079 - mse: 7.8870\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4063 - mse: 8.1069\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3028 - mse: 6.0235\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4549 - mse: 9.7045\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3794 - mse: 7.4572\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3842 - mse: 7.6692\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3337 - mse: 7.0324\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4737 - mse: 9.8704\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4137 - mse: 8.7568\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3209 - mse: 7.6199\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3124 - mse: 7.4765\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3905 - mse: 8.2750\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4370 - mse: 8.0568\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4062 - mse: 9.8606\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3669 - mse: 8.0670\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3490 - mse: 7.3452\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4601 - mse: 9.6420\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3925 - mse: 8.9490\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4482 - mse: 9.6388\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4093 - mse: 8.4538\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4313 - mse: 8.9255\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5051 - mse: 10.8210\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4011 - mse: 8.9907\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3193 - mse: 6.9769\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4636 - mse: 10.4149\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4469 - mse: 8.8317\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3928 - mse: 8.3968\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4157 - mse: 9.3973\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3793 - mse: 7.4543\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4710 - mse: 9.1088\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4353 - mse: 8.5188\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4984 - mse: 9.4261\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4005 - mse: 8.2347\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3738 - mse: 7.2731\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3824 - mse: 8.1404\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4325 - mse: 8.8109\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2888 - mse: 7.4533\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3994 - mse: 8.3621\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3792 - mse: 7.8605\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3933 - mse: 8.4323\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3802 - mse: 8.0975\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3263 - mse: 6.8363\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4357 - mse: 8.6636\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3698 - mse: 8.9817\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3374 - mse: 6.7622\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4028 - mse: 8.6769\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4508 - mse: 9.1494\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3074 - mse: 6.7259\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4034 - mse: 8.1308\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4736 - mse: 10.1070\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4272 - mse: 9.7376\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4450 - mse: 9.3209\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3933 - mse: 8.6029\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3775 - mse: 8.0364\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4774 - mse: 10.9166\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4147 - mse: 7.7932\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4375 - mse: 9.7229\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3688 - mse: 7.6457\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3774 - mse: 8.3091\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3885 - mse: 8.1837\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4476 - mse: 9.2128\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4234 - mse: 8.2691\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4726 - mse: 9.8785\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4480 - mse: 8.5692\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4408 - mse: 9.3743\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3545 - mse: 7.8708\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4642 - mse: 10.5384\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4651 - mse: 9.6160\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4692 - mse: 10.1574\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4603 - mse: 8.8930\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4679 - mse: 10.1261\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3969 - mse: 8.7039\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5032 - mse: 9.8345\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4307 - mse: 9.1164\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4196 - mse: 9.0309\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3937 - mse: 8.6950\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4906 - mse: 11.0160\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4644 - mse: 9.7958\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4216 - mse: 8.3105\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4382 - mse: 8.7112\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3996 - mse: 8.1697\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3852 - mse: 8.9274\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3535 - mse: 8.1518\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3938 - mse: 8.7223\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3804 - mse: 8.3198\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4466 - mse: 8.8667\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4170 - mse: 9.4673\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4623 - mse: 9.4082\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4495 - mse: 9.2719\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3961 - mse: 8.1513\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3484 - mse: 7.1457\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3966 - mse: 8.4601\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4177 - mse: 8.4817\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4341 - mse: 9.2326\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4594 - mse: 10.7031\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4336 - mse: 7.6437\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4905 - mse: 10.4711\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3295 - mse: 7.4490\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3590 - mse: 7.4480\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3330 - mse: 7.4559\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4803 - mse: 10.0570\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4325 - mse: 8.7055\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4741 - mse: 9.8388\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4354 - mse: 8.4171\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4684 - mse: 9.4075\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4154 - mse: 8.6426\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3557 - mse: 7.4795\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3638 - mse: 8.2454\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4548 - mse: 10.0668\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5257 - mse: 10.5341\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3661 - mse: 7.7484\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4694 - mse: 8.8280\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4379 - mse: 9.3175\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4102 - mse: 8.5011\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4145 - mse: 9.0794\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4185 - mse: 9.0794\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3857 - mse: 7.9344\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4091 - mse: 8.8948\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5590 - mse: 10.7640\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3330 - mse: 7.0846\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4152 - mse: 8.2389\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4223 - mse: 9.4223\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4260 - mse: 8.3758\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3712 - mse: 7.4605\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4003 - mse: 8.7982\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4443 - mse: 9.4245\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4184 - mse: 9.2257\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5140 - mse: 9.8669\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4951 - mse: 10.6569\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4098 - mse: 8.4795\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4163 - mse: 8.3518\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4741 - mse: 10.1693\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4267 - mse: 8.1284\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4853 - mse: 11.1352\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4629 - mse: 10.0205\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3497 - mse: 7.8008\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4384 - mse: 9.3165\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4318 - mse: 9.1391\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3660 - mse: 7.4811\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3735 - mse: 8.1778\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3714 - mse: 7.2598\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3370 - mse: 7.8820\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.4716 - mse: 9.0559\n",
            "Mean Absolute Error, Mean Squared Error  [1.4715862274169922, 9.055871963500977]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQYj-k6sudI_"
      },
      "source": [
        "### Improvement \n",
        "Use Embeddings and pass them through a Neural Net as well as a Convolutional Neural Net."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6QVwA5JicO5"
      },
      "source": [
        "\n",
        "#### ELMO - Contextualized Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GdtAQ3l2D_a"
      },
      "source": [
        "  elmo_embeddings_train_path_lotion = '/content/gdrive/MyDrive/266/final/data/elmo_embeddings_train_lotion.npy'\n",
        "  elmo_embedding_test_path_lotion = '/content/gdrive/MyDrive/266/final/data/elmo_embeddings_test_lotion.npy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd_dOXfpGVU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc2678e-2497-4355-8510-22b26aa1f911"
      },
      "source": [
        "train_features.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 'B00GYDKEOE',\n",
              "        'silicon mix bamboo nutritive shampoo treatment 16oz set', ...,\n",
              "        nan, None,\n",
              "        \"silicon mix silicon mix bambu extract nutrit prod oz ounc nutrit prod bamboo extract regener nourish vitamin e c f carthamu oil almond oil improv health beauti hair hair type includ natur process hair follow silicon mix bambu treatment good result work great brittl dull hair free residu improv silicon mix bambu treatment ' s effici \"],\n",
              "       [1, 'B001I7UL8A', 'clay esthe shampoo ex oz refill', ..., nan,\n",
              "        None,\n",
              "        'clay esth prod ex oz refil improv health scalp hair keep scalp healthi remov dirt excess oil sweat caus clog pore increa moistur soft loosen tension caus scalp relax leav hair moistur soft healthi'],\n",
              "       [2, 'B0009EXOO6',\n",
              "        'sexy hair concepts healthy sexy hair soy triwheat leavein conditioner oz',\n",
              "        ..., nan, None,\n",
              "        'healthi sexi hair triwheat leav prod oz  power combin soy cocoa work synergist perfectli care hair prod nourish moistur mild protein reconstructor safe use colortr hair truli amaz prod reconstruct moistur combat environment stress type hair'],\n",
              "       ...,\n",
              "       [2993, 'B007WK3AIK',\n",
              "        'manzanilla chamomille shine repair shampoo silky shine conditioner set 400ml grisi',\n",
              "        ..., nan, None,\n",
              "        'manzanilla chamomil shine repair prod silki shine prod set ml grisi x grisi prod manzanilla chamomil oz x grisi prod manzanilla chamomil oz prod prod combo shini repair hair silicon protein'],\n",
              "       [2994, 'B008UE16P6',\n",
              "        'nutrine garlic conditioner uncented 16oz pack', ..., nan, None,\n",
              "        'nutrin garlic prod uncent oz pack nutrin garlic prod uncent oz pack ounc countri origin unit state packag dimen product cml x cmw x cmh nutrin garlic prod uncent oz pack'],\n",
              "       [2995, 'B005Q7H3KM',\n",
              "        'tec italy silk system shine sulfate free shampoo ounce', ...,\n",
              "        nan, None,\n",
              "        'tec itali silk system shine sulfat free prod ounc design dri hair without shine silk protien shield hair harm uv ray oz prod silk system shine sulfat free prod launch design hous tec itali recommend normal use silk system shine sulfat free prod tec itali unisex oz shampoo']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNksUKgVY-d6"
      },
      "source": [
        "def create_and_save_elmo_embeddings(elmo_embeddings_train_path_, elmo_embedding_test_path_):\n",
        "  # Load pre-trained ELMO model\n",
        "  elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "  # NO BATCH - crashes\n",
        "  # tf_constants = tf.constant(train_features, dtype = tf.string) \n",
        "  # elmo_embeddings = elmo.signatures[\"default\"](tf_constants)[\"default\"]\n",
        "\n",
        "  # BATCH\n",
        "  # Training Embeddings\n",
        "  tf_constants = tf.constant(train_features[\"full_text_trunc\"], dtype = tf.string) \n",
        "  batch_size = 200\n",
        "  embeddings = []\n",
        "  for index in (np.arange(0, len(tf_constants), batch_size)):\n",
        "    batch_embeddings = elmo.signatures[\"default\"](tf_constants[index:index+batch_size])[\"default\"]\n",
        "    embeddings.append(batch_embeddings)\n",
        "\n",
        "  elmo_embeddings = tf.concat(embeddings, axis=0)\n",
        "\n",
        "  # Test Embeddings \n",
        "  tf_constants_test = tf.constant(test_features[\"full_text_trunc\"], dtype = tf.string)\n",
        "  embeddings_test = []\n",
        "  for index in (np.arange(0, len(tf_constants_test), batch_size)):\n",
        "    batch_embeddings = elmo.signatures[\"default\"](tf_constants_test[index:index+batch_size])[\"default\"]\n",
        "    embeddings_test.append(batch_embeddings)\n",
        "\n",
        "  elmo_embeddings_test = tf.concat(embeddings_test, axis=0)\n",
        "\n",
        "  # Store embeddings so its faster to load and use them later\n",
        "  np.save(elmo_embeddings_train_path_, elmo_embeddings)\n",
        "  np.save(elmo_embedding_test_path_, elmo_embeddings_test)\n",
        "\n",
        "\n",
        "create_and_save_elmo_embeddings(elmo_embeddings_train_path_lotion,elmo_embedding_test_path_lotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq1OSa-4XUJE"
      },
      "source": [
        "# Load pre-saved elmo word embeddings \n",
        "elmo_embeddings_lotion = np.load(elmo_embeddings_train_path_lotion)\n",
        "elmo_embeddings_lotion_test = np.load(elmo_embedding_test_path_lotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXYZRJx-XRkr",
        "outputId": "5bf2b5e0-8baf-449c-a14b-ff68de703c15"
      },
      "source": [
        "print(\"elmo_embeddings_lotion shape: \", elmo_embeddings_lotion.shape)\n",
        "print(\"elmo_embeddings_lotion_test shape: \", elmo_embeddings_lotion_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo_embeddings_lotion shape:  (1977, 1024)\n",
            "elmo_embeddings_lotion_test shape:  (487, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpdMbE_Anufq"
      },
      "source": [
        "#### BERT (Variations) Embeddings\n",
        "Semi-Adapted from [Source](https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfUC248Mgtzl"
      },
      "source": [
        "def create_and_save_bert_embeddings(text, embeddings_path, bert_type=\"distilbert\", batch_size=100):\n",
        "\n",
        "  # Load Pre-Trained BERT Model \n",
        "  if bert_type == \"distilbert\":\n",
        "    model_class, tokenizer_class, pretrained_weights = (transformer.DistilBertModel, transformer.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "  elif bert_type == \"roberta\":\n",
        "    model_class, tokenizer_class, pretrained_weights = (transformer.RobertaModel, transformer.RobertaTokenizer, 'roberta-base')\n",
        "  else:\n",
        "    model_class, tokenizer_class, pretrained_weights = (transformer.BertModel, transformer.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "  # Load pretrained model/tokenizer\n",
        "  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "  model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "  # Extract embeddings in batches\n",
        "  embeddings = []\n",
        "  for index in tqdm(np.arange(0, len(text[\"full_text_trunc\"]), batch_size)):\n",
        "    batch = text[\"full_text_trunc\"][index:index+batch_size]\n",
        "    tokenized = batch.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "\n",
        "    # Add padding so all inputs are the same size \n",
        "    max_len = 0\n",
        "    for i in tokenized.values:\n",
        "      if len(i) > max_len:\n",
        "          max_len = len(i)\n",
        "\n",
        "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "    # Apply attention mask to ignore padding \n",
        "    attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "    input_ids = torch.tensor(padded)  \n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Retrieve the representation of the entire sentence \n",
        "    embeddings.append(last_hidden_states[0][:,0,:].numpy())\n",
        "\n",
        "  # Since we did batches, concatenate into a single embeeddings list \n",
        "  embeddings = np.concatenate(embeddings, 0)  # N, 768\n",
        "  # Save to file \n",
        "  np.save(embeddings_path,embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQrWvpnenzPT"
      },
      "source": [
        " **DistilBERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0095WLBqbHPc"
      },
      "source": [
        "distilbert_embeddings_train_path_lotion = \"/content/gdrive/MyDrive/266/final/data/distilbert_embeddings_train_lotion.npy\"\n",
        "distilbert_embeddings_test_path_lotion =  \"/content/gdrive/MyDrive/266/final/data/distilbert_embeddings_test_lotion.npy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3m8c8vsf40H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167eb57a-81dc-4137-bec6-3cd1b36f8419"
      },
      "source": [
        "create_and_save_bert_embeddings(train_features, distilbert_embeddings_train_path_lotion  , bert_type=\"distilbert\")\n",
        "create_and_save_bert_embeddings(test_features, distilbert_embeddings_test_path_lotion, bert_type=\"distilbert\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [04:05<00:00, 12.29s/it]\n",
            "100%|██████████| 5/5 [01:00<00:00, 12.09s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lO45Vw-au7e"
      },
      "source": [
        "# Load pre-saved DistilBERT word embeddings \n",
        "distilbert_embeddings_lotion = np.load(distilbert_embeddings_train_path_lotion)\n",
        "distilbert_embeddings_lotion_test = np.load(distilbert_embeddings_test_path_lotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ByYzbkzlYC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a31dae-8143-4196-b0e3-81ab2ceed86a"
      },
      "source": [
        "print(\"distilbert_embeddings_lotion shape: \", distilbert_embeddings_lotion.shape)\n",
        "print(\"distilbert_embeddings_lotion_test shape: \", distilbert_embeddings_lotion_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distilbert_embeddings_lotion shape:  (1977, 768)\n",
            "distilbert_embeddings_lotion_test shape:  (487, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt0qz4eon1vn"
      },
      "source": [
        " **RoBERTa**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zqkgUZxbkYr"
      },
      "source": [
        "roberta_embeddings_train_path_lotion = \"/content/gdrive/MyDrive/266/final/data/roberta_embeddings_train_lotion.npy\"\n",
        "roberta_embeddings_test_path_lotion =  \"/content/gdrive/MyDrive/266/final/data/roberta_embeddings_test_lotion.npy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBYZY796bkYt",
        "outputId": "70bf3391-24cb-4e81-d94a-288c76d0415b"
      },
      "source": [
        "create_and_save_bert_embeddings(train_features, roberta_embeddings_train_path_lotion  , bert_type=\"roberta\")\n",
        "create_and_save_bert_embeddings(test_features, roberta_embeddings_test_path_lotion, bert_type=\"roberta\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [07:38<00:00, 22.92s/it]\n",
            "100%|██████████| 5/5 [01:45<00:00, 21.19s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSRo347EbkYu"
      },
      "source": [
        "# Load pre-saved RoBERTa word embeddings \n",
        "roberta_embeddings_lotion = np.load(roberta_embeddings_train_path_lotion)\n",
        "roberta_embeddings_lotion_test = np.load(roberta_embeddings_test_path_lotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqj6RG3ObkYu",
        "outputId": "27ed8cf6-535b-4489-bc1f-edfbf8e57964"
      },
      "source": [
        "print(\"roberta_embeddings_lotion shape: \", roberta_embeddings_lotion.shape)\n",
        "print(\"roberta_embeddings_lotion_test shape: \", roberta_embeddings_lotion_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta_embeddings_lotion shape:  (1977, 768)\n",
            "roberta_embeddings_lotion_test shape:  (487, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzpomyUqn4It"
      },
      "source": [
        "**BERT (original)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSiP0Wf1cLr7"
      },
      "source": [
        "bert_embeddings_train_path_lotion = \"/content/gdrive/MyDrive/266/final/data/bert_embeddings_train_lotion.npy\"\n",
        "bert_embeddings_test_path_lotion =  \"/content/gdrive/MyDrive/266/final/data/bert_embeddings_test_lotion.npy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc0jLBV6cLsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da157caf-c80c-48b7-8bce-04ef786bb4ea"
      },
      "source": [
        "create_and_save_bert_embeddings(train_features, bert_embeddings_train_path_lotion  , bert_type=\"bert\")\n",
        "create_and_save_bert_embeddings(test_features, bert_embeddings_test_path_lotion, bert_type=\"bert\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [08:14<00:00, 24.74s/it]\n",
            "100%|██████████| 5/5 [02:01<00:00, 24.33s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyR_nG8VcLsC"
      },
      "source": [
        "# Load pre-saved RoBERTa word embeddings \n",
        "bert_embeddings_lotion = np.load(bert_embeddings_train_path_lotion)\n",
        "bert_embeddings_lotion_test = np.load(bert_embeddings_test_path_lotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uecRkL65cLsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681bba91-179c-4555-8fe5-509144e37723"
      },
      "source": [
        "print(\"bert_embeddings_lotion shape: \", bert_embeddings_lotion.shape)\n",
        "print(\"bert_embeddings_lotion_test shape: \", bert_embeddings_lotion_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_embeddings_lotion shape:  (1977, 768)\n",
            "bert_embeddings_lotion_test shape:  (487, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgPje98Gifky"
      },
      "source": [
        "### Neural Network - Run for Improvement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_L4jPxui3NI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2d1641c-8197-4f10-94e7-fd0df0bb9c89"
      },
      "source": [
        "# ELMO\n",
        "run_model((1024,), elmo_embeddings_lotion, train_labels, elmo_embeddings_lotion_test, test_labels, 200)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error  [11.123788833618164, 4236.31884765625]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 1.5473 - mse: 9.3098\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3872 - mse: 7.8063\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.6155 - mse: 11.4117\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3370 - mse: 7.7866\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3573 - mse: 7.5294\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3978 - mse: 8.4461\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3397 - mse: 8.4036\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3573 - mse: 8.9310\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3913 - mse: 8.9624\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4076 - mse: 8.8540\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3330 - mse: 7.6743\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2807 - mse: 7.0752\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3194 - mse: 7.6414\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2554 - mse: 6.8633\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3702 - mse: 9.7030\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3134 - mse: 7.8437\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3445 - mse: 8.9716\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2657 - mse: 7.7330\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2826 - mse: 7.9341\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2317 - mse: 7.0440\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2968 - mse: 9.6729\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2201 - mse: 7.7632\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2809 - mse: 7.9647\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2409 - mse: 7.4397\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2309 - mse: 6.6990\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3311 - mse: 9.3855\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1337 - mse: 6.1812\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1313 - mse: 5.3371\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2048 - mse: 7.6263\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2466 - mse: 8.9071\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1159 - mse: 6.3575\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1730 - mse: 7.2713\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2232 - mse: 7.4118\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1938 - mse: 6.9492\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2830 - mse: 9.4078\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1436 - mse: 7.1694\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1181 - mse: 6.6950\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1393 - mse: 7.3005\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0984 - mse: 6.1595\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1802 - mse: 8.8722\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2046 - mse: 8.3251\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0961 - mse: 5.8242\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0164 - mse: 6.0947\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0755 - mse: 6.2042\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0734 - mse: 6.0586\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0569 - mse: 6.1753\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0982 - mse: 6.4298\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0874 - mse: 5.8571\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0795 - mse: 6.3315\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0296 - mse: 5.5305\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0897 - mse: 6.1070\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1021 - mse: 6.9489\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0302 - mse: 5.7459\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0605 - mse: 6.1135\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0355 - mse: 6.4269\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9790 - mse: 5.1668\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0411 - mse: 5.4605\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0854 - mse: 7.3324\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0219 - mse: 5.6658\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9050 - mse: 4.3451\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0019 - mse: 6.0949\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0064 - mse: 5.8557\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8769 - mse: 3.7433\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0494 - mse: 6.4800\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8464 - mse: 3.9021\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9223 - mse: 4.4737\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9781 - mse: 5.3510\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8721 - mse: 3.8838\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9286 - mse: 4.8223\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9337 - mse: 4.8987\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9144 - mse: 4.6070\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9083 - mse: 4.8807\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8352 - mse: 3.2128\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8872 - mse: 4.2996\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8682 - mse: 3.8958\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8714 - mse: 4.3657\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9672 - mse: 5.2962\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9752 - mse: 5.5415\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9396 - mse: 5.1955\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8159 - mse: 3.5626\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8812 - mse: 4.6039\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 3.5026\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8296 - mse: 3.6195\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8785 - mse: 5.2844\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9072 - mse: 4.3748\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7841 - mse: 2.9847\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 3.7665\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8082 - mse: 3.5914\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8519 - mse: 4.1017\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7767 - mse: 3.6856\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8577 - mse: 4.4095\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8458 - mse: 4.0105\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7951 - mse: 3.7020\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8487 - mse: 4.4697\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8532 - mse: 4.4022\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7832 - mse: 3.4473\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7894 - mse: 3.9470\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6905 - mse: 2.6218\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7118 - mse: 2.9543\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7705 - mse: 4.0878\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8210 - mse: 4.1397\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7212 - mse: 2.5609\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8054 - mse: 3.5273\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8304 - mse: 4.0850\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7480 - mse: 2.6695\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8552 - mse: 4.5414\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7893 - mse: 4.2278\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7904 - mse: 3.7674\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7395 - mse: 3.5722\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7842 - mse: 4.0554\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7304 - mse: 3.5858\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7000 - mse: 2.5823\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7393 - mse: 3.1045\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8078 - mse: 3.6758\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7299 - mse: 3.5687\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6374 - mse: 2.1337\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7872 - mse: 5.1058\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6893 - mse: 2.2463\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7595 - mse: 3.4586\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7676 - mse: 3.8066\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7630 - mse: 3.7069\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6544 - mse: 2.4463\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7331 - mse: 3.3330\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8022 - mse: 3.6302\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7646 - mse: 3.3054\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7059 - mse: 3.0116\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6835 - mse: 2.4672\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7259 - mse: 3.6638\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6236 - mse: 2.6971\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6552 - mse: 2.0568\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6939 - mse: 3.4314\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6059 - mse: 1.8589\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6602 - mse: 3.0076\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6770 - mse: 2.6465\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7157 - mse: 3.4883\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7304 - mse: 2.2842\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7218 - mse: 3.1253\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7071 - mse: 2.7641\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7042 - mse: 2.7783\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6794 - mse: 3.5004\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6217 - mse: 2.4015\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6793 - mse: 3.7984\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7246 - mse: 3.3889\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6888 - mse: 2.9026\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6910 - mse: 3.3351\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6611 - mse: 2.9867\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6222 - mse: 2.5307\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6019 - mse: 1.8765\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6475 - mse: 2.7607\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6576 - mse: 2.9774\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6598 - mse: 2.8051\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5879 - mse: 2.3284\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6517 - mse: 2.8716\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6317 - mse: 2.5706\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6735 - mse: 3.0999\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6546 - mse: 2.7201\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6503 - mse: 2.4905\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7002 - mse: 4.0425\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6397 - mse: 2.9887\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6465 - mse: 2.9248\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6952 - mse: 2.8599\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6177 - mse: 2.1124\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6895 - mse: 2.7315\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7036 - mse: 3.4849\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6309 - mse: 2.6309\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6110 - mse: 1.8140\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6601 - mse: 2.7041\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6080 - mse: 2.1274\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6009 - mse: 1.9751\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5644 - mse: 2.1363\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6044 - mse: 1.8903\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5922 - mse: 2.1367\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5476 - mse: 1.7780\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6029 - mse: 2.8048\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6237 - mse: 2.5468\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6114 - mse: 2.1683\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6547 - mse: 2.7141\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6148 - mse: 2.4678\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5362 - mse: 1.3528\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5668 - mse: 2.2673\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5729 - mse: 2.3929\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5437 - mse: 1.8643\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5973 - mse: 2.1241\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6020 - mse: 2.9382\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5931 - mse: 2.6520\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5250 - mse: 1.6979\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5399 - mse: 1.7432\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6636 - mse: 3.2080\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5097 - mse: 1.8419\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5758 - mse: 2.5427\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5461 - mse: 1.6421\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6122 - mse: 2.3178\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5803 - mse: 2.3048\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5839 - mse: 2.0455\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5741 - mse: 2.6372\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5586 - mse: 2.1959\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5754 - mse: 2.0391\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5543 - mse: 1.8112\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5346 - mse: 1.9498\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5762 - mse: 2.2934\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4678 - mse: 8.9170\n",
            "Mean Absolute Error, Mean Squared Error  [1.4677588939666748, 8.916979789733887]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error  [11.123788833618164, 4236.31884765625]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XEZXr7Pwzhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88626950-81c0-41a7-b7b8-45193bea7ce9"
      },
      "source": [
        "# DistilBERT\n",
        "run_model((768,), distilbert_embeddings_lotion, train_labels, distilbert_embeddings_lotion_test, test_labels, 200)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error  [6.725437641143799, 338.0096740722656]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.6234 - mse: 8.6808\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5028 - mse: 10.5511\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3653 - mse: 8.7254\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4152 - mse: 8.8881\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4287 - mse: 9.1792\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4755 - mse: 9.4612\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3899 - mse: 8.1886\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3481 - mse: 8.0312\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3980 - mse: 9.8459\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3723 - mse: 8.2716\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3502 - mse: 8.5480\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3668 - mse: 8.1745\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3476 - mse: 9.7585\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3083 - mse: 8.0857\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3562 - mse: 9.0427\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4239 - mse: 9.7605\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3165 - mse: 8.2278\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2133 - mse: 6.7302\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2683 - mse: 7.7831\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1808 - mse: 6.1343\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3570 - mse: 9.3888\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3108 - mse: 9.1630\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2541 - mse: 7.6187\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3254 - mse: 8.7808\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3644 - mse: 9.6354\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2808 - mse: 8.1411\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2480 - mse: 7.7200\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2020 - mse: 6.7623\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2258 - mse: 7.1368\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2661 - mse: 8.5649\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2033 - mse: 7.1836\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2266 - mse: 7.2891\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1560 - mse: 7.2611\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1236 - mse: 6.1074\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1507 - mse: 6.0839\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1712 - mse: 6.6307\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2119 - mse: 8.3047\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2625 - mse: 7.9186\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2147 - mse: 8.4885\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1094 - mse: 6.2653\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1633 - mse: 6.9481\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2424 - mse: 9.0185\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0843 - mse: 5.7621\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1154 - mse: 6.1598\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0872 - mse: 5.8529\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2192 - mse: 8.6991\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1677 - mse: 8.3705\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1105 - mse: 6.2830\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0723 - mse: 6.1393\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1180 - mse: 7.1100\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0516 - mse: 5.9533\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0745 - mse: 6.7687\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1314 - mse: 7.4232\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1287 - mse: 7.6398\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0811 - mse: 6.5512\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9976 - mse: 5.9339\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1362 - mse: 7.5958\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9820 - mse: 5.1296\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0923 - mse: 7.0268\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0020 - mse: 5.2075\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9901 - mse: 6.1735\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9806 - mse: 6.0653\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9981 - mse: 5.3415\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0505 - mse: 6.1140\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0257 - mse: 6.4678\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9338 - mse: 5.0558\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0873 - mse: 6.8371\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9297 - mse: 5.0783\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9345 - mse: 5.1014\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9996 - mse: 6.0216\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9407 - mse: 4.7795\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0095 - mse: 6.1897\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9719 - mse: 5.4666\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0725 - mse: 7.5748\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9057 - mse: 4.6496\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8921 - mse: 4.8561\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9513 - mse: 4.8734\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9517 - mse: 5.4096\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8389 - mse: 3.6587\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8408 - mse: 4.3968\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8648 - mse: 4.6122\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9676 - mse: 6.1027\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8863 - mse: 4.3634\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8962 - mse: 4.4666\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8656 - mse: 4.1156\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8366 - mse: 3.9246\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8506 - mse: 4.2201\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8255 - mse: 4.2049\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7920 - mse: 3.6152\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9933 - mse: 5.6563\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8596 - mse: 4.6488\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7414 - mse: 3.1848\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8166 - mse: 4.1119\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8244 - mse: 4.2040\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8670 - mse: 5.6370\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8886 - mse: 5.6058\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8808 - mse: 5.4602\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8777 - mse: 5.1185\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8636 - mse: 4.6426\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9107 - mse: 5.8773\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8095 - mse: 4.2499\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8091 - mse: 3.5312\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8712 - mse: 4.6737\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8545 - mse: 3.7243\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8994 - mse: 5.8794\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8549 - mse: 4.4852\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8931 - mse: 5.0746\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7737 - mse: 3.4425\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8131 - mse: 4.4931\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8228 - mse: 5.3973\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8213 - mse: 4.1353\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8930 - mse: 5.2424\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8297 - mse: 4.2088\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8210 - mse: 5.4397\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7418 - mse: 3.9182\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8544 - mse: 4.1408\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7928 - mse: 4.6269\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7763 - mse: 3.6428\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7744 - mse: 3.9076\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6852 - mse: 2.6133\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7109 - mse: 3.1492\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7593 - mse: 5.0757\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7275 - mse: 2.9950\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7048 - mse: 3.1263\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7078 - mse: 3.7060\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6367 - mse: 2.5487\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7305 - mse: 3.5524\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7179 - mse: 2.9217\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7403 - mse: 3.9095\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7511 - mse: 3.8979\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7055 - mse: 3.2606\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7261 - mse: 4.1490\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6704 - mse: 2.6287\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6996 - mse: 3.3701\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7690 - mse: 3.7889\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7259 - mse: 3.9952\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6993 - mse: 2.9806\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6311 - mse: 2.5942\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6708 - mse: 2.9599\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6432 - mse: 3.0425\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6289 - mse: 2.3317\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6450 - mse: 2.5350\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6916 - mse: 2.8680\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6251 - mse: 2.7756\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6353 - mse: 2.7052\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6898 - mse: 3.0300\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7011 - mse: 3.1670\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7052 - mse: 2.8894\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6508 - mse: 2.9279\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7045 - mse: 3.6663\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6249 - mse: 2.6304\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6548 - mse: 2.9180\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6691 - mse: 2.8925\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6777 - mse: 3.6082\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6430 - mse: 2.6816\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6839 - mse: 3.6809\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6874 - mse: 3.4992\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7069 - mse: 3.8522\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5992 - mse: 1.7710\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6498 - mse: 2.8919\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6249 - mse: 2.7245\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5940 - mse: 2.3958\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5749 - mse: 2.1671\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6813 - mse: 3.5591\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6592 - mse: 2.6300\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6867 - mse: 2.3046\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7085 - mse: 2.9296\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6904 - mse: 2.5869\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5943 - mse: 1.8178\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6880 - mse: 2.5472\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6833 - mse: 2.5203\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7052 - mse: 3.6812\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6288 - mse: 2.0586\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6934 - mse: 3.0209\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5623 - mse: 1.7915\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6780 - mse: 2.6727\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6397 - mse: 2.4566\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6625 - mse: 2.7015\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5942 - mse: 1.8738\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6472 - mse: 3.1794\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7074 - mse: 3.4826\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5919 - mse: 1.8553\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6345 - mse: 2.1936\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6233 - mse: 2.4571\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5835 - mse: 2.4329\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6246 - mse: 2.5664\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6028 - mse: 2.3656\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5988 - mse: 2.4101\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5917 - mse: 2.4415\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6459 - mse: 2.5933\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6272 - mse: 2.5403\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6099 - mse: 2.2115\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6245 - mse: 2.3154\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5885 - mse: 2.0395\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6052 - mse: 1.8701\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5359 - mse: 1.4596\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6526 - mse: 2.8954\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5696 - mse: 1.9181\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5938 - mse: 1.6725\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5675 - mse: 1.6599\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5081 - mse: 8.6911\n",
            "Mean Absolute Error, Mean Squared Error  [1.5081337690353394, 8.691109657287598]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error  [6.725437641143799, 338.0096740722656]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw6JSDpMw05Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a82b569-e52a-46ee-e311-a7bd0c69f379"
      },
      "source": [
        "# RoBERTa\n",
        "run_model((768,), roberta_embeddings_lotion, train_labels, roberta_embeddings_lotion_test, test_labels, 200)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error  [6.730209827423096, 325.2977600097656]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.8138 - mse: 12.6301\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3500 - mse: 7.1661\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4314 - mse: 8.5962\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4618 - mse: 9.4439\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3291 - mse: 7.2046\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4178 - mse: 9.0179\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3532 - mse: 8.0313\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3350 - mse: 8.3150\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3957 - mse: 9.3645\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3641 - mse: 8.9467\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3998 - mse: 9.0519\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2712 - mse: 7.4585\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3727 - mse: 8.3286\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3365 - mse: 8.8128\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 9.2178\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1932 - mse: 6.6050\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2647 - mse: 7.3366\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3095 - mse: 9.2712\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2583 - mse: 7.1087\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3292 - mse: 8.3180\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3716 - mse: 10.2672\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2251 - mse: 7.2648\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2912 - mse: 8.7955\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3241 - mse: 9.1056\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2750 - mse: 7.5513\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2733 - mse: 8.4233\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2524 - mse: 8.6305\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2251 - mse: 7.6566\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1175 - mse: 6.0832\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2903 - mse: 9.3179\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1291 - mse: 6.0836\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1648 - mse: 6.8708\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2608 - mse: 8.2093\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2138 - mse: 9.3317\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1061 - mse: 5.3399\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0962 - mse: 6.0894\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1709 - mse: 6.8167\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0600 - mse: 5.3850\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1202 - mse: 6.7036\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0462 - mse: 5.6067\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0865 - mse: 6.7861\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0868 - mse: 6.5507\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1087 - mse: 7.0407\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0421 - mse: 6.7753\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9737 - mse: 5.5328\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9858 - mse: 4.9886\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0853 - mse: 7.4359\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0183 - mse: 6.3070\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0347 - mse: 6.0191\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0001 - mse: 6.1580\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0340 - mse: 6.2742\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9729 - mse: 5.1181\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9133 - mse: 4.8683\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9542 - mse: 5.7684\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9377 - mse: 5.3400\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9337 - mse: 4.9956\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0018 - mse: 6.8091\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9096 - mse: 4.5385\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8962 - mse: 5.2800\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8975 - mse: 5.4067\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9297 - mse: 5.5708\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8522 - mse: 4.3068\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8724 - mse: 5.2754\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8477 - mse: 4.6449\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8337 - mse: 4.3702\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8460 - mse: 4.3897\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8053 - mse: 4.0296\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9088 - mse: 6.0519\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8841 - mse: 4.9072\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8459 - mse: 4.6949\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8370 - mse: 4.3721\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8165 - mse: 4.2564\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7540 - mse: 3.8293\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8303 - mse: 4.3219\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7935 - mse: 3.9589\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7602 - mse: 4.0010\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8400 - mse: 5.0773\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7743 - mse: 4.1267\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7846 - mse: 3.7821\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7739 - mse: 3.6416\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7207 - mse: 2.9280\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7706 - mse: 4.4577\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7954 - mse: 4.8038\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7513 - mse: 4.2281\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7348 - mse: 4.1317\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7511 - mse: 3.6492\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7331 - mse: 3.9077\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6462 - mse: 2.5627\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6540 - mse: 3.1417\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7018 - mse: 3.5877\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7572 - mse: 4.2519\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7868 - mse: 4.0199\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7467 - mse: 3.5584\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6554 - mse: 2.9091\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6999 - mse: 3.0821\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6182 - mse: 2.6013\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7089 - mse: 3.4140\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6906 - mse: 3.3605\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6240 - mse: 2.2555\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6629 - mse: 2.6572\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6717 - mse: 3.0910\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7144 - mse: 3.5966\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6103 - mse: 2.2174\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6006 - mse: 1.9866\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5599 - mse: 2.0009\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6191 - mse: 2.8487\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6120 - mse: 2.6245\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5629 - mse: 2.0298\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6086 - mse: 2.4378\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5928 - mse: 2.3072\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5783 - mse: 2.3538\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5729 - mse: 2.0949\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5858 - mse: 2.0050\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5880 - mse: 2.2373\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5563 - mse: 2.1738\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6053 - mse: 3.2680\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5556 - mse: 1.8161\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5284 - mse: 1.6484\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5843 - mse: 2.3163\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5586 - mse: 1.9966\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5726 - mse: 1.9780\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5298 - mse: 1.8242\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5821 - mse: 1.8620\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5127 - mse: 1.3703\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5592 - mse: 2.0936\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5182 - mse: 1.7948\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5009 - mse: 1.8072\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5392 - mse: 1.8802\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5758 - mse: 2.6720\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5464 - mse: 2.0683\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5359 - mse: 1.7175\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5252 - mse: 1.6681\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4913 - mse: 1.8085\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4730 - mse: 1.3509\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5271 - mse: 1.5445\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5074 - mse: 1.3484\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4874 - mse: 1.5567\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 1.2366\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4753 - mse: 1.3864\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4994 - mse: 1.5268\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4917 - mse: 1.5995\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4506 - mse: 0.8881\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5690 - mse: 2.5617\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5267 - mse: 2.0013\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4731 - mse: 1.4368\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4756 - mse: 1.3212\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4503 - mse: 1.3229\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4512 - mse: 1.0963\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4725 - mse: 1.5458\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4541 - mse: 1.3424\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 1.8536\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4442 - mse: 1.3956\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4858 - mse: 1.1198\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5091 - mse: 2.2101\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4632 - mse: 1.2438\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5003 - mse: 1.5423\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4342 - mse: 1.2373\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4749 - mse: 1.3853\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5272 - mse: 1.6857\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4304 - mse: 0.9936\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4467 - mse: 0.9993\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4728 - mse: 1.3910\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4289 - mse: 1.1052\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4374 - mse: 0.9747\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4314 - mse: 1.2346\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4479 - mse: 0.8891\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4781 - mse: 1.9307\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5390 - mse: 1.6941\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 1.2747\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4140 - mse: 1.2133\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4295 - mse: 0.9924\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4997 - mse: 1.7983\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4151 - mse: 0.9847\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4474 - mse: 0.7912\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4354 - mse: 1.4410\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3992 - mse: 1.0322\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4512 - mse: 1.6235\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4796 - mse: 1.4436\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4671 - mse: 1.7730\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3851 - mse: 0.6700\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4140 - mse: 1.0499\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4278 - mse: 1.0050\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4419 - mse: 1.3067\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4346 - mse: 1.3703\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4440 - mse: 1.5155\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3665 - mse: 0.9052\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3941 - mse: 0.6683\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4162 - mse: 0.9735\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4111 - mse: 0.8471\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4020 - mse: 0.8904\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4068 - mse: 0.8814\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3937 - mse: 0.8460\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4305 - mse: 1.1776\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4358 - mse: 1.9269\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3372 - mse: 0.6323\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4861 - mse: 1.5274\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3792 - mse: 0.8657\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3801 - mse: 1.0261\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3969 - mse: 0.8201\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4127 - mse: 1.1576\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.7776 - mse: 12.3143\n",
            "Mean Absolute Error, Mean Squared Error  [1.777553915977478, 12.314288139343262]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error  [6.730209827423096, 325.2977600097656]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ5XoQonw1BV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f50ffd39-51f1-4005-a22c-64132ce3236f"
      },
      "source": [
        "# BERT\n",
        "run_model((768,), bert_embeddings_lotion, train_labels, bert_embeddings_lotion_test, test_labels, 200)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error  [5.8421783447265625, 309.8191223144531]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.6878 - mse: 9.2238\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4326 - mse: 8.0591\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3536 - mse: 8.3371\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3021 - mse: 6.5223\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2780 - mse: 6.8436\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3567 - mse: 8.3212\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3289 - mse: 8.6539\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2758 - mse: 7.3302\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3053 - mse: 7.7325\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1910 - mse: 6.3230\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2690 - mse: 7.1510\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3785 - mse: 8.8701\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2633 - mse: 7.2939\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4570 - mse: 9.5841\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3330 - mse: 8.6060\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 6.9720\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1990 - mse: 6.6713\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2023 - mse: 7.4161\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2212 - mse: 7.8172\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2321 - mse: 7.7395\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2148 - mse: 7.7448\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2273 - mse: 7.3739\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1816 - mse: 7.3586\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1221 - mse: 6.9120\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2273 - mse: 8.0268\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1308 - mse: 7.3224\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1617 - mse: 7.3932\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1204 - mse: 6.3342\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0246 - mse: 5.9240\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0386 - mse: 5.8354\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0540 - mse: 5.2020\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1081 - mse: 6.8789\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1308 - mse: 7.7538\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0327 - mse: 5.6418\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0943 - mse: 5.7362\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0243 - mse: 5.8879\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0831 - mse: 6.8684\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9282 - mse: 4.3616\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0928 - mse: 6.4187\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0527 - mse: 6.3685\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0589 - mse: 6.3803\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9428 - mse: 5.1358\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0065 - mse: 6.1909\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9102 - mse: 5.7258\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9458 - mse: 4.4456\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9151 - mse: 4.6434\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9277 - mse: 5.1425\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9579 - mse: 6.2429\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9305 - mse: 4.7844\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8808 - mse: 4.4307\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9091 - mse: 5.2291\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8426 - mse: 4.2724\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9275 - mse: 4.9124\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9303 - mse: 5.4235\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8101 - mse: 3.2962\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8583 - mse: 4.7813\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9045 - mse: 5.6718\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8088 - mse: 3.6968\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7844 - mse: 3.9388\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7875 - mse: 3.6667\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7987 - mse: 3.9072\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8151 - mse: 4.3409\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7519 - mse: 2.8475\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8058 - mse: 3.7042\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7452 - mse: 2.8729\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7040 - mse: 2.5197\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7567 - mse: 3.0379\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8484 - mse: 5.6949\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8105 - mse: 3.8978\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7924 - mse: 3.5881\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6732 - mse: 2.3781\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7252 - mse: 2.6629\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7655 - mse: 3.1982\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6870 - mse: 2.2895\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7058 - mse: 3.2961\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7579 - mse: 3.9125\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7267 - mse: 2.8655\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7201 - mse: 2.5821\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7197 - mse: 2.7945\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7534 - mse: 3.6225\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7022 - mse: 2.6777\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7119 - mse: 3.5639\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6796 - mse: 3.0318\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6990 - mse: 3.4598\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7136 - mse: 3.1064\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6838 - mse: 2.8068\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6013 - mse: 2.0888\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6423 - mse: 3.0212\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6550 - mse: 2.7194\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6345 - mse: 2.1313\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6326 - mse: 2.2473\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6711 - mse: 2.9673\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5646 - mse: 1.7830\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6185 - mse: 2.8263\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6019 - mse: 1.7723\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6575 - mse: 2.9348\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6259 - mse: 2.6154\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6058 - mse: 2.1983\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6067 - mse: 2.4591\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6456 - mse: 2.6405\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5811 - mse: 2.2253\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6271 - mse: 1.8190\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6529 - mse: 2.8284\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5207 - mse: 1.5359\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6674 - mse: 2.2745\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5582 - mse: 1.9635\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5439 - mse: 1.8273\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6782 - mse: 3.7677\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5944 - mse: 2.3768\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5690 - mse: 2.0177\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5072 - mse: 1.3705\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5809 - mse: 2.4005\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5901 - mse: 2.1649\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5200 - mse: 1.5587\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4988 - mse: 1.4014\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5591 - mse: 2.2366\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5703 - mse: 2.0284\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5270 - mse: 1.9024\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4972 - mse: 1.3494\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5176 - mse: 1.2887\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5764 - mse: 2.4222\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5177 - mse: 1.5244\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5262 - mse: 1.8265\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5250 - mse: 1.5312\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5142 - mse: 2.1006\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5781 - mse: 2.6142\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5388 - mse: 2.4978\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6002 - mse: 2.4340\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4701 - mse: 1.0674\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5271 - mse: 1.7586\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4906 - mse: 1.7563\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5438 - mse: 2.4655\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5340 - mse: 1.6450\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5366 - mse: 1.7302\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4572 - mse: 1.3441\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5633 - mse: 2.5019\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4789 - mse: 1.1686\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4960 - mse: 1.0079\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4541 - mse: 1.2047\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5122 - mse: 1.7020\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5232 - mse: 1.5226\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5020 - mse: 1.5634\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4246 - mse: 0.7388\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4811 - mse: 1.3467\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4920 - mse: 1.4583\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4644 - mse: 1.0913\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4926 - mse: 2.0091\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4260 - mse: 0.7793\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4846 - mse: 1.1526\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4014 - mse: 0.8733\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4681 - mse: 1.3163\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4681 - mse: 1.2999\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4735 - mse: 1.2184\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 1.1503\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4598 - mse: 1.4235\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4827 - mse: 1.4391\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4901 - mse: 2.4671\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5708 - mse: 2.3079\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4698 - mse: 1.6936\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4375 - mse: 1.1965\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4657 - mse: 1.4462\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4456 - mse: 1.1237\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4856 - mse: 1.4074\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4579 - mse: 1.4161\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4395 - mse: 1.1919\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4676 - mse: 1.2633\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4642 - mse: 1.8738\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4504 - mse: 1.3221\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4575 - mse: 1.3372\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4201 - mse: 1.1074\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3957 - mse: 0.8527\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4167 - mse: 0.9409\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 1.5204\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4144 - mse: 0.8533\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3991 - mse: 0.8062\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4319 - mse: 0.9059\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4277 - mse: 0.8510\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4146 - mse: 0.8444\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4722 - mse: 1.3892\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4663 - mse: 1.5152\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4847 - mse: 1.5939\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4141 - mse: 1.2602\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3913 - mse: 0.5931\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4343 - mse: 1.2870\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4257 - mse: 1.4870\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4381 - mse: 1.2169\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3944 - mse: 0.6937\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4200 - mse: 0.9476\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3713 - mse: 0.9405\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4416 - mse: 1.1948\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4110 - mse: 1.0278\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4589 - mse: 1.5098\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4195 - mse: 1.2128\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4095 - mse: 1.0515\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4232 - mse: 1.1592\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4422 - mse: 0.8935\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4092 - mse: 0.7108\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4162 - mse: 0.7987\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4722 - mse: 1.9733\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4033 - mse: 0.9332\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5056 - mse: 9.4881\n",
            "Mean Absolute Error, Mean Squared Error  [1.5056098699569702, 9.488116264343262]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error  [5.8421783447265625, 309.8191223144531]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFyhFSbmoI_a"
      },
      "source": [
        "### Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ynwo-0hDOqM"
      },
      "source": [
        "def train_cnn(embeddings_train, train_labels, epochs=100):    \n",
        "  # Specify model hyperparameters.\n",
        "    num_filters = [4, 4, 4, 4]\n",
        "    kernel_sizes= [4, 2, 3, 4]\n",
        "    dense_layer_dims = [64,64]\n",
        "    dropout_rate = 0.85\n",
        " \n",
        "    # Input is a special \"layer\".  It defines a placeholder that will be overwritten by the training data.\n",
        "    word_embeddings = keras.layers.Input(shape=(embeddings_train.shape[1],1))\n",
        "\n",
        "    # Construct \"filters\" randomly initialized filters with dimension \"kernel_size\" for each size of filter we want.\n",
        "    conv_layers_for_all_kernel_sizes = []\n",
        "    for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
        "        conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(word_embeddings)\n",
        "        conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
        "        conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
        "\n",
        "    # Concat the feature maps from each different size.\n",
        "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
        "\n",
        "    # Add dropout to improve generalization\n",
        "    h = keras.layers.Dropout(rate=dropout_rate)(h)\n",
        "\n",
        "    # A fully connected layer for each dense layer dimension in dense_layer_dims.\n",
        "    for dense_layer_dimension in dense_layer_dims:\n",
        "        keras.layers.Dense(dense_layer_dimension, activation='relu')\n",
        "\n",
        "    prediction = keras.layers.Dense(1)(h)\n",
        "\n",
        "    model = keras.Model(inputs=word_embeddings, outputs=prediction)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='mean_absolute_error',  \n",
        "                  metrics=['mse'])        \n",
        "    model.reset_states()\n",
        "    model.fit(embeddings_train, train_labels, epochs=epochs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkOE6zY6DYBf"
      },
      "source": [
        "# Explore Hyperparameters \n",
        "def explore_hyper_parameters():\n",
        "  dropout_rates_list = []\n",
        "  num_filters_list = []\n",
        "  kernel_sizes_list = []\n",
        "  results_list = []\n",
        "  for i in range(10):\n",
        "      print(\"Run: \", i)\n",
        "      dropout_rate = random.uniform(0, 1)\n",
        "      num = np.random.randint(3,5)\n",
        "      num_filters = [np.random.randint(3,8)]*num\n",
        "      kernel_sizes = []\n",
        "      for i in range(num):\n",
        "        kernel_sizes.append(np.random.randint(2,5))\n",
        "      print(num, num_filters, kernel_sizes)\n",
        "      model = train_cnn(bert_embeddings_hair,train_labels, dropout_rate, kernel_sizes, num_filters)\n",
        "      results = model.evaluate(bert_embeddings_hair_test, test_labels)\n",
        "      dropout_rates_list.append(dropout_rate)\n",
        "      num_filters_list.append(num_filters)\n",
        "      kernel_sizes_list.append(kernel_sizes)\n",
        "      results_list.append(results)\n",
        "\n",
        "      \n",
        "  for i in range(30):\n",
        "    print(dropout_rates_list[i],num_filters_list[i],kernel_sizes_list[i],results_list[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaxE-_I7OqA9",
        "outputId": "a166eb9e-2254-4ebd-8462-b52b6cc5a8a5"
      },
      "source": [
        "type(train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sYQ-dXcEuuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1359326-ba8e-4c94-b1b4-581676f74c60"
      },
      "source": [
        "# ELMO\n",
        "cnn = train_cnn(elmo_embeddings_lotion, np.array(train_labels))\n",
        "results = cnn.evaluate(elmo_embeddings_lotion_test, test_labels)\n",
        "print(\"Mean Absolute Error, Mean Squared Error: \", results)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error:  [1.4698753356933594, 9.138339042663574]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 1s 4ms/step - loss: 3.3254 - mse: 20.2459\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.7130 - mse: 14.9390\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.3948 - mse: 13.5749\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.1293 - mse: 12.1197\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.9822 - mse: 11.3006\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.7590 - mse: 9.3315\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.8053 - mse: 10.2902\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.8461 - mse: 14.6861\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6803 - mse: 10.6584\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6155 - mse: 8.9375\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6303 - mse: 9.8424\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5733 - mse: 9.0162\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6951 - mse: 10.4396\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5840 - mse: 10.4902\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6162 - mse: 10.8477\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6447 - mse: 10.4119\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5711 - mse: 10.5660\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5172 - mse: 8.1865\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5204 - mse: 8.8754\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5835 - mse: 10.0955\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4564 - mse: 8.4105\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4259 - mse: 7.4814\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5539 - mse: 9.8891\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4840 - mse: 8.5761\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5407 - mse: 9.7212\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5413 - mse: 10.2978\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4217 - mse: 8.2081\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4460 - mse: 8.5682\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4179 - mse: 8.5777\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4502 - mse: 8.6362\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5491 - mse: 11.3274\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4228 - mse: 9.0948\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3831 - mse: 8.1185\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4217 - mse: 9.3941\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4115 - mse: 8.9613\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4765 - mse: 9.5691\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3619 - mse: 8.2094\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4643 - mse: 9.6076\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3097 - mse: 7.0471\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4554 - mse: 9.9557\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4530 - mse: 9.9161\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4205 - mse: 8.5464\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4205 - mse: 8.9178\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3870 - mse: 8.6733\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4397 - mse: 8.9949\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3648 - mse: 8.3408\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3461 - mse: 8.1043\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3956 - mse: 8.6822\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4545 - mse: 9.2734\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3564 - mse: 7.6103\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3379 - mse: 6.8577\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4070 - mse: 8.5708\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4140 - mse: 8.9650\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3525 - mse: 7.8886\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4350 - mse: 9.5169\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3922 - mse: 8.2284\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3609 - mse: 7.9411\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3851 - mse: 8.1723\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4686 - mse: 9.5129\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4085 - mse: 7.8638\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4545 - mse: 9.2983\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3385 - mse: 7.7300\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4333 - mse: 9.3641\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3566 - mse: 7.9323\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3967 - mse: 7.9721\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4376 - mse: 9.0295\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4424 - mse: 9.6286\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4951 - mse: 10.4327\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3958 - mse: 9.1059\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4552 - mse: 9.0489\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4237 - mse: 8.9305\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3109 - mse: 7.1705\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3985 - mse: 7.8091\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4143 - mse: 8.3269\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3360 - mse: 7.2181\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4735 - mse: 9.6963\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4025 - mse: 8.5706\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4757 - mse: 9.7321\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5328 - mse: 10.8365\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3993 - mse: 7.9199\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4196 - mse: 9.0406\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3555 - mse: 7.9362\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4473 - mse: 9.3186\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3742 - mse: 7.9242\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4521 - mse: 9.0038\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3295 - mse: 7.0068\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4582 - mse: 9.0293\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3924 - mse: 7.9953\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5408 - mse: 11.5681\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4046 - mse: 8.9035\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4378 - mse: 9.9269\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4337 - mse: 9.2902\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3445 - mse: 7.3943\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4013 - mse: 8.1410\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3314 - mse: 7.1978\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3348 - mse: 7.0741\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3838 - mse: 8.2979\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4420 - mse: 9.7345\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3754 - mse: 8.1402\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4042 - mse: 8.3050\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.4700 - mse: 9.1165\n",
            "Mean Absolute Error, Mean Squared Error:  [1.4699536561965942, 9.116475105285645]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error:  [1.4698753356933594, 9.138339042663574]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7pRbHekiU9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc1cfc10-53b7-407e-e0b7-f296afe0b473"
      },
      "source": [
        "# DistilBERT\n",
        "cnn = train_cnn(distilbert_embeddings_lotion, train_labels)\n",
        "results = cnn.evaluate(distilbert_embeddings_lotion_test, test_labels)\n",
        "print(\"Mean Absolute Error, Mean Squared Error: \", results)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error:  [1.4674744606018066, 9.114583015441895]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 1s 4ms/step - loss: 6.9203 - mse: 81.0167\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 5.6024 - mse: 52.7964\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 4.7530 - mse: 39.3904\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 3.8360 - mse: 26.3953\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 3.0981 - mse: 18.6499\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.6327 - mse: 14.4721\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.3534 - mse: 12.5267\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.0447 - mse: 10.4381\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.8848 - mse: 10.8266\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.8816 - mse: 12.4370\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6040 - mse: 8.5058\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6015 - mse: 9.9639\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6830 - mse: 11.5226\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5654 - mse: 7.9460\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5282 - mse: 9.0370\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5455 - mse: 9.7549\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4847 - mse: 8.5564\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4891 - mse: 9.1779\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4953 - mse: 8.7415\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6778 - mse: 12.7687\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5459 - mse: 9.8522\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3951 - mse: 7.6128\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5970 - mse: 11.9421\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4226 - mse: 7.7570\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3803 - mse: 7.4902\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4553 - mse: 8.5182\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5790 - mse: 12.3720\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4456 - mse: 9.8286\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3581 - mse: 7.1742\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4127 - mse: 7.8947\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3909 - mse: 7.7646\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3918 - mse: 7.8477\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4111 - mse: 8.7240\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4574 - mse: 9.3658\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5134 - mse: 10.4580\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4703 - mse: 9.6081\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4933 - mse: 11.4527\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3981 - mse: 8.7166\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4791 - mse: 10.1700\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.2977 - mse: 6.6704\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4685 - mse: 9.4328\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4385 - mse: 8.9021\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4260 - mse: 9.8373\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3985 - mse: 8.3440\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3849 - mse: 7.8329\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3242 - mse: 7.5310\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5518 - mse: 10.5085\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4034 - mse: 9.2132\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4251 - mse: 9.4706\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4596 - mse: 8.6705\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3860 - mse: 8.5606\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4034 - mse: 8.9150\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3992 - mse: 8.8855\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4209 - mse: 10.2427\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3522 - mse: 7.5328\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5277 - mse: 10.0823\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4185 - mse: 9.0065\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4601 - mse: 10.0632\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4042 - mse: 8.2188\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3690 - mse: 7.3117\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4970 - mse: 9.4582\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3436 - mse: 8.1845\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3823 - mse: 8.6259\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3564 - mse: 7.7257\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4140 - mse: 8.4752\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4016 - mse: 8.7773\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3947 - mse: 9.0838\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3787 - mse: 8.1561\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4449 - mse: 9.4278\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3285 - mse: 6.8704\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3768 - mse: 7.9846\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3258 - mse: 7.7459\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5014 - mse: 9.3935\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4109 - mse: 9.2372\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3782 - mse: 7.8346\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4625 - mse: 10.0452\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4060 - mse: 8.3661\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4584 - mse: 9.7336\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3935 - mse: 8.6815\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4712 - mse: 9.8520\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3843 - mse: 8.8706\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4557 - mse: 10.0565\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5561 - mse: 11.9004\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3976 - mse: 7.7853\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3968 - mse: 9.1301\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4022 - mse: 8.2076\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3267 - mse: 7.4023\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5232 - mse: 10.1828\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4297 - mse: 8.7874\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3484 - mse: 7.8566\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4482 - mse: 9.8365\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3639 - mse: 8.2595\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4207 - mse: 8.8832\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3752 - mse: 7.8706\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3238 - mse: 7.2410\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4161 - mse: 8.2940\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4355 - mse: 9.1822\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3879 - mse: 7.9570\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4281 - mse: 8.5829\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4153 - mse: 8.7674\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.4675 - mse: 9.1146\n",
            "Mean Absolute Error, Mean Squared Error:  [1.4674744606018066, 9.114583015441895]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error:  [5.8317155838012695, 328.1436767578125]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzt2rCf4idcB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1c7a3dd-6f1c-4dbd-938d-ebd3466d4855"
      },
      "source": [
        "# RoBERTa\n",
        "cnn = train_cnn(roberta_embeddings_lotion, train_labels)\n",
        "results = cnn.evaluate(roberta_embeddings_lotion_test, test_labels)\n",
        "print(\"Mean Absolute Error, Mean Squared Error: \", results)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error:  [1.471461534500122, 9.153165817260742]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 1s 3ms/step - loss: 9.3051 - mse: 144.6431\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 7.6285 - mse: 94.0152\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 5.5970 - mse: 53.9058\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 4.6246 - mse: 35.6795\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 3.4907 - mse: 22.8704\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.8297 - mse: 15.8330\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.4225 - mse: 13.3116\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.1226 - mse: 11.8539\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.8570 - mse: 11.2842\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6813 - mse: 9.7157\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.7350 - mse: 10.4554\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5622 - mse: 8.9154\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5595 - mse: 9.6735\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6162 - mse: 10.8927\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6140 - mse: 10.8555\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5576 - mse: 10.5566\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5145 - mse: 8.3426\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4291 - mse: 7.4872\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4971 - mse: 9.2644\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3954 - mse: 7.9544\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4740 - mse: 8.1937\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4995 - mse: 9.4240\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5014 - mse: 9.5827\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4598 - mse: 8.5093\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5462 - mse: 10.6548\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4809 - mse: 9.8257\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3914 - mse: 6.9959\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3010 - mse: 6.6122\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3308 - mse: 7.1550\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4358 - mse: 8.6267\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4816 - mse: 9.9506\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3880 - mse: 8.4293\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4464 - mse: 9.3081\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3958 - mse: 8.7587\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4630 - mse: 10.2907\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4339 - mse: 9.4278\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4782 - mse: 10.1914\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4821 - mse: 9.7425\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4287 - mse: 8.4534\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4024 - mse: 8.7180\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4935 - mse: 9.3485\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4918 - mse: 9.8407\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3708 - mse: 8.7116\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3684 - mse: 8.4600\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4986 - mse: 10.3158\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4862 - mse: 9.6116\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3127 - mse: 7.2710\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.2860 - mse: 7.2063\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3678 - mse: 7.8371\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4667 - mse: 9.4998\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3495 - mse: 7.4431\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5756 - mse: 12.0588\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3543 - mse: 7.5803\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3679 - mse: 7.4657\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4004 - mse: 9.3221\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4348 - mse: 10.0474\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4564 - mse: 10.6380\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4215 - mse: 8.5854\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4112 - mse: 8.3221\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3876 - mse: 7.5390\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4039 - mse: 8.5330\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4296 - mse: 9.1348\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3960 - mse: 8.4580\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4347 - mse: 8.7475\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4073 - mse: 8.1751\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3851 - mse: 7.4089\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.2641 - mse: 5.8143\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3604 - mse: 7.6925\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4222 - mse: 9.0501\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3515 - mse: 7.5884\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4477 - mse: 9.5383\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4302 - mse: 9.6171\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3077 - mse: 7.1777\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3978 - mse: 8.4205\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3775 - mse: 8.2628\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3773 - mse: 7.6700\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3065 - mse: 7.2244\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3619 - mse: 7.5456\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3833 - mse: 7.6667\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3765 - mse: 7.3914\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3748 - mse: 8.4242\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3538 - mse: 7.3493\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3995 - mse: 8.2944\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4011 - mse: 8.6701\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4719 - mse: 9.8051\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3570 - mse: 7.3644\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4914 - mse: 10.8625\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4378 - mse: 8.8241\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4126 - mse: 8.3107\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3168 - mse: 6.8323\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3537 - mse: 7.8894\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4376 - mse: 8.4638\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5118 - mse: 11.2339\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4212 - mse: 8.9437\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4259 - mse: 8.8385\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4319 - mse: 9.2527\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4726 - mse: 9.8798\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4475 - mse: 9.8048\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3935 - mse: 8.1857\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4328 - mse: 10.2596\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4715 - mse: 9.1532\n",
            "Mean Absolute Error, Mean Squared Error:  [1.471461534500122, 9.153165817260742]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error:  [5.817124843597412, 328.12060546875]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyk97vGJihN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e21db36-bebc-4930-83ad-5b759e812394"
      },
      "source": [
        "# BERT\n",
        "cnn = train_cnn(bert_embeddings_lotion, train_labels)\n",
        "results = cnn.evaluate(bert_embeddings_lotion_test, test_labels)\n",
        "print(\"Mean Absolute Error, Mean Squared Error: \", results)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error:  [1.47007417678833, 9.10583782196045]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 1s 3ms/step - loss: 8.1541 - mse: 113.9964\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 6.5697 - mse: 74.5100\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 4.9807 - mse: 44.5814\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 4.1455 - mse: 30.4882\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 3.2063 - mse: 19.3889\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.5880 - mse: 13.4496\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.3110 - mse: 12.3810\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.2133 - mse: 12.7164\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.9326 - mse: 9.7864\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.8156 - mse: 10.3183\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.8321 - mse: 11.2555\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.7441 - mse: 10.7591\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6782 - mse: 10.1619\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6365 - mse: 10.3689\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5688 - mse: 9.2562\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5778 - mse: 9.1538\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5207 - mse: 7.9952\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5483 - mse: 9.4842\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4651 - mse: 7.8066\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4834 - mse: 8.5556\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6146 - mse: 10.7105\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4548 - mse: 8.2385\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4472 - mse: 8.4882\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4865 - mse: 8.5765\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5182 - mse: 9.6068\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4062 - mse: 7.6917\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5180 - mse: 10.2458\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4398 - mse: 8.1618\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4432 - mse: 9.0730\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4194 - mse: 8.2323\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5023 - mse: 9.7841\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4352 - mse: 8.2609\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4780 - mse: 8.7736\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3684 - mse: 7.5839\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3711 - mse: 7.5798\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4017 - mse: 8.2478\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3390 - mse: 6.7867\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5157 - mse: 10.4537\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4400 - mse: 8.8824\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3740 - mse: 8.1309\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4078 - mse: 8.8728\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4869 - mse: 9.6782\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3265 - mse: 7.4855\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4069 - mse: 7.5797\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4202 - mse: 9.0909\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4933 - mse: 9.7240\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4946 - mse: 9.7708\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4300 - mse: 8.9818\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4102 - mse: 8.3004\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4531 - mse: 9.4424\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3727 - mse: 8.0815\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3709 - mse: 7.5523\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3688 - mse: 7.7367\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6313 - mse: 12.7241\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4993 - mse: 9.8159\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4493 - mse: 9.3295\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3783 - mse: 8.3424\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4228 - mse: 9.0683\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5096 - mse: 9.9009\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3624 - mse: 8.0460\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4515 - mse: 9.1798\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3203 - mse: 7.7030\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4276 - mse: 9.7545\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3867 - mse: 7.9096\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4624 - mse: 9.5789\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4633 - mse: 8.6104\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5160 - mse: 11.2919\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4078 - mse: 8.3609\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.2663 - mse: 6.1577\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4711 - mse: 9.4734\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3275 - mse: 7.3570\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4617 - mse: 9.7027\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3886 - mse: 8.8829\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3923 - mse: 8.4814\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4328 - mse: 9.7764\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3974 - mse: 7.4687\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4707 - mse: 10.0158\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3847 - mse: 8.3942\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4378 - mse: 9.6994\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4012 - mse: 8.5430\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4709 - mse: 9.3876\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3956 - mse: 8.9523\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4371 - mse: 8.9815\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3909 - mse: 8.1498\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3493 - mse: 6.5258\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3628 - mse: 7.9754\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4256 - mse: 9.0861\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3870 - mse: 8.7427\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3807 - mse: 8.6219\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4639 - mse: 9.7415\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3939 - mse: 7.8359\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3952 - mse: 9.2715\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3354 - mse: 7.6032\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4034 - mse: 8.6420\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3897 - mse: 8.4640\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3907 - mse: 8.3801\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3547 - mse: 7.9029\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3488 - mse: 7.9280\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3823 - mse: 8.7221\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3081 - mse: 6.9436\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4701 - mse: 9.1058\n",
            "Mean Absolute Error, Mean Squared Error:  [1.47007417678833, 9.10583782196045]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error:  [5.816550254821777, 328.2109375]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynM6cdd9vPdj"
      },
      "source": [
        "# Hair Product Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XQpC0C9vPdm"
      },
      "source": [
        "## Data Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0m6vEIgvPdm"
      },
      "source": [
        "# Reduce dataset to only columns we're interested in. \n",
        "hair_df = hair_df[[\"product_id\", \"product_title_cl\", 'title_cl', 'about_cl', 'description_cl', \"price_cl\", 'price_unit', 'ingredients', \"category\", \"full_text\", \"title\"]]\n",
        "# Remove all products where the unit price is NAN\n",
        "hair_df = hair_df[hair_df['price_unit'].notna()]\n",
        "# Reset the index to ensure linearity. \n",
        "hair_df = hair_df.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTfwUbrVvPdn",
        "outputId": "05faa9be-29f9-4879-de41-5da748423f67"
      },
      "source": [
        "hair_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2487, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJMm5w2ZvPdo"
      },
      "source": [
        "Remove Outliers: [Source](https://www.kite.com/python/answers/how-to-remove-outliers-from-a-pandas-dataframe-in-python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BpamwGpvPdo"
      },
      "source": [
        "# Remove Outliers \n",
        "z_scores = stats.zscore(hair_df[\"price_unit\"].values)\n",
        "abs_z_scores = np.abs(z_scores).reshape(-1,1)\n",
        "filtered_entries = (abs_z_scores < 1.7).all(axis=1)\n",
        "hair_df = hair_df[filtered_entries]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxhH6VMFvPdo",
        "outputId": "478b4662-eef1-4431-f3de-96de0e7d447d"
      },
      "source": [
        "# Number of unique product_ids.  \n",
        "len(hair_df.product_id.unique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyIFzQs4vPdp",
        "outputId": "ab8e4405-d4ff-4ebf-b1b2-ce6f57ddacea"
      },
      "source": [
        "# Examine the sub-categories in the hair dataset. \n",
        "hair_df[\"category\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shampoo        1496\n",
              "conditioner     968\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "WOgN2PibvPdp",
        "outputId": "2ae3835a-c3ac-4814-a9c2-0e6d7d8b6899"
      },
      "source": [
        "# Examine the distribution of price per unit (ounces).\n",
        "plt.hist(hair_df['price_unit'], range=[0,40])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMyj2zq9vPdq",
        "outputId": "f6784abd-eaaf-4dc3-ea2d-9fa32a533319"
      },
      "source": [
        "hair_df[\"price_unit\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2464.000000\n",
              "mean        2.489824\n",
              "std         2.857140\n",
              "min         0.017240\n",
              "25%         1.081667\n",
              "50%         1.720714\n",
              "75%         2.833885\n",
              "max        32.950000\n",
              "Name: price_unit, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBzrI7U5vPdq",
        "outputId": "4eeb8d35-0dae-4200-eb34-32f33b8ddf20"
      },
      "source": [
        "# Value to split products into lower and higher priced hair products\n",
        "cutoff_hair = hair_df[\"price_unit\"].median()\n",
        "cutoff_hair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7207142857142856"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XD2y_M7vPds"
      },
      "source": [
        "## Freedman and Jurafsky Analysis (Scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70G3gWcwvPds"
      },
      "source": [
        "### Falutin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDUjTK67vPds"
      },
      "source": [
        "Flesch Reading Score and Grade Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTlaCqG9vPds"
      },
      "source": [
        "# The flesch kincaid grade indicates the grade that a student would be able to read\n",
        "# the text. For example, a 9.3 means a ninth grader could read the text. \n",
        "hair_df[\"flesch_kincaid_grade\"] = hair_df[\"full_text\"].apply(textstat.flesch_kincaid_grade)\n",
        "\n",
        "# The flesch read ease score has no lower bound, but has an upper bound of 121.22. \n",
        "# The lower the score is, the harder the text is to read. \n",
        "hair_df[\"reading_ease\"] = hair_df[\"full_text\"].apply(textstat.flesch_reading_ease)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARpDMNhcvPds"
      },
      "source": [
        "Number of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S4AKD4GvPds"
      },
      "source": [
        "# Count the number of words present in the text (does not include punctuation by default).\n",
        "hair_df[\"lexicon_count\"] = hair_df[\"full_text\"].apply(textstat.lexicon_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQkY_eIAvPdt"
      },
      "source": [
        "Word Commonality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gSFl56zvPdt"
      },
      "source": [
        "# Raw count of marketing terms present. \n",
        "hair_df[\"marketing_term_count_raw\"] = hair_df[\"full_text\"].apply(count_marketing_terms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "L11LG7VavPdt",
        "outputId": "f517af9e-3951-4bfc-829b-ce059e776caa"
      },
      "source": [
        "# Examine falutin variables (as a whole).\n",
        "hair_df[[\"flesch_kincaid_grade\",\"reading_ease\", \"lexicon_count\",\"marketing_term_count_raw\" ]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flesch_kincaid_grade</th>\n",
              "      <th>reading_ease</th>\n",
              "      <th>lexicon_count</th>\n",
              "      <th>marketing_term_count_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2464.000000</td>\n",
              "      <td>2464.000000</td>\n",
              "      <td>2464.000000</td>\n",
              "      <td>2464.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>33.324107</td>\n",
              "      <td>-2.362658</td>\n",
              "      <td>79.426136</td>\n",
              "      <td>8.640828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>27.288936</td>\n",
              "      <td>73.259133</td>\n",
              "      <td>69.118892</td>\n",
              "      <td>8.756718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>-634.630000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>15.800000</td>\n",
              "      <td>-23.260000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>25.100000</td>\n",
              "      <td>17.340000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>40.400000</td>\n",
              "      <td>45.090000</td>\n",
              "      <td>98.250000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>276.700000</td>\n",
              "      <td>104.640000</td>\n",
              "      <td>704.000000</td>\n",
              "      <td>83.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       flesch_kincaid_grade  ...  marketing_term_count_raw\n",
              "count           2464.000000  ...               2464.000000\n",
              "mean              33.324107  ...                  8.640828\n",
              "std               27.288936  ...                  8.756718\n",
              "min                0.900000  ...                  0.000000\n",
              "25%               15.800000  ...                  3.000000\n",
              "50%               25.100000  ...                  6.000000\n",
              "75%               40.400000  ...                 12.000000\n",
              "max              276.700000  ...                 83.000000\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opNW1plivPdt"
      },
      "source": [
        "# Split dataset based on the median of the price. \n",
        "lower_priced_products = hair_df[hair_df['price_unit'] <= cutoff_hair]\n",
        "higher_priced_products = hair_df[hair_df['price_unit'] > cutoff_hair]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "v-bFMfIpvPdu",
        "outputId": "a501abfe-631d-4550-ca80-a10b6a441f3d"
      },
      "source": [
        "# Examine distribution of falutin variables for lower priced products. \n",
        "lower_priced_products[[\"flesch_kincaid_grade\",\"reading_ease\", \"lexicon_count\",\"marketing_term_count_raw\" ]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flesch_kincaid_grade</th>\n",
              "      <th>reading_ease</th>\n",
              "      <th>lexicon_count</th>\n",
              "      <th>marketing_term_count_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>34.793344</td>\n",
              "      <td>-6.378782</td>\n",
              "      <td>83.088474</td>\n",
              "      <td>9.103896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>27.358890</td>\n",
              "      <td>73.396034</td>\n",
              "      <td>69.307268</td>\n",
              "      <td>8.947300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.900000</td>\n",
              "      <td>-379.870000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>16.600000</td>\n",
              "      <td>-28.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>26.300000</td>\n",
              "      <td>14.980000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>42.300000</td>\n",
              "      <td>41.370000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>178.800000</td>\n",
              "      <td>97.540000</td>\n",
              "      <td>453.000000</td>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       flesch_kincaid_grade  ...  marketing_term_count_raw\n",
              "count           1232.000000  ...               1232.000000\n",
              "mean              34.793344  ...                  9.103896\n",
              "std               27.358890  ...                  8.947300\n",
              "min                2.900000  ...                  0.000000\n",
              "25%               16.600000  ...                  3.000000\n",
              "50%               26.300000  ...                  7.000000\n",
              "75%               42.300000  ...                 12.000000\n",
              "max              178.800000  ...                 55.000000\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "eJL-v_omvPdu",
        "outputId": "58dca8e7-9009-408b-a454-f78b50d0fcf1"
      },
      "source": [
        "# Examine distribution of falutin variabels for higher priced products. \n",
        "higher_priced_products[[\"flesch_kincaid_grade\",\"reading_ease\", \"lexicon_count\",\"marketing_term_count_raw\" ]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flesch_kincaid_grade</th>\n",
              "      <th>reading_ease</th>\n",
              "      <th>lexicon_count</th>\n",
              "      <th>marketing_term_count_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>31.854870</td>\n",
              "      <td>1.653466</td>\n",
              "      <td>75.763799</td>\n",
              "      <td>8.177760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>27.150457</td>\n",
              "      <td>72.930778</td>\n",
              "      <td>68.763208</td>\n",
              "      <td>8.540441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>-634.630000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>14.600000</td>\n",
              "      <td>-18.870000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>23.600000</td>\n",
              "      <td>20.050000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>5.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>38.875000</td>\n",
              "      <td>48.132500</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>11.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>276.700000</td>\n",
              "      <td>104.640000</td>\n",
              "      <td>704.000000</td>\n",
              "      <td>83.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       flesch_kincaid_grade  ...  marketing_term_count_raw\n",
              "count           1232.000000  ...               1232.000000\n",
              "mean              31.854870  ...                  8.177760\n",
              "std               27.150457  ...                  8.540441\n",
              "min                0.900000  ...                  0.000000\n",
              "25%               14.600000  ...                  2.000000\n",
              "50%               23.600000  ...                  5.500000\n",
              "75%               38.875000  ...                 11.000000\n",
              "max              276.700000  ...                 83.000000\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCcl0l0PvPdu"
      },
      "source": [
        "### Distinction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xo9bG5_vPdu"
      },
      "source": [
        "#### Unique and Comparator Words: Baseline\n",
        "\n",
        "As a baseline anlaysis, we hand-curated a list of words that indicate \"uniqueness\". This feature will just indicate the number of unique words present in the title."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYzKwGgEvPdu",
        "outputId": "d2ac9879-0686-44b3-f5c9-3799cab21d9d"
      },
      "source": [
        "hair_df[\"unique_word_count_basic\"] = hair_df[\"full_text\"].apply(count_unique_and_comparator_words)\n",
        "hair_df[\"unique_word_count_basic\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2464.000000\n",
              "mean        0.313718\n",
              "std         0.614646\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max         5.000000\n",
              "Name: unique_word_count_basic, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKCdeLj6vPdv"
      },
      "source": [
        "This method provided next to no signal. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Cj1MyQvPdv"
      },
      "source": [
        "#### Unique and Comparator Words: Improvement\n",
        "\n",
        "We tried two different things to improve this signal. \n",
        "\n",
        "**1. Unigrams, Bigrams, Trigrams Using a Custom Word2Vec Model**\n",
        "\n",
        "First, we extracted unigrams, bigrams and trigrams. We combined the bigrams and trigrams by underscores to create unique unigrams capturing the two or three words. Then we created our own embeddings using a custom Word2Vec model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiiL5uKzvPdv"
      },
      "source": [
        "# Extract unigrams, bigrams and trigrams. \n",
        "# Think we should remove prod from unigram/bigram/trigram.\n",
        "hair_df[\"unigrams\"] = hair_df[\"full_text\"].apply(lambda x: np.array(str.split(x)))\n",
        "hair_df[\"bigrams\"] = hair_df[\"unigrams\"].apply(nltk.bigrams).apply(list).apply(connect_by_underscore)\n",
        "hair_df[\"trigrams\"] = hair_df[\"unigrams\"].apply(nltk.trigrams).apply(list).apply(connect_by_underscore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKitNEAkvPdv",
        "outputId": "cdf0aedf-b9df-4087-d3a5-180cfe62952a"
      },
      "source": [
        "# Examine Unigrams\n",
        "hair_df[\"unigrams\"][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [silicon, mix, silicon, mix, bambu, extract, nutrit, prod, oz, ounc, nutrit, prod, bamboo, extract, regener, nourish, vitamin, e, c, f, carthamu, oil, almond, oil, improv, health, beauti, hair, hair, type, includ, natur, process, hair, follow, silicon, mix, bambu, treatment, good, result, work, great, brittl, dull, hair, free, residu, improv, silicon, mix, bambu, treatment, ', s, effici]\n",
              "1                                                                                                                                                                   [clay, esth, prod, ex, oz, refil, improv, health, scalp, hair, keep, scalp, healthi, remov, dirt, excess, oil, sweat, caus, clog, pore, increa, moistur, soft, loosen, tension, caus, scalp, relax, leav, hair, moistur, soft, healthi]\n",
              "2                                                                                                                      [healthi, sexi, hair, triwheat, leav, prod, oz, power, combin, soy, cocoa, work, synergist, perfectli, care, hair, prod, nourish, moistur, mild, protein, reconstructor, safe, use, colortr, hair, truli, amaz, prod, reconstruct, moistur, combat, environment, stress, type, hair]\n",
              "3                                                                                                                [rainbow, research, henna, hair, color, prod, persian, copper, oz, red, vendor, rainbow, research, rainbow, research, henna, hair, color, prod, persian, copper, red, copper, oz, make, we, color, persian, copper, rainbow, research, henna, hair, color, prod, persian, copper, oz, red]\n",
              "4                                                                                                                                                                                             [avalon, organ, strengthen, peppermint, prod, prod, set, ounc, wheat, protein, alo, vitamin, rosemari, essenti, oil, strengthen, thicken, thin, limp, lifeless, hair, creat, full, volum, healthylook, shine]\n",
              "Name: unigrams, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jNpfb28vPdv",
        "outputId": "92c36283-2619-4ea7-cd6a-a38fafc40093"
      },
      "source": [
        "# Create a single vocabulary out of all unigrams, bigrams and trigrams.\n",
        "vocab_unigrams_bigrams_trigrams = list(itertools.chain.from_iterable(list([hair_df[\"unigrams\"].values,hair_df[\"bigrams\"].values,hair_df[\"trigrams\"].values])))\n",
        "vocab_unigrams_bigrams_trigrams = list(itertools.chain.from_iterable(vocab_unigrams_bigrams_trigrams))\n",
        "print(vocab_unigrams_bigrams_trigrams[-5:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['free_prod_tec', 'prod_tec_itali', 'tec_itali_unisex', 'itali_unisex_oz', 'unisex_oz_shampoo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxWHb2khvPdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a783cd-f31b-4d15-da30-8f5abe86933e"
      },
      "source": [
        "# Create word vectors for the vocabulary using Word2Vec.\n",
        "custom_model = gensim.models.Word2Vec([vocab_unigrams_bigrams_trigrams])\n",
        "custom_model.save('custom.embedding')\n",
        "custom_model = gensim.models.Word2Vec.load('custom.embedding')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "kgTHBASxvPdw",
        "outputId": "319f073b-3ff5-41a4-9124-7e110a6ebff2"
      },
      "source": [
        "# Find the list and count of marketing terms in the product text that has \n",
        "# a high cosine similarity to the unique/custom word list. \n",
        "counts_custom, other_unique_and_comparator_words_custom = extract_similar_words(unique_and_comparator_words_cl, custom_model)\n",
        "plt.hist(counts_custom.values())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqC9aFO9vPdw"
      },
      "source": [
        "# Merge the counts back into the main df using a join on product_id.\n",
        "hair_df = hair_df.merge(pd.DataFrame(list(counts_custom.items()),columns = ['product_id','mktg_embedding_counts_custom']), how=\"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55wwn72qvPdw",
        "outputId": "3b0776e6-ad72-4b2d-f994-2f547fea036d"
      },
      "source": [
        "# Examine the distribution of the number of marketing terms present \n",
        "# (based on cosine similarity).\n",
        "hair_df['mktg_embedding_counts_custom'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2464.000000\n",
              "mean        0.409497\n",
              "std         0.896562\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max        12.000000\n",
              "Name: mktg_embedding_counts_custom, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOUtGKvHvPdw",
        "outputId": "ff8425a1-0551-4b42-950c-56cc2ba6e446"
      },
      "source": [
        "# Examine words in the text that had a high cosine similarity to the unique/comparator word list.\n",
        "list(other_unique_and_comparator_words_custom)[:min(len(other_unique_and_comparator_words_custom), 25)] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['premium',\n",
              " 'littl',\n",
              " 'new',\n",
              " 'ultra',\n",
              " 'differ',\n",
              " 'singl',\n",
              " 'less',\n",
              " 'least',\n",
              " 'special',\n",
              " 'far',\n",
              " 'uniqu']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kja7iNDmvPdw"
      },
      "source": [
        "# Split products based on the median price to analyze any significant differences \n",
        "# in the distribution of the number marketing terms present. \n",
        "lower_priced_products = hair_df[hair_df['price_unit'] <= cutoff_hair]\n",
        "higher_priced_products = hair_df[hair_df['price_unit'] > cutoff_hair]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPtjeEe1vPdx",
        "outputId": "2bcb9a4d-595e-4ce2-b592-8ceb48beea49"
      },
      "source": [
        "# Examine distribution of the number of marketing terms in the text for lower \n",
        "# priced products.\n",
        "lower_priced_products[\"mktg_embedding_counts_custom\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1232.000000\n",
              "mean        0.371753\n",
              "std         0.822973\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max         7.000000\n",
              "Name: mktg_embedding_counts_custom, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "b5O6-sqsvPdx",
        "outputId": "8f3b236a-4077-43a1-a935-df53fe4449d1"
      },
      "source": [
        "plt.hist(lower_priced_products[\"mktg_embedding_counts_custom\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryJk6RilvPdx",
        "outputId": "83a1ef70-55dc-4e79-c138-5c5e8631ab1e"
      },
      "source": [
        "# Examine distribution of the number of marketing terms in the text for higher \n",
        "# priced products. \n",
        "higher_priced_products[\"mktg_embedding_counts_custom\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1232.000000\n",
              "mean        0.447240\n",
              "std         0.963413\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         1.000000\n",
              "max        12.000000\n",
              "Name: mktg_embedding_counts_custom, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "ynP6Y2tZvPdx",
        "outputId": "257e29f1-9e56-4c69-a0cf-b3b0560d43fe"
      },
      "source": [
        "plt.hist(higher_priced_products[\"mktg_embedding_counts_custom\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTiqL6M2vPdx"
      },
      "source": [
        "**2. Pre-trained Glove Embeddings on Unigrams**\n",
        "\n",
        "The next thing we tried to do to improve on the baseline was to use only unigrams and pre-trained word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_FBLu86vPdx",
        "outputId": "c2c01888-61d6-45b5-dc1a-48c826d493b4"
      },
      "source": [
        "# Only use unigrams, since bigrams and trigrams joined on '_' won't be in vocab. \n",
        "vocab_unigrams = list(itertools.chain.from_iterable(hair_df[\"unigrams\"]))\n",
        "print(vocab_unigrams[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['silicon', 'mix', 'silicon', 'mix', 'bambu', 'extract', 'nutrit', 'prod', 'oz', 'ounc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVJreeMWvPdy",
        "outputId": "23bea4b1-d770-49a5-d5f1-e4e774a56b0c"
      },
      "source": [
        "# Extract words and the count of those words in each text using cosine similarity to\n",
        "# the hand curated unique/comparator word list. \n",
        "counts_glove, other_unique_and_comparator_words_glove = extract_similar_words(unique_and_comparator_words_cl, glove_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieeZ4f-avPdy"
      },
      "source": [
        "# Merge the counts back into the main df using a join on product_id.\n",
        "hair_df = hair_df.merge(pd.DataFrame(list(counts_glove.items()),columns = ['product_id','mktg_embedding_counts_glov']), how=\"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvpnRHDQvPdy",
        "outputId": "0ba69e95-b9b6-45b1-89a0-64b6da7b1e26"
      },
      "source": [
        "# Examine the distribution of the number of marketing terms present \n",
        "# (based on cosine similarity).\n",
        "hair_df['mktg_embedding_counts_glov'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2464.000000\n",
              "mean        9.353896\n",
              "std        12.983349\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         5.000000\n",
              "75%        12.000000\n",
              "max       108.000000\n",
              "Name: mktg_embedding_counts_glov, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAohckTTvPdy",
        "outputId": "e17d5de7-54a4-4d27-d9c7-4183fc344c77"
      },
      "source": [
        "# Examine words in the text that had a high cosine similarity to the unique/comparator word list.\n",
        "list(other_unique_and_comparator_words_glove)[:min(len(other_unique_and_comparator_words_glove), 25)] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lot',\n",
              " 'kind',\n",
              " 'two',\n",
              " 'around',\n",
              " 'half',\n",
              " 'five',\n",
              " 'differ',\n",
              " 'can',\n",
              " 'sleeker',\n",
              " 'special',\n",
              " 'help',\n",
              " 'play',\n",
              " 'job',\n",
              " 'so',\n",
              " 'bring',\n",
              " 'dead',\n",
              " 'current',\n",
              " 'go',\n",
              " 'last',\n",
              " 'million',\n",
              " 'rather',\n",
              " 'nine',\n",
              " 'give',\n",
              " 'often',\n",
              " 'come']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxgsnqKOvPdy"
      },
      "source": [
        "# Split products based on the median price to analyze any significant differences \n",
        "# in the distribution of the number marketing terms present. \n",
        "lower_priced_products = hair_df[hair_df['price_unit'] <= cutoff_hair]\n",
        "higher_priced_products = hair_df[hair_df['price_unit'] > cutoff_hair]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR2kSVisvPdz",
        "outputId": "c0fce51e-157e-4335-de3a-c33df9151ad8"
      },
      "source": [
        "# Examine distribution of the number of marketing terms in the text for lower \n",
        "# priced products.\n",
        "lower_priced_products[\"mktg_embedding_counts_glov\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1232.000000\n",
              "mean        9.920455\n",
              "std        13.303927\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         5.000000\n",
              "75%        12.000000\n",
              "max       102.000000\n",
              "Name: mktg_embedding_counts_glov, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "iltlHdNXvPdz",
        "outputId": "fc22312f-b334-4c2d-d536-7031d41e54a8"
      },
      "source": [
        "plt.hist(lower_priced_products[\"mktg_embedding_counts_glov\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A5TAaHgvPdz",
        "outputId": "5ebe95a2-6649-4161-a23d-b25659e9999b"
      },
      "source": [
        "# Examine distribution of the number of marketing terms in the text for higher \n",
        "# priced products. \n",
        "higher_priced_products[\"mktg_embedding_counts_glov\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1232.000000\n",
              "mean        8.787338\n",
              "std        12.634662\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         4.000000\n",
              "75%        12.000000\n",
              "max       108.000000\n",
              "Name: mktg_embedding_counts_glov, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "7XkzV9Y9vPdz",
        "outputId": "2b9677c7-e15e-4c8f-f205-b4d97d45b450"
      },
      "source": [
        "plt.hist(higher_priced_products[\"mktg_embedding_counts_glov\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azAioG9QvPdz"
      },
      "source": [
        "#### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "dod8GeKovPdz",
        "outputId": "ab22b73c-c57e-4489-9131-bffa6d8e83bf"
      },
      "source": [
        "# Extract sentiment scores for each text using nltk SentimentIntensityAnalyzer.\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "sentiment_df = hair_df[\"full_text\"].apply(sentiment_analyzer.polarity_scores).to_frame()\n",
        "sentiment_df  = sentiment_df[\"full_text\"].apply(pd.Series)\n",
        "# Add results to the main dataframe. \n",
        "hair_df[\"pos\"] = sentiment_df[\"pos\"]\n",
        "hair_df[\"neg\"] = sentiment_df[\"neg\"]\n",
        "hair_df[\"neu\"] = sentiment_df[\"neu\"]\n",
        "hair_df[\"compound\"] = sentiment_df[\"compound\"]\n",
        "# Examine values\n",
        "hair_df[[\"pos\",\"neg\",\"neu\",\"compound\"]].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.204</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.752</td>\n",
              "      <td>0.8689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.075</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.803</td>\n",
              "      <td>-0.2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.141</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.739</td>\n",
              "      <td>0.2263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.228</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.7003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pos    neg    neu  compound\n",
              "0  0.204  0.044  0.752    0.8689\n",
              "1  0.075  0.122  0.803   -0.2023\n",
              "2  0.141  0.120  0.739    0.2263\n",
              "3  0.000  0.000  1.000    0.0000\n",
              "4  0.228  0.000  0.772    0.7003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdJS6QgsvPd0"
      },
      "source": [
        "# Split products based on price. \n",
        "lower_priced_products = hair_df[hair_df[\"price_unit\"] <= cutoff_hair] \n",
        "higher_priced_products = hair_df[hair_df[\"price_unit\"] > cutoff_hair]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "yR225UaIvPd0",
        "outputId": "205edeb4-51cd-4fc2-a64d-393c04fd1c87"
      },
      "source": [
        "# Examine sentiment scores for the lower priced products.\n",
        "lower_priced_products[[\"pos\",\"neg\",\"neu\",\"compound\"]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.182294</td>\n",
              "      <td>0.018390</td>\n",
              "      <td>0.799310</td>\n",
              "      <td>0.714098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.102369</td>\n",
              "      <td>0.033779</td>\n",
              "      <td>0.105961</td>\n",
              "      <td>0.376015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.406000</td>\n",
              "      <td>-0.840200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>0.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.193000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.790500</td>\n",
              "      <td>0.896800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.249000</td>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.866250</td>\n",
              "      <td>0.969000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.594000</td>\n",
              "      <td>0.292000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               pos          neg          neu     compound\n",
              "count  1232.000000  1232.000000  1232.000000  1232.000000\n",
              "mean      0.182294     0.018390     0.799310     0.714098\n",
              "std       0.102369     0.033779     0.105961     0.376015\n",
              "min       0.000000     0.000000     0.406000    -0.840200\n",
              "25%       0.115000     0.000000     0.730000     0.612400\n",
              "50%       0.193000     0.000000     0.790500     0.896800\n",
              "75%       0.249000     0.028000     0.866250     0.969000\n",
              "max       0.594000     0.292000     1.000000     0.998700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "WQWrbqHevPd0",
        "outputId": "de39948c-154b-4164-e682-4947567d71eb"
      },
      "source": [
        "# Examine sentiment scores for the higher priced products. \n",
        "higher_priced_products[[\"pos\",\"neg\",\"neu\",\"compound\"]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>1232.00000</td>\n",
              "      <td>1232.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.169657</td>\n",
              "      <td>0.017519</td>\n",
              "      <td>0.81283</td>\n",
              "      <td>0.663473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.105497</td>\n",
              "      <td>0.036884</td>\n",
              "      <td>0.11027</td>\n",
              "      <td>0.393575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.44000</td>\n",
              "      <td>-0.855500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.73575</td>\n",
              "      <td>0.493900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.173000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.80800</td>\n",
              "      <td>0.848100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.245000</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>0.88500</td>\n",
              "      <td>0.957100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.329000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.999400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               pos          neg         neu     compound\n",
              "count  1232.000000  1232.000000  1232.00000  1232.000000\n",
              "mean      0.169657     0.017519     0.81283     0.663473\n",
              "std       0.105497     0.036884     0.11027     0.393575\n",
              "min       0.000000     0.000000     0.44000    -0.855500\n",
              "25%       0.100000     0.000000     0.73575     0.493900\n",
              "50%       0.173000     0.000000     0.80800     0.848100\n",
              "75%       0.245000     0.023000     0.88500     0.957100\n",
              "max       0.560000     0.329000     1.00000     0.999400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhS5MudNvPd0"
      },
      "source": [
        "### Authenticity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqQUQmEVvPd0"
      },
      "source": [
        "#### Term Frequency - Inverse Document Frequency  \n",
        "TFIDF gives a score for each token/word in 1 document of documents cominbed into one. For authenticy, we want the words and its tfidf over the whole lower price \"document\" and higher price \"document\" (lower_priced_products, higher_priced_products)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XHTu_VPvPd1"
      },
      "source": [
        "#  Create TFIDF data for lower priced hairs: titles, about, and description\n",
        "lower_title_tf = tfidf(lower_priced_products.title_cl, col_name='l_title_tfidf')\n",
        "lower_about_tf = tfidf(lower_priced_products.about_cl, col_name='l_about_tfidf')\n",
        "lower_desc_tf = tfidf(lower_priced_products.description_cl, col_name='l_desc_tfidf')\n",
        "\n",
        "#  Create TFIDF data for higher priced hairs: titles, about, and description\n",
        "higher_title_tf = tfidf(higher_priced_products.title_cl, col_name='h_title_tfidf')\n",
        "higher_about_tf = tfidf(higher_priced_products.about_cl, col_name='h_about_tfidf')\n",
        "higher_desc_tf = tfidf(higher_priced_products.description_cl, col_name='h_desc_tfidf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "4QcQ7OaCvPd1",
        "outputId": "0b372d4b-a3ec-4ca5-e65c-2d16877034ee"
      },
      "source": [
        "# Create TFIDF dataframe for lower priced products\n",
        "lower_tf = lower_title_tf.join(lower_about_tf, how='outer')\n",
        "lower_tf = lower_tf.join(lower_desc_tf, how='outer')\n",
        "lower_tf['word'] = lower_tf.index\n",
        "lower_tf.sort_values(by=['l_desc_tfidf'], ascending=False).head(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>l_title_tfidf</th>\n",
              "      <th>l_about_tfidf</th>\n",
              "      <th>l_desc_tfidf</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hair</th>\n",
              "      <td>0.243264</td>\n",
              "      <td>0.748543</td>\n",
              "      <td>0.706335</td>\n",
              "      <td>hair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prod</th>\n",
              "      <td>0.779860</td>\n",
              "      <td>0.341208</td>\n",
              "      <td>0.400664</td>\n",
              "      <td>prod</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>moistur</th>\n",
              "      <td>0.100680</td>\n",
              "      <td>0.162522</td>\n",
              "      <td>0.164803</td>\n",
              "      <td>moistur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>natur</th>\n",
              "      <td>0.069115</td>\n",
              "      <td>0.110209</td>\n",
              "      <td>0.142115</td>\n",
              "      <td>natur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oil</th>\n",
              "      <td>0.089795</td>\n",
              "      <td>0.133427</td>\n",
              "      <td>0.126657</td>\n",
              "      <td>oil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>color</th>\n",
              "      <td>0.069115</td>\n",
              "      <td>0.114912</td>\n",
              "      <td>0.123665</td>\n",
              "      <td>color</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product</th>\n",
              "      <td>0.005986</td>\n",
              "      <td>0.116675</td>\n",
              "      <td>0.122917</td>\n",
              "      <td>product</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>use</th>\n",
              "      <td>0.002177</td>\n",
              "      <td>0.104332</td>\n",
              "      <td>0.105713</td>\n",
              "      <td>use</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>help</th>\n",
              "      <td>0.004898</td>\n",
              "      <td>0.104919</td>\n",
              "      <td>0.101475</td>\n",
              "      <td>help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clean</th>\n",
              "      <td>0.019592</td>\n",
              "      <td>0.106683</td>\n",
              "      <td>0.095242</td>\n",
              "      <td>clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>scalp</th>\n",
              "      <td>0.030476</td>\n",
              "      <td>0.089049</td>\n",
              "      <td>0.091253</td>\n",
              "      <td>scalp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>formula</th>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.076412</td>\n",
              "      <td>0.089757</td>\n",
              "      <td>formula</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smooth</th>\n",
              "      <td>0.041360</td>\n",
              "      <td>0.083759</td>\n",
              "      <td>0.084022</td>\n",
              "      <td>smooth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shampoo</th>\n",
              "      <td>0.005442</td>\n",
              "      <td>0.031740</td>\n",
              "      <td>0.083773</td>\n",
              "      <td>shampoo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dri</th>\n",
              "      <td>0.039728</td>\n",
              "      <td>0.079939</td>\n",
              "      <td>0.078288</td>\n",
              "      <td>dri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz</th>\n",
              "      <td>0.394012</td>\n",
              "      <td>0.050843</td>\n",
              "      <td>0.076293</td>\n",
              "      <td>oz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shine</th>\n",
              "      <td>0.027755</td>\n",
              "      <td>0.079057</td>\n",
              "      <td>0.073800</td>\n",
              "      <td>shine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nourish</th>\n",
              "      <td>0.029932</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.070559</td>\n",
              "      <td>nourish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>damag</th>\n",
              "      <td>0.017959</td>\n",
              "      <td>0.079351</td>\n",
              "      <td>0.068813</td>\n",
              "      <td>damag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leav</th>\n",
              "      <td>0.013605</td>\n",
              "      <td>0.073179</td>\n",
              "      <td>0.068315</td>\n",
              "      <td>leav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condition</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034973</td>\n",
              "      <td>0.062830</td>\n",
              "      <td>condition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soft</th>\n",
              "      <td>0.008707</td>\n",
              "      <td>0.075530</td>\n",
              "      <td>0.062580</td>\n",
              "      <td>soft</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodi</th>\n",
              "      <td>0.038639</td>\n",
              "      <td>0.044378</td>\n",
              "      <td>0.060835</td>\n",
              "      <td>bodi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condit</th>\n",
              "      <td>0.019048</td>\n",
              "      <td>0.059660</td>\n",
              "      <td>0.060586</td>\n",
              "      <td>condit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>protect</th>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.067595</td>\n",
              "      <td>0.059588</td>\n",
              "      <td>protect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>style</th>\n",
              "      <td>0.004898</td>\n",
              "      <td>0.029389</td>\n",
              "      <td>0.058342</td>\n",
              "      <td>style</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ingredi</th>\n",
              "      <td>0.011429</td>\n",
              "      <td>0.068477</td>\n",
              "      <td>0.056597</td>\n",
              "      <td>ingredi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>care</th>\n",
              "      <td>0.033197</td>\n",
              "      <td>0.042908</td>\n",
              "      <td>0.056347</td>\n",
              "      <td>care</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>0.004898</td>\n",
              "      <td>0.075824</td>\n",
              "      <td>0.055599</td>\n",
              "      <td>make</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>size</th>\n",
              "      <td>0.005442</td>\n",
              "      <td>0.009698</td>\n",
              "      <td>0.050862</td>\n",
              "      <td>size</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           l_title_tfidf  l_about_tfidf  l_desc_tfidf       word\n",
              "hair            0.243264       0.748543      0.706335       hair\n",
              "prod            0.779860       0.341208      0.400664       prod\n",
              "moistur         0.100680       0.162522      0.164803    moistur\n",
              "natur           0.069115       0.110209      0.142115      natur\n",
              "oil             0.089795       0.133427      0.126657        oil\n",
              "color           0.069115       0.114912      0.123665      color\n",
              "product         0.005986       0.116675      0.122917    product\n",
              "use             0.002177       0.104332      0.105713        use\n",
              "help            0.004898       0.104919      0.101475       help\n",
              "clean           0.019592       0.106683      0.095242      clean\n",
              "scalp           0.030476       0.089049      0.091253      scalp\n",
              "formula         0.016871       0.076412      0.089757    formula\n",
              "smooth          0.041360       0.083759      0.084022     smooth\n",
              "shampoo         0.005442       0.031740      0.083773    shampoo\n",
              "dri             0.039728       0.079939      0.078288        dri\n",
              "oz              0.394012       0.050843      0.076293         oz\n",
              "shine           0.027755       0.079057      0.073800      shine\n",
              "nourish         0.029932       0.077881      0.070559    nourish\n",
              "damag           0.017959       0.079351      0.068813      damag\n",
              "leav            0.013605       0.073179      0.068315       leav\n",
              "condition            NaN       0.034973      0.062830  condition\n",
              "soft            0.008707       0.075530      0.062580       soft\n",
              "bodi            0.038639       0.044378      0.060835       bodi\n",
              "condit          0.019048       0.059660      0.060586     condit\n",
              "protect         0.016871       0.067595      0.059588    protect\n",
              "style           0.004898       0.029389      0.058342      style\n",
              "ingredi         0.011429       0.068477      0.056597    ingredi\n",
              "care            0.033197       0.042908      0.056347       care\n",
              "make            0.004898       0.075824      0.055599       make\n",
              "size            0.005442       0.009698      0.050862       size"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "lnU6gZsevPd1",
        "outputId": "6ece0a64-ca1e-46f2-810f-f688e6838fc0"
      },
      "source": [
        "# Create TFIDF dataframe for higher priced products\n",
        "higher_tf = higher_title_tf.join(higher_about_tf, how='outer')\n",
        "higher_tf = higher_tf.join(higher_desc_tf, how='outer')\n",
        "higher_tf['word'] = higher_tf.index\n",
        "higher_tf.sort_values(by=['h_desc_tfidf'], ascending=False).head(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>h_title_tfidf</th>\n",
              "      <th>h_about_tfidf</th>\n",
              "      <th>h_desc_tfidf</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hair</th>\n",
              "      <td>0.245107</td>\n",
              "      <td>0.761475</td>\n",
              "      <td>0.752358</td>\n",
              "      <td>hair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prod</th>\n",
              "      <td>0.758765</td>\n",
              "      <td>0.362374</td>\n",
              "      <td>0.368224</td>\n",
              "      <td>prod</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>color</th>\n",
              "      <td>0.074598</td>\n",
              "      <td>0.130687</td>\n",
              "      <td>0.136663</td>\n",
              "      <td>color</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>moistur</th>\n",
              "      <td>0.054350</td>\n",
              "      <td>0.117833</td>\n",
              "      <td>0.124162</td>\n",
              "      <td>moistur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>natur</th>\n",
              "      <td>0.058080</td>\n",
              "      <td>0.127627</td>\n",
              "      <td>0.121036</td>\n",
              "      <td>natur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oil</th>\n",
              "      <td>0.060211</td>\n",
              "      <td>0.136808</td>\n",
              "      <td>0.113649</td>\n",
              "      <td>oil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product</th>\n",
              "      <td>0.009058</td>\n",
              "      <td>0.079881</td>\n",
              "      <td>0.105410</td>\n",
              "      <td>product</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>use</th>\n",
              "      <td>0.002131</td>\n",
              "      <td>0.111099</td>\n",
              "      <td>0.104841</td>\n",
              "      <td>use</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>help</th>\n",
              "      <td>0.004263</td>\n",
              "      <td>0.097633</td>\n",
              "      <td>0.101432</td>\n",
              "      <td>help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>scalp</th>\n",
              "      <td>0.043693</td>\n",
              "      <td>0.092736</td>\n",
              "      <td>0.088931</td>\n",
              "      <td>scalp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clean</th>\n",
              "      <td>0.028773</td>\n",
              "      <td>0.089981</td>\n",
              "      <td>0.088646</td>\n",
              "      <td>clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dri</th>\n",
              "      <td>0.058080</td>\n",
              "      <td>0.083860</td>\n",
              "      <td>0.084100</td>\n",
              "      <td>dri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smooth</th>\n",
              "      <td>0.031970</td>\n",
              "      <td>0.063048</td>\n",
              "      <td>0.082396</td>\n",
              "      <td>smooth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz</th>\n",
              "      <td>0.474228</td>\n",
              "      <td>0.079575</td>\n",
              "      <td>0.075009</td>\n",
              "      <td>oz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leav</th>\n",
              "      <td>0.018649</td>\n",
              "      <td>0.067027</td>\n",
              "      <td>0.069042</td>\n",
              "      <td>leav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shine</th>\n",
              "      <td>0.021314</td>\n",
              "      <td>0.060294</td>\n",
              "      <td>0.067905</td>\n",
              "      <td>shine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>0.009058</td>\n",
              "      <td>0.069475</td>\n",
              "      <td>0.066769</td>\n",
              "      <td>make</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>protect</th>\n",
              "      <td>0.019715</td>\n",
              "      <td>0.067945</td>\n",
              "      <td>0.066201</td>\n",
              "      <td>protect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>damag</th>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.077739</td>\n",
              "      <td>0.066201</td>\n",
              "      <td>damag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shampoo</th>\n",
              "      <td>0.006927</td>\n",
              "      <td>0.037033</td>\n",
              "      <td>0.064780</td>\n",
              "      <td>shampoo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract</th>\n",
              "      <td>0.006394</td>\n",
              "      <td>0.038563</td>\n",
              "      <td>0.062791</td>\n",
              "      <td>extract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condit</th>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.053866</td>\n",
              "      <td>0.060802</td>\n",
              "      <td>condit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ingredi</th>\n",
              "      <td>0.005328</td>\n",
              "      <td>0.059988</td>\n",
              "      <td>0.060234</td>\n",
              "      <td>ingredi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condition</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.033054</td>\n",
              "      <td>0.055688</td>\n",
              "      <td>condition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>style</th>\n",
              "      <td>0.007993</td>\n",
              "      <td>0.028157</td>\n",
              "      <td>0.054836</td>\n",
              "      <td>style</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>formula</th>\n",
              "      <td>0.013854</td>\n",
              "      <td>0.063354</td>\n",
              "      <td>0.054552</td>\n",
              "      <td>formula</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soft</th>\n",
              "      <td>0.008525</td>\n",
              "      <td>0.050806</td>\n",
              "      <td>0.052847</td>\n",
              "      <td>soft</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>care</th>\n",
              "      <td>0.030905</td>\n",
              "      <td>0.048969</td>\n",
              "      <td>0.052847</td>\n",
              "      <td>care</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>protein</th>\n",
              "      <td>0.010124</td>\n",
              "      <td>0.036727</td>\n",
              "      <td>0.045744</td>\n",
              "      <td>protein</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nourish</th>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.062130</td>\n",
              "      <td>0.044891</td>\n",
              "      <td>nourish</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           h_title_tfidf  h_about_tfidf  h_desc_tfidf       word\n",
              "hair            0.245107       0.761475      0.752358       hair\n",
              "prod            0.758765       0.362374      0.368224       prod\n",
              "color           0.074598       0.130687      0.136663      color\n",
              "moistur         0.054350       0.117833      0.124162    moistur\n",
              "natur           0.058080       0.127627      0.121036      natur\n",
              "oil             0.060211       0.136808      0.113649        oil\n",
              "product         0.009058       0.079881      0.105410    product\n",
              "use             0.002131       0.111099      0.104841        use\n",
              "help            0.004263       0.097633      0.101432       help\n",
              "scalp           0.043693       0.092736      0.088931      scalp\n",
              "clean           0.028773       0.089981      0.088646      clean\n",
              "dri             0.058080       0.083860      0.084100        dri\n",
              "smooth          0.031970       0.063048      0.082396     smooth\n",
              "oz              0.474228       0.079575      0.075009         oz\n",
              "leav            0.018649       0.067027      0.069042       leav\n",
              "shine           0.021314       0.060294      0.067905      shine\n",
              "make            0.009058       0.069475      0.066769       make\n",
              "protect         0.019715       0.067945      0.066201    protect\n",
              "damag           0.020781       0.077739      0.066201      damag\n",
              "shampoo         0.006927       0.037033      0.064780    shampoo\n",
              "extract         0.006394       0.038563      0.062791    extract\n",
              "condit          0.020781       0.053866      0.060802     condit\n",
              "ingredi         0.005328       0.059988      0.060234    ingredi\n",
              "condition            NaN       0.033054      0.055688  condition\n",
              "style           0.007993       0.028157      0.054836      style\n",
              "formula         0.013854       0.063354      0.054552    formula\n",
              "soft            0.008525       0.050806      0.052847       soft\n",
              "care            0.030905       0.048969      0.052847       care\n",
              "protein         0.010124       0.036727      0.045744    protein\n",
              "nourish         0.020781       0.062130      0.044891    nourish"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "ZWUpXZg2vPd1",
        "outputId": "18cf1b4b-e352-4b09-e4cc-0aaf6661d010"
      },
      "source": [
        "# Create joined TFIDF dataframe\n",
        "tfidf_joined = lower_tf.merge(higher_tf, how='outer', on ='word')\n",
        "tfidf_joined = tfidf_joined[['word', 'l_title_tfidf', 'h_title_tfidf',\t'l_about_tfidf', 'h_about_tfidf', 'l_desc_tfidf', 'h_desc_tfidf']]\n",
        "tfidf_joined.sort_values(by='l_title_tfidf', ascending=False) # sort to see, change by accordingly "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>l_title_tfidf</th>\n",
              "      <th>h_title_tfidf</th>\n",
              "      <th>l_about_tfidf</th>\n",
              "      <th>h_about_tfidf</th>\n",
              "      <th>l_desc_tfidf</th>\n",
              "      <th>h_desc_tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3580</th>\n",
              "      <td>prod</td>\n",
              "      <td>0.779860</td>\n",
              "      <td>0.758765</td>\n",
              "      <td>0.341208</td>\n",
              "      <td>0.362374</td>\n",
              "      <td>0.400664</td>\n",
              "      <td>0.368224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3275</th>\n",
              "      <td>oz</td>\n",
              "      <td>0.394012</td>\n",
              "      <td>0.474228</td>\n",
              "      <td>0.050843</td>\n",
              "      <td>0.079575</td>\n",
              "      <td>0.076293</td>\n",
              "      <td>0.075009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3246</th>\n",
              "      <td>ounc</td>\n",
              "      <td>0.250883</td>\n",
              "      <td>0.209406</td>\n",
              "      <td>0.023805</td>\n",
              "      <td>0.031830</td>\n",
              "      <td>0.040391</td>\n",
              "      <td>0.034095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022</th>\n",
              "      <td>hair</td>\n",
              "      <td>0.243264</td>\n",
              "      <td>0.245107</td>\n",
              "      <td>0.748543</td>\n",
              "      <td>0.761475</td>\n",
              "      <td>0.706335</td>\n",
              "      <td>0.752358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1705</th>\n",
              "      <td>fl</td>\n",
              "      <td>0.115918</td>\n",
              "      <td>0.098576</td>\n",
              "      <td>0.008229</td>\n",
              "      <td>0.009488</td>\n",
              "      <td>0.019447</td>\n",
              "      <td>0.011081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7717</th>\n",
              "      <td>zingib</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7718</th>\n",
              "      <td>zip</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7719</th>\n",
              "      <td>zipper</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7720</th>\n",
              "      <td>zizyphu</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7721</th>\n",
              "      <td>zogic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7722 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         word  l_title_tfidf  ...  l_desc_tfidf  h_desc_tfidf\n",
              "3580     prod       0.779860  ...      0.400664      0.368224\n",
              "3275       oz       0.394012  ...      0.076293      0.075009\n",
              "3246     ounc       0.250883  ...      0.040391      0.034095\n",
              "2022     hair       0.243264  ...      0.706335      0.752358\n",
              "1705       fl       0.115918  ...      0.019447      0.011081\n",
              "...       ...            ...  ...           ...           ...\n",
              "7717   zingib            NaN  ...           NaN      0.000284\n",
              "7718      zip            NaN  ...           NaN           NaN\n",
              "7719   zipper            NaN  ...           NaN           NaN\n",
              "7720  zizyphu            NaN  ...           NaN      0.000284\n",
              "7721    zogic            NaN  ...           NaN           NaN\n",
              "\n",
              "[7722 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mogn25YJvPd2"
      },
      "source": [
        "### Health\n",
        "\n",
        "\n",
        "*   Ingredients from Amazon Products \n",
        "*   Toxicity Levels from California Chemicals in Cosmetics Database \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "D0QL1_nRvPd2",
        "outputId": "58baade9-c415-4b52-83f3-f89434829e03"
      },
      "source": [
        "# Check if ingredient exists in our data\n",
        "hair_df[hair_df['ingredients'].str.contains('retinol', na=False)][[\"product_title_cl\",\"ingredients\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_title_cl</th>\n",
              "      <th>ingredients</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [product_title_cl, ingredients]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Yq5CEXvPd3",
        "outputId": "aff5c941-d3d4-473b-9ba9-417104a0b877"
      },
      "source": [
        "hair_df.ingredients.unique()[0:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan,\n",
              "       'Lawsonia inermis (red henna), indigofereae (black henna) and cassia obovata (neutral henna).',\n",
              "       'Honey, Oat, Vitamin E'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ImAVLq4vPd3"
      },
      "source": [
        "# Retrieve count of toxic ingredients.\n",
        "hair_df['toxic_count'] = hair_df['ingredients'].apply(lambda x: check_toxic(x) if pd.notnull(x) else None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EsBwE8gvPd3"
      },
      "source": [
        "# Retrieve list of toxic ingredients. \n",
        "hair_df['toxic_ing'] = hair_df['ingredients'].apply(lambda x: check_toxic_ingredient(x) if pd.notnull(x) else None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrUQqWGdvPd4"
      },
      "source": [
        "# Split products based on price. \n",
        "lower_priced_products = hair_df[hair_df[\"price_unit\"] <= cutoff_hair] \n",
        "higher_priced_products = hair_df[hair_df[\"price_unit\"] > cutoff_hair]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPu7yKvsvPd4",
        "outputId": "894e4a25-01cf-4978-cbb5-c3098c844158"
      },
      "source": [
        "# Examine distribution of toxic ingredients for lower priced products. \n",
        "lower_priced_products[\"toxic_count\"].describe() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    508.000000\n",
              "mean       0.250000\n",
              "std        0.542567\n",
              "min        0.000000\n",
              "25%        0.000000\n",
              "50%        0.000000\n",
              "75%        0.000000\n",
              "max        4.000000\n",
              "Name: toxic_count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROepDoUAvPd4",
        "outputId": "ab4caad8-717a-4b53-c3f6-6f6e6d52bd6e"
      },
      "source": [
        "# Examine distribution of toxic ingredients for higher  priced products.\n",
        "higher_priced_products[\"toxic_count\"].describe() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    453.000000\n",
              "mean       0.253863\n",
              "std        0.523361\n",
              "min        0.000000\n",
              "25%        0.000000\n",
              "50%        0.000000\n",
              "75%        0.000000\n",
              "max        2.000000\n",
              "Name: toxic_count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ezbb5yHvPd4"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNvVb74_vPd4"
      },
      "source": [
        "# Truncate all text to a max length of 100\n",
        "hair_df[\"full_text_trunc\"] = hair_df[\"full_text\"].apply(truncate_or_fill)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_7FMQrRvPd4"
      },
      "source": [
        "#### Neural Network (Definition) \n",
        "\n",
        "Used for the baseline and the improvement.[link text](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5ccH4AvvPd5"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QJ-xk7NvPd5"
      },
      "source": [
        "# Split data into train and test.\n",
        "random_df = pd.DataFrame(np.random.randn(hair_df[\"full_text_trunc\"].shape[0]))\n",
        "mask = np.random.rand(len(random_df)) < 0.8\n",
        "train_features = hair_df[mask]\n",
        "train_labels = hair_df[\"price_unit\"][mask].fillna(0)\n",
        "test_features = hair_df[~mask]\n",
        "test_labels = hair_df[\"price_unit\"][~mask].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6it6SFsvPd5",
        "outputId": "45bfdba6-a488-4ede-ad58-470a043a2472"
      },
      "source": [
        "print(train_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_features.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1958, 29)\n",
            "(1958,)\n",
            "(506, 29)\n",
            "(506,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y65luvQUvPd5"
      },
      "source": [
        "### Baseline\n",
        "Run a Neural Network with features computed during the Freedman and Jurafsky Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZrBbrgjvPd5"
      },
      "source": [
        "# Scale a dataframe column\n",
        "def scale_data(data):\n",
        "  min_max_scaler = preprocessing.StandardScaler()\n",
        "  data_scaled = min_max_scaler.fit_transform(data)\n",
        "  return pd.DataFrame(data_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWeyT8fUvPd5"
      },
      "source": [
        "hair_df[\"marketing_term_count_scaled\"] = scale_data((hair_df[\"marketing_term_count_raw\"]+1).values.reshape(-1,1))\n",
        "hair_df[\"flesch_kincaid_grade_scaled\"] = scale_data((hair_df[\"flesch_kincaid_grade\"]).values.reshape(-1,1))\n",
        "hair_df[\"reading_ease_scaled\"] = scale_data((hair_df[\"reading_ease\"]).values.reshape(-1,1))\n",
        "hair_df[\"lexicon_count_scaled\"] = scale_data((hair_df[\"lexicon_count\"]).values.reshape(-1,1))\n",
        "hair_df[\"mktg_embedding_counts_glov_scaled\"] = scale_data((hair_df[\"mktg_embedding_counts_glov\"]).values.reshape(-1,1))\n",
        "hair_df[\"compound_scaled\"] = scale_data((hair_df[\"compound\"]).values.reshape(-1,1))\n",
        "hair_df[\"toxic_count_scaled\"] = scale_data((hair_df[\"toxic_count\"]).values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qltkYVbP0o5l"
      },
      "source": [
        "Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRPyTy9lvPd5"
      },
      "source": [
        "# Scaled Data\n",
        "# Mean Absolute Error, Mean Squared Error  [1.1124241352081299, 4.136312961578369]\n",
        "train = hair_df[[\"marketing_term_count_scaled\" , \"flesch_kincaid_grade_scaled\" , \"reading_ease_scaled\" , \"lexicon_count_scaled\" , \"mktg_embedding_counts_glov_scaled\" , \"compound_scaled\",\"toxic_count_scaled\"]][mask]\n",
        "test = hair_df[[\"marketing_term_count_scaled\" , \"flesch_kincaid_grade_scaled\" , \"reading_ease_scaled\" , \"lexicon_count_scaled\" , \"mktg_embedding_counts_glov_scaled\" , \"compound_scaled\",\"toxic_count_scaled\"]][~mask]\n",
        "\n",
        "# Results of using Unscaled Features. \n",
        "# Mean Absolute Error, Mean Squared Error  [1.065545678138733, 4.275307655334473]\n",
        "train = hair_df[[\"marketing_term_count_raw\" , \"flesch_kincaid_grade\" , \"reading_ease\" , \"lexicon_count\" , \"mktg_embedding_counts_glov\" , \"compound\",\"toxic_count\"]][mask]\n",
        "test = hair_df[[\"marketing_term_count_raw\" , \"flesch_kincaid_grade\" , \"reading_ease\" , \"lexicon_count\" , \"mktg_embedding_counts_glov\" , \"compound\",\"toxic_count\"]][~mask]\n",
        "\n",
        "# Scaled and neu instead of compount\n",
        "# Mean Absolute Error, Mean Squared Error  [1.0878902673721313, 4.176847457885742]\n",
        "train = hair_df[[\"marketing_term_count_scaled\" ,\"flesch_kincaid_grade_scaled\" , \"reading_ease_scaled\" , \"lexicon_count_scaled\" , \"mktg_embedding_counts_glov_scaled\" , \"neu\",\"toxic_count_scaled\"]][mask]\n",
        "test = hair_df[[\"marketing_term_count_scaled\" ,\"flesch_kincaid_grade_scaled\" , \"reading_ease_scaled\" , \"lexicon_count_scaled\" , \"mktg_embedding_counts_glov_scaled\" , \"neu\",\"toxic_count_scaled\"]][~mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28cABoOzvPd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebdc7755-199b-47ce-c682-a049e0283783"
      },
      "source": [
        "run_model((train.shape[1],), train, train_labels, test, test_labels, 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 2.4025 - mse: 14.4474\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.8594 - mse: 10.5798\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5196 - mse: 10.7446\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4704 - mse: 10.0479\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3759 - mse: 8.8936\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4217 - mse: 8.9688\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4462 - mse: 9.4850\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4765 - mse: 9.7544\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4410 - mse: 9.1129\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3789 - mse: 8.6651\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.6188 - mse: 12.3510\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4481 - mse: 8.9841\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3768 - mse: 7.7346\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4658 - mse: 9.8661\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4397 - mse: 9.5809\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5378 - mse: 12.0836\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4654 - mse: 9.6382\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3493 - mse: 7.9266\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4463 - mse: 9.4913\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4002 - mse: 8.2742\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5564 - mse: 10.8980\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4204 - mse: 9.4877\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4563 - mse: 9.7372\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4527 - mse: 9.6910\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4745 - mse: 8.9261\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3984 - mse: 8.3460\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4349 - mse: 9.3416\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4142 - mse: 8.1270\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5077 - mse: 10.8020\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4383 - mse: 9.3051\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4988 - mse: 11.0406\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5344 - mse: 11.1199\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5025 - mse: 10.3925\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4129 - mse: 9.4165\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4151 - mse: 8.3057\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4072 - mse: 7.7934\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4094 - mse: 8.5068\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5578 - mse: 12.5046\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5068 - mse: 10.4223\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4729 - mse: 9.6809\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4844 - mse: 10.6599\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4348 - mse: 9.5845\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3296 - mse: 7.2644\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4934 - mse: 10.6938\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4577 - mse: 9.7948\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3122 - mse: 7.8487\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4388 - mse: 9.6062\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4587 - mse: 9.9279\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4316 - mse: 9.4784\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4460 - mse: 9.2897\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4357 - mse: 9.9131\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4656 - mse: 10.0755\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5006 - mse: 10.5245\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4618 - mse: 9.3963\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4303 - mse: 8.3585\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5378 - mse: 10.8603\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4230 - mse: 9.2800\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4168 - mse: 8.2960\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4151 - mse: 8.3701\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4741 - mse: 10.9355\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5470 - mse: 11.3523\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4967 - mse: 11.1002\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3537 - mse: 7.2781\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5815 - mse: 12.3081\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4532 - mse: 9.5750\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4721 - mse: 9.8057\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4674 - mse: 9.7989\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4499 - mse: 9.1745\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3308 - mse: 7.1395\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4909 - mse: 9.9786\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3645 - mse: 7.7772\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3306 - mse: 6.7623\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4328 - mse: 9.3404\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5202 - mse: 10.3699\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5298 - mse: 10.9422\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4633 - mse: 10.8405\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4937 - mse: 10.1435\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3964 - mse: 8.0168\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4354 - mse: 8.4211\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4475 - mse: 8.7525\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3989 - mse: 8.6401\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3466 - mse: 7.2036\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5406 - mse: 12.3948\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3573 - mse: 7.8343\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5189 - mse: 10.6422\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4383 - mse: 9.4643\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3707 - mse: 8.0664\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4868 - mse: 10.3221\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3268 - mse: 7.3717\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4259 - mse: 8.6810\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3813 - mse: 8.8860\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4538 - mse: 9.0132\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4135 - mse: 8.5697\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5095 - mse: 10.4389\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5281 - mse: 10.4007\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4096 - mse: 8.4214\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4125 - mse: 7.9784\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5023 - mse: 9.3477\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4726 - mse: 10.3001\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3787 - mse: 7.9446\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4324 - mse: 9.3286\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3780 - mse: 8.1216\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3795 - mse: 8.3834\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4142 - mse: 8.3985\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3975 - mse: 8.3034\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3908 - mse: 8.5416\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5097 - mse: 11.0019\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3815 - mse: 8.2350\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4017 - mse: 8.7146\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4282 - mse: 8.9220\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4288 - mse: 8.6687\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5384 - mse: 11.8793\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4807 - mse: 9.8350\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3823 - mse: 8.4966\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5032 - mse: 10.0911\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4736 - mse: 10.1544\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4781 - mse: 9.5511\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4206 - mse: 8.6878\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3969 - mse: 8.4683\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3595 - mse: 7.5148\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4768 - mse: 9.6713\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4418 - mse: 9.3455\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3913 - mse: 8.2446\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4296 - mse: 8.9726\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3982 - mse: 8.0609\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4282 - mse: 8.7179\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4856 - mse: 10.6260\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3785 - mse: 8.7290\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5500 - mse: 10.7334\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4124 - mse: 8.7438\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5190 - mse: 10.6513\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3660 - mse: 8.5127\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3944 - mse: 8.5886\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4858 - mse: 9.7893\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4178 - mse: 9.4452\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3468 - mse: 7.9798\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4579 - mse: 9.2556\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4877 - mse: 10.0517\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3748 - mse: 8.3389\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4481 - mse: 9.4521\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4882 - mse: 10.0313\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3712 - mse: 8.2196\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5210 - mse: 10.1072\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5054 - mse: 10.2378\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 8.2095\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4227 - mse: 8.6585\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3532 - mse: 7.9033\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3947 - mse: 7.8248\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3928 - mse: 8.1257\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4003 - mse: 8.3220\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4662 - mse: 9.6885\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5015 - mse: 10.7580\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4541 - mse: 10.0901\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4335 - mse: 8.0890\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5047 - mse: 10.0083\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4769 - mse: 9.7813\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3760 - mse: 8.4697\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5437 - mse: 11.3050\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4171 - mse: 8.3325\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4269 - mse: 9.6776\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4326 - mse: 8.6743\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4126 - mse: 8.0323\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4098 - mse: 8.2854\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4769 - mse: 10.0987\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4176 - mse: 9.4894\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5173 - mse: 10.5141\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4929 - mse: 9.9091\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3262 - mse: 7.1651\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4284 - mse: 8.6771\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4310 - mse: 9.3265\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5092 - mse: 11.1743\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4056 - mse: 8.3870\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5137 - mse: 11.2381\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4753 - mse: 9.2123\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4980 - mse: 10.4204\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4401 - mse: 8.5016\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4381 - mse: 9.0464\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5321 - mse: 10.6438\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4621 - mse: 9.4429\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4414 - mse: 9.5052\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4588 - mse: 9.2621\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3765 - mse: 9.0657\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3680 - mse: 8.5920\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4564 - mse: 9.9091\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4488 - mse: 9.1468\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4990 - mse: 10.9587\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5684 - mse: 11.3943\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4425 - mse: 8.9364\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4954 - mse: 10.1464\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5079 - mse: 9.9404\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3761 - mse: 7.4067\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4923 - mse: 10.2773\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4357 - mse: 8.8225\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5795 - mse: 11.0152\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4405 - mse: 8.6756\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4044 - mse: 8.2073\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3756 - mse: 8.0079\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4657 - mse: 9.7678\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5359 - mse: 11.6175\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3800 - mse: 7.9811\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.3410 - mse: 6.5421\n",
            "Mean Absolute Error, Mean Squared Error  [1.341025710105896, 6.542056560516357]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zSpwDtuvPd6"
      },
      "source": [
        "### Improvement \n",
        "Use Embeddings and pass them through a Neural Net as well as a Convolutional Neural Net."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqjpiTb4vPd6"
      },
      "source": [
        "\n",
        "#### ELMO - Contextualized Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83dwpOW7vPd6"
      },
      "source": [
        "  elmo_embeddings_train_path_hair = '/content/gdrive/MyDrive/266/final/data/elmo_embeddings_train_hair.npy'\n",
        "  elmo_embedding_test_path_hair = '/content/gdrive/MyDrive/266/final/data/elmo_embeddings_test_hair.npy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzMSIfcUHdoj"
      },
      "source": [
        "create_and_save_elmo_embeddings(elmo_embeddings_train_path_hair,elmo_embedding_test_path_hair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBAyhjYOvPd6"
      },
      "source": [
        "# Load pre-saved elmo word embeddings \n",
        "elmo_embeddings_hair = np.load(elmo_embeddings_train_path_hair)\n",
        "elmo_embeddings_hair_test = np.load(elmo_embedding_test_path_hair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5pmmiNjvPd7",
        "outputId": "513044bf-03cf-4e32-a822-b4b29d71a88d"
      },
      "source": [
        "print(\"elmo_embeddings_hair shape: \", elmo_embeddings_hair.shape)\n",
        "print(\"elmo_embeddings_hair_test shape: \", elmo_embeddings_hair_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo_embeddings_hair shape:  (1958, 1024)\n",
            "elmo_embeddings_hair_test shape:  (506, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NunhyZVdvPd7"
      },
      "source": [
        "#### BERT (Variations) Embeddings\n",
        "Semi-Adapted from [Source](https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHpFTS6EvPd7"
      },
      "source": [
        " **DistilBERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFNIWUyMvPd7"
      },
      "source": [
        "distilbert_embeddings_train_path_hair = \"/content/gdrive/MyDrive/266/final/data/distilbert_embeddings_train_hair.npy\"\n",
        "distilbert_embeddings_test_path_hair =  \"/content/gdrive/MyDrive/266/final/data/distilbert_embeddings_test_hair.npy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM3L_mmVvPd7",
        "outputId": "42ea5e47-d201-4d32-987a-7b36da153651"
      },
      "source": [
        "create_and_save_bert_embeddings(train_features, distilbert_embeddings_train_path_hair  , bert_type=\"distilbert\")\n",
        "create_and_save_bert_embeddings(test_features, distilbert_embeddings_test_path_hair, bert_type=\"distilbert\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [04:03<00:00, 12.20s/it]\n",
            "100%|██████████| 6/6 [01:02<00:00, 10.34s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKp0h2oevPd7"
      },
      "source": [
        "# Load pre-saved DistilBERT word embeddings \n",
        "distilbert_embeddings_hair = np.load(distilbert_embeddings_train_path_hair)\n",
        "distilbert_embeddings_hair_test = np.load(distilbert_embeddings_test_path_hair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwuYeT_GvPd8",
        "outputId": "49e50b33-ce1d-4208-87c7-2ea1bc8ae355"
      },
      "source": [
        "print(\"distilbert_embeddings_hair shape: \", distilbert_embeddings_hair.shape)\n",
        "print(\"distilbert_embeddings_hair_test shape: \", distilbert_embeddings_hair_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distilbert_embeddings_hair shape:  (1958, 768)\n",
            "distilbert_embeddings_hair_test shape:  (506, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pTDqrusvPd8"
      },
      "source": [
        " **RoBERTa**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q330H8lfvPd8"
      },
      "source": [
        "roberta_embeddings_train_path_hair = \"/content/gdrive/MyDrive/266/final/data/roberta_embeddings_train_hair.npy\"\n",
        "roberta_embeddings_test_path_hair =  \"/content/gdrive/MyDrive/266/final/data/roberta_embeddings_test_hair.npy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s22NJnmGvPd8",
        "outputId": "a9225d81-8715-4ec1-89a6-697d246113b7"
      },
      "source": [
        "create_and_save_bert_embeddings(train_features, roberta_embeddings_train_path_hair  , bert_type=\"roberta\")\n",
        "create_and_save_bert_embeddings(test_features, roberta_embeddings_test_path_hair, bert_type=\"roberta\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [07:30<00:00, 22.52s/it]\n",
            "100%|██████████| 6/6 [01:52<00:00, 18.76s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZCeCl-4vPd8"
      },
      "source": [
        "# Load pre-saved RoBERTa word embeddings \n",
        "roberta_embeddings_hair = np.load(roberta_embeddings_train_path_hair)\n",
        "roberta_embeddings_hair_test = np.load(roberta_embeddings_test_path_hair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz0bbYE8vPd8",
        "outputId": "eb09663d-22a1-4410-9cf7-39d3e05633bb"
      },
      "source": [
        "print(\"roberta_embeddings_hair shape: \", roberta_embeddings_hair.shape)\n",
        "print(\"roberta_embeddings_hair_test shape: \", roberta_embeddings_hair_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta_embeddings_hair shape:  (1958, 768)\n",
            "roberta_embeddings_hair_test shape:  (506, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TceGA9PgvPd8"
      },
      "source": [
        "**BERT (original)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Z7WzMgvPd8"
      },
      "source": [
        "bert_embeddings_train_path_hair = \"/content/gdrive/MyDrive/266/final/data/bert_embeddings_train_hair.npy\"\n",
        "bert_embeddings_test_path_hair =  \"/content/gdrive/MyDrive/266/final/data/bert_embeddings_test_hair.npy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFl_GzHCvPd9",
        "outputId": "dcd91512-2292-4e7b-80b1-2fb0b1604f28"
      },
      "source": [
        "create_and_save_bert_embeddings(train_features, bert_embeddings_train_path_hair  , bert_type=\"bert\")\n",
        "create_and_save_bert_embeddings(test_features, bert_embeddings_test_path_hair, bert_type=\"bert\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [08:09<00:00, 24.49s/it]\n",
            "100%|██████████| 6/6 [02:04<00:00, 20.81s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LohMA0IOvPd9"
      },
      "source": [
        "# Load pre-saved RoBERTa word embeddings \n",
        "bert_embeddings_hair = np.load(bert_embeddings_train_path_hair)\n",
        "bert_embeddings_hair_test = np.load(bert_embeddings_test_path_hair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE906JDXvPd9",
        "outputId": "9a3f9e3e-dcf3-494a-c54c-1bc58cacc207"
      },
      "source": [
        "print(\"bert_embeddings shape: \", bert_embeddings_hair.shape)\n",
        "print(\"bert_embeddings_test shape: \", bert_embeddings_hair_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_embeddings shape:  (1958, 768)\n",
            "bert_embeddings_test shape:  (506, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFVeTncDvPd9"
      },
      "source": [
        "### Neural Network - Run for Improvement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meoFoqL3vPd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9dc7ea65-032f-46dc-fc2a-c07902992d9a"
      },
      "source": [
        "# ELMO\n",
        "run_model((1024,), elmo_embeddings_hair, train_labels, elmo_embeddings_hair_test, test_labels, 200)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error  [1.1777405738830566, 4.052495002746582]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 1.4644 - mse: 8.4311\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5035 - mse: 10.4092\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4173 - mse: 8.9808\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4737 - mse: 9.1023\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5537 - mse: 11.3299\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4462 - mse: 9.5241\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4417 - mse: 9.1316\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4913 - mse: 10.6525\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2690 - mse: 6.9687\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4283 - mse: 8.2753\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3942 - mse: 8.8846\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4715 - mse: 11.6570\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3617 - mse: 8.9690\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3419 - mse: 9.0805\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2941 - mse: 7.7425\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2629 - mse: 7.4774\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3446 - mse: 7.7804\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4132 - mse: 9.9465\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3628 - mse: 9.7392\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3244 - mse: 9.1263\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2112 - mse: 7.3697\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3675 - mse: 10.8206\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2480 - mse: 6.9303\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2324 - mse: 7.3564\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3227 - mse: 8.7415\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2166 - mse: 8.0026\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2550 - mse: 7.3033\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1708 - mse: 6.1004\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2838 - mse: 8.3275\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2329 - mse: 9.0898\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2476 - mse: 7.8283\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2347 - mse: 8.1039\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0851 - mse: 5.4646\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2309 - mse: 8.4126\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1633 - mse: 7.1302\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1231 - mse: 6.7881\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1513 - mse: 7.8993\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1581 - mse: 7.1210\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1661 - mse: 7.1999\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1667 - mse: 7.7856\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1640 - mse: 6.4465\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1113 - mse: 5.8808\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2623 - mse: 8.8432\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1287 - mse: 6.6854\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0692 - mse: 5.8278\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1023 - mse: 6.1908\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1038 - mse: 6.3068\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0786 - mse: 6.3877\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0172 - mse: 5.7594\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1341 - mse: 7.4987\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2175 - mse: 8.3883\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0811 - mse: 6.2510\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0308 - mse: 6.1392\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0964 - mse: 6.3239\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1127 - mse: 6.7018\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0727 - mse: 5.8031\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0769 - mse: 6.3920\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0274 - mse: 5.5893\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0685 - mse: 6.0900\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9638 - mse: 4.9813\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0887 - mse: 7.6622\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0770 - mse: 7.6809\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0654 - mse: 6.1915\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0679 - mse: 7.3253\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0829 - mse: 7.6031\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0017 - mse: 5.1998\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9752 - mse: 4.6835\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0028 - mse: 6.1010\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9375 - mse: 4.6804\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1026 - mse: 7.0183\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0430 - mse: 8.0296\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8899 - mse: 4.4826\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9384 - mse: 4.9537\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0234 - mse: 4.8950\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1212 - mse: 6.4171\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9831 - mse: 5.6243\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0576 - mse: 6.3825\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8818 - mse: 4.4767\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0070 - mse: 5.6589\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0042 - mse: 6.5670\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8532 - mse: 3.9890\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8732 - mse: 4.3395\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9678 - mse: 6.6867\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0020 - mse: 5.3753\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0448 - mse: 6.9939\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8865 - mse: 4.0659\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9620 - mse: 5.5414\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0140 - mse: 5.0645\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9181 - mse: 4.0451\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9105 - mse: 4.1403\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9444 - mse: 4.4992\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9493 - mse: 5.8789\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8727 - mse: 4.8244\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0068 - mse: 5.4501\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8938 - mse: 5.0870\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8735 - mse: 4.2630\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8822 - mse: 5.5917\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7998 - mse: 3.4066\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8767 - mse: 4.5320\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8425 - mse: 4.1890\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8736 - mse: 4.2215\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8325 - mse: 3.6046\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8519 - mse: 4.2175\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8369 - mse: 3.7499\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8031 - mse: 3.5979\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7509 - mse: 3.6657\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8836 - mse: 5.5769\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8472 - mse: 4.5376\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8275 - mse: 4.0129\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8286 - mse: 4.0626\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8701 - mse: 4.4971\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8338 - mse: 4.0048\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8508 - mse: 4.8236\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8117 - mse: 3.7353\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7516 - mse: 3.1433\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8151 - mse: 3.4795\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7660 - mse: 3.3219\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7972 - mse: 3.6286\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7633 - mse: 3.4703\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7766 - mse: 4.0903\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8105 - mse: 3.7409\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7494 - mse: 2.9292\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 4.5250\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7711 - mse: 3.0481\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7992 - mse: 3.2712\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8506 - mse: 4.7101\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7744 - mse: 3.5105\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7245 - mse: 3.1679\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7532 - mse: 3.5966\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7148 - mse: 2.9741\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7189 - mse: 3.5078\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8001 - mse: 3.8208\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7870 - mse: 3.4082\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8345 - mse: 3.5511\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7937 - mse: 3.3476\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 3.2092\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7460 - mse: 3.2902\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7064 - mse: 2.9320\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7192 - mse: 3.5702\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7102 - mse: 3.3226\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7548 - mse: 2.8410\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7167 - mse: 3.0844\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8139 - mse: 3.6690\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6664 - mse: 2.8607\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7737 - mse: 3.7939\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7083 - mse: 3.4615\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6789 - mse: 2.2460\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6828 - mse: 3.3374\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7079 - mse: 2.9048\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6528 - mse: 2.4574\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6991 - mse: 2.7510\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6707 - mse: 2.3934\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6579 - mse: 2.5362\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7425 - mse: 3.2981\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6896 - mse: 2.7847\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6373 - mse: 1.9040\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7291 - mse: 2.7458\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7213 - mse: 3.0085\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6770 - mse: 2.4805\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6773 - mse: 2.5605\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6490 - mse: 1.9911\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6648 - mse: 2.7936\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7257 - mse: 3.1441\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7161 - mse: 2.7278\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6896 - mse: 2.7680\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6708 - mse: 1.9782\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7311 - mse: 2.9193\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5787 - mse: 1.7821\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6567 - mse: 2.2626\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6336 - mse: 1.9483\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6150 - mse: 2.0074\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7809 - mse: 3.0273\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6721 - mse: 2.3223\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6282 - mse: 2.2013\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6652 - mse: 2.9548\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6936 - mse: 2.6323\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6658 - mse: 2.5858\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6891 - mse: 3.2835\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5784 - mse: 1.8987\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6166 - mse: 2.1054\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6512 - mse: 3.0629\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6914 - mse: 1.9768\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6192 - mse: 2.2151\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6309 - mse: 3.2156\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5687 - mse: 1.8176\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6775 - mse: 2.5934\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6471 - mse: 2.3118\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6668 - mse: 2.7899\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6601 - mse: 2.0436\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5963 - mse: 1.9706\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6247 - mse: 2.6059\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6304 - mse: 2.5839\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6297 - mse: 2.7235\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6098 - mse: 2.1094\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5912 - mse: 2.0192\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6139 - mse: 2.2644\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6405 - mse: 2.6558\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5947 - mse: 1.5326\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6421 - mse: 3.2846\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5825 - mse: 1.9175\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3572 - mse: 6.3713\n",
            "Mean Absolute Error, Mean Squared Error  [1.3571596145629883, 6.371335983276367]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error  [1.1777405738830566, 4.052495002746582]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkTGD5oLvPd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "847369b8-4b35-4123-b65e-75a49961946b"
      },
      "source": [
        "# DistilBERT\n",
        "run_model((768,), distilbert_embeddings_hair, train_labels, distilbert_embeddings_hair_test, test_labels, 200)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error  [1.0419797897338867, 3.6801388263702393]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.6316 - mse: 10.6465\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4207 - mse: 9.5812\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4691 - mse: 9.5364\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3735 - mse: 8.3415\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4638 - mse: 9.9147\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4129 - mse: 8.8541\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3570 - mse: 7.8905\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3717 - mse: 9.1481\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3138 - mse: 7.1682\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4108 - mse: 10.0043\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3977 - mse: 8.7465\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3392 - mse: 8.7603\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3255 - mse: 7.8447\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3617 - mse: 7.9917\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4331 - mse: 10.3271\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3229 - mse: 8.8075\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3101 - mse: 8.4042\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4365 - mse: 11.3946\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3119 - mse: 8.1273\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2732 - mse: 8.0428\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3434 - mse: 8.8622\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2692 - mse: 7.1889\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3020 - mse: 7.9825\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2824 - mse: 9.2204\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2670 - mse: 8.0724\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2681 - mse: 7.0076\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1906 - mse: 7.0815\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2004 - mse: 7.8088\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2634 - mse: 7.6868\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2537 - mse: 8.7593\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3323 - mse: 8.6048\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3826 - mse: 9.5689\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2150 - mse: 7.3016\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 5.3842\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2694 - mse: 8.6074\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2701 - mse: 8.4173\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2016 - mse: 7.1850\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2129 - mse: 7.4738\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2487 - mse: 8.9364\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1996 - mse: 8.1564\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0798 - mse: 5.6947\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1688 - mse: 7.5153\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1379 - mse: 5.8591\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0750 - mse: 5.8648\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1718 - mse: 7.5864\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2677 - mse: 9.7344\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0870 - mse: 6.5256\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1589 - mse: 7.5812\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1367 - mse: 7.2227\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1369 - mse: 6.8020\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1824 - mse: 8.7254\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1699 - mse: 7.4984\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1901 - mse: 8.5030\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0954 - mse: 6.4831\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1784 - mse: 7.9663\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0700 - mse: 5.6102\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0752 - mse: 6.4652\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1014 - mse: 6.8681\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0850 - mse: 6.6922\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0870 - mse: 6.2804\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0282 - mse: 6.2109\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1107 - mse: 7.7050\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0752 - mse: 6.7145\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0195 - mse: 5.7830\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0377 - mse: 6.1072\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0916 - mse: 7.1244\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0262 - mse: 6.0821\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9227 - mse: 4.5920\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0856 - mse: 6.6243\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0619 - mse: 7.1816\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9695 - mse: 6.0520\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0544 - mse: 6.1268\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9625 - mse: 5.5797\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9165 - mse: 5.3843\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8831 - mse: 3.7409\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8602 - mse: 3.9540\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9474 - mse: 4.8489\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9382 - mse: 4.9960\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9421 - mse: 6.1864\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8224 - mse: 3.6673\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9053 - mse: 4.8744\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8885 - mse: 4.3743\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9827 - mse: 6.6402\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9812 - mse: 6.3056\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9118 - mse: 4.1747\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9613 - mse: 6.4304\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9414 - mse: 5.0661\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8246 - mse: 4.1014\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8867 - mse: 4.4693\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9143 - mse: 4.9774\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9386 - mse: 4.8305\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8413 - mse: 4.0630\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9041 - mse: 4.0782\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9491 - mse: 4.7591\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 3.8323\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9900 - mse: 5.9693\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8573 - mse: 3.6722\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9520 - mse: 6.1304\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9361 - mse: 4.9426\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8925 - mse: 5.1391\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9012 - mse: 4.3872\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8802 - mse: 4.1526\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8679 - mse: 4.8081\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9141 - mse: 5.3240\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8722 - mse: 4.7575\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9026 - mse: 5.0356\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8762 - mse: 5.1939\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8205 - mse: 4.2798\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8417 - mse: 3.8877\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8738 - mse: 4.2299\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8548 - mse: 3.7289\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9162 - mse: 5.3755\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8530 - mse: 4.4503\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8097 - mse: 3.8659\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8617 - mse: 4.8874\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8900 - mse: 5.1053\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8458 - mse: 3.6859\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8653 - mse: 4.2882\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7892 - mse: 3.6130\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8156 - mse: 3.4836\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7873 - mse: 3.8345\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7536 - mse: 3.4428\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8374 - mse: 5.0578\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8144 - mse: 4.2273\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7228 - mse: 3.6428\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8642 - mse: 4.9313\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 4.2644\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8089 - mse: 3.6552\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7956 - mse: 4.5144\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7076 - mse: 2.9492\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8458 - mse: 3.7011\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8421 - mse: 4.3379\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7154 - mse: 3.5308\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7717 - mse: 3.7678\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7842 - mse: 3.9920\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7538 - mse: 3.8728\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7455 - mse: 3.0153\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6913 - mse: 2.7629\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7227 - mse: 3.1206\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7319 - mse: 3.2527\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8105 - mse: 4.5454\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6946 - mse: 2.7823\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7780 - mse: 3.7134\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7274 - mse: 3.3685\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7774 - mse: 3.4399\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7888 - mse: 4.0528\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7855 - mse: 3.4439\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7684 - mse: 4.1521\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6824 - mse: 2.5482\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7241 - mse: 3.3729\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7097 - mse: 2.8001\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7626 - mse: 3.3380\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6867 - mse: 3.2110\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7845 - mse: 4.4847\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7412 - mse: 3.6697\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7522 - mse: 3.8416\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7704 - mse: 4.1161\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6916 - mse: 2.7752\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6951 - mse: 3.2537\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6748 - mse: 2.7721\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7353 - mse: 3.8668\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7444 - mse: 3.6657\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7478 - mse: 3.7707\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6527 - mse: 3.0460\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7768 - mse: 4.1610\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7219 - mse: 3.8705\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6454 - mse: 2.7022\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6812 - mse: 2.9276\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6591 - mse: 2.8953\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6594 - mse: 3.1764\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7340 - mse: 3.5134\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7013 - mse: 2.8815\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7255 - mse: 3.7324\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6777 - mse: 2.9524\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6780 - mse: 3.0289\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7131 - mse: 3.7160\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6803 - mse: 3.0034\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6785 - mse: 3.0492\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6808 - mse: 3.1458\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6865 - mse: 3.7343\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6699 - mse: 2.8352\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7046 - mse: 2.8947\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6935 - mse: 3.2478\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6637 - mse: 2.8031\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6602 - mse: 3.0430\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6642 - mse: 3.5172\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6441 - mse: 3.4420\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7304 - mse: 3.6280\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6454 - mse: 2.4136\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6245 - mse: 2.6072\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6901 - mse: 3.7117\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6515 - mse: 3.0408\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6874 - mse: 2.9079\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6795 - mse: 3.0315\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6386 - mse: 2.4638\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7286 - mse: 3.3033\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5735 - mse: 1.8462\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6310 - mse: 2.5180\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5833 - mse: 2.5048\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7249 - mse: 3.5416\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.4230 - mse: 6.6445\n",
            "Mean Absolute Error, Mean Squared Error  [1.4230331182479858, 6.644531726837158]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error  [1.0419797897338867, 3.6801388263702393]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AM7ZQ-BvPd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "841afa4c-5d6f-42b3-b150-1e86ecf9d356"
      },
      "source": [
        "# RoBERTa\n",
        "run_model((768,), roberta_embeddings_hair, train_labels, roberta_embeddings_hair_test, test_labels, 200)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error  [1.0948551893234253, 3.6444075107574463]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5634 - mse: 9.2003\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4195 - mse: 8.5031\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4111 - mse: 8.3168\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4226 - mse: 8.3040\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3354 - mse: 7.3659\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3334 - mse: 7.7708\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3588 - mse: 8.5390\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3865 - mse: 8.3252\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2834 - mse: 7.7309\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3030 - mse: 7.7155\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4968 - mse: 11.0830\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3814 - mse: 9.0168\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3441 - mse: 8.7787\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3748 - mse: 9.4850\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 6.4286\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2781 - mse: 8.7117\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3166 - mse: 8.5355\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2160 - mse: 7.3933\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2478 - mse: 7.4410\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3281 - mse: 9.0852\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3049 - mse: 8.8624\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2257 - mse: 7.2090\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2892 - mse: 9.0286\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2312 - mse: 7.4198\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2242 - mse: 7.8394\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2472 - mse: 8.7432\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2622 - mse: 8.7026\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2972 - mse: 10.3012\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2299 - mse: 8.8178\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2302 - mse: 8.4381\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1060 - mse: 6.1650\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1597 - mse: 7.1014\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2363 - mse: 8.9431\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2407 - mse: 9.7943\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1477 - mse: 8.0309\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1428 - mse: 6.7075\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1854 - mse: 8.1499\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0886 - mse: 7.5054\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0474 - mse: 6.1931\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0552 - mse: 6.6224\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0197 - mse: 6.5605\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1389 - mse: 8.6181\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0557 - mse: 6.3325\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0292 - mse: 6.1931\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0752 - mse: 6.6157\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0678 - mse: 7.2342\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0821 - mse: 7.3133\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9600 - mse: 5.2268\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0164 - mse: 6.2585\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0663 - mse: 7.7214\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9429 - mse: 5.6415\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9345 - mse: 5.3899\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9546 - mse: 6.1222\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9775 - mse: 5.6896\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 8.0401\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9569 - mse: 6.9955\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0070 - mse: 7.6055\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8898 - mse: 4.9909\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9091 - mse: 5.7416\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0208 - mse: 7.9829\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9138 - mse: 6.0914\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9612 - mse: 7.2161\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8458 - mse: 5.3370\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9027 - mse: 5.8481\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9067 - mse: 6.5673\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8656 - mse: 5.5205\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9539 - mse: 6.6264\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8274 - mse: 4.7047\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9185 - mse: 6.8449\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8167 - mse: 4.9288\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9310 - mse: 6.1383\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8484 - mse: 5.5548\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8042 - mse: 4.5200\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7949 - mse: 4.5729\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8603 - mse: 6.2962\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8990 - mse: 6.3600\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7817 - mse: 5.1941\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7577 - mse: 4.6755\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 5.1836\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7781 - mse: 4.4032\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7514 - mse: 4.0995\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7242 - mse: 3.6912\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7556 - mse: 4.3439\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6405 - mse: 2.7856\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7149 - mse: 3.3548\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6940 - mse: 2.9951\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7485 - mse: 4.3940\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7164 - mse: 3.7074\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7000 - mse: 4.2071\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7144 - mse: 3.1802\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6706 - mse: 3.1458\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6838 - mse: 3.2703\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6547 - mse: 2.9085\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6873 - mse: 3.5568\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7022 - mse: 3.2906\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6404 - mse: 2.6965\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6598 - mse: 3.2082\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6370 - mse: 3.2969\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6992 - mse: 3.5081\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6138 - mse: 3.1207\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6030 - mse: 2.6436\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6756 - mse: 3.2279\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6637 - mse: 3.7689\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6129 - mse: 3.1492\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6412 - mse: 2.4513\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5880 - mse: 3.0368\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6190 - mse: 2.6601\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6677 - mse: 4.0964\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5659 - mse: 3.0207\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6868 - mse: 3.4048\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5835 - mse: 2.1628\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5442 - mse: 2.1748\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5898 - mse: 2.8163\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5907 - mse: 2.8015\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5512 - mse: 2.2740\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5625 - mse: 2.2272\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5236 - mse: 1.8302\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5923 - mse: 2.9801\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5561 - mse: 2.2501\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6310 - mse: 2.6180\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6433 - mse: 2.7335\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5420 - mse: 1.8551\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5954 - mse: 2.8302\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5835 - mse: 2.9921\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5663 - mse: 1.7351\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4806 - mse: 1.5527\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5933 - mse: 2.4370\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5536 - mse: 1.9609\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5366 - mse: 2.0185\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5667 - mse: 2.4215\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4964 - mse: 1.4267\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5299 - mse: 1.9241\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5962 - mse: 2.9949\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6357 - mse: 2.6907\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5302 - mse: 1.7686\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5267 - mse: 1.8870\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4902 - mse: 1.5974\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5087 - mse: 1.6711\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5565 - mse: 2.4695\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5646 - mse: 2.3026\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5034 - mse: 1.4709\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5103 - mse: 1.8548\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4952 - mse: 1.6218\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5565 - mse: 2.3744\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4702 - mse: 1.4078\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4758 - mse: 1.5269\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5012 - mse: 1.7719\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4706 - mse: 1.5324\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5170 - mse: 2.4266\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4544 - mse: 1.3110\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4801 - mse: 1.7285\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4492 - mse: 1.3607\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5354 - mse: 1.8502\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4750 - mse: 1.5945\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4506 - mse: 1.3009\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5033 - mse: 1.8017\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4622 - mse: 1.3730\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4743 - mse: 1.3493\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5621 - mse: 2.6217\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5197 - mse: 1.7935\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5093 - mse: 1.9306\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4837 - mse: 1.3863\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4515 - mse: 1.6706\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4319 - mse: 1.6110\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4643 - mse: 1.5621\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4488 - mse: 1.5057\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4481 - mse: 1.0221\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4426 - mse: 1.4995\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5157 - mse: 2.0497\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4537 - mse: 1.2408\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4734 - mse: 1.6972\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4550 - mse: 1.2559\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 1.1959\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4743 - mse: 1.1381\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4467 - mse: 1.2999\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4778 - mse: 1.2681\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 2.1514\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4572 - mse: 1.4414\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4758 - mse: 1.6806\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4365 - mse: 1.1850\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4177 - mse: 0.9782\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4770 - mse: 1.9114\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4817 - mse: 1.3112\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4132 - mse: 0.8448\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4628 - mse: 1.3551\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4154 - mse: 1.3299\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4520 - mse: 1.0843\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4111 - mse: 1.2125\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4371 - mse: 1.4669\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4265 - mse: 1.0184\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4782 - mse: 1.7973\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4108 - mse: 0.7195\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4029 - mse: 0.8868\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4238 - mse: 1.5061\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4163 - mse: 1.0300\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4508 - mse: 1.2362\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4412 - mse: 1.2438\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3998 - mse: 0.7970\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4513 - mse: 1.2360\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4066 - mse: 1.0532\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4154 - mse: 8.2057\n",
            "Mean Absolute Error, Mean Squared Error  [1.4153684377670288, 8.205737113952637]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error  [1.0948551893234253, 3.6444075107574463]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_1_0yHxvPd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42c5219d-910a-4a50-98f5-ad8266f605e8"
      },
      "source": [
        "# BERT\n",
        "run_model((768,), bert_embeddings_hair, train_labels, bert_embeddings_hair_test, test_labels, 200)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error  [1.102107286453247, 3.9629263877868652]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5286 - mse: 8.2813\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4737 - mse: 10.1940\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4550 - mse: 9.3647\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4326 - mse: 9.3068\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2993 - mse: 7.9065\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4236 - mse: 9.6917\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2948 - mse: 7.4466\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3973 - mse: 9.7729\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2967 - mse: 7.2143\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3476 - mse: 7.9626\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4304 - mse: 10.4639\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2364 - mse: 7.2149\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3877 - mse: 10.6302\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2842 - mse: 8.1435\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2899 - mse: 9.1270\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2074 - mse: 7.0890\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3066 - mse: 8.0036\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1926 - mse: 6.9255\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2036 - mse: 7.1076\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1640 - mse: 6.4693\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1588 - mse: 6.7370\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3059 - mse: 9.0782\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1342 - mse: 6.4626\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2719 - mse: 7.9746\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1282 - mse: 8.0619\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1815 - mse: 8.1928\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1576 - mse: 7.7117\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0935 - mse: 7.0837\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0985 - mse: 6.8873\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1000 - mse: 7.1706\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1480 - mse: 7.0425\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0768 - mse: 6.3392\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0446 - mse: 6.2505\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9495 - mse: 4.8769\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0291 - mse: 6.4304\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0274 - mse: 6.4359\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 6.4901\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0543 - mse: 7.2266\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0048 - mse: 6.4818\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1094 - mse: 8.9519\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8943 - mse: 4.7103\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.0499 - mse: 7.4337\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9810 - mse: 5.7045\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9536 - mse: 5.6501\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9732 - mse: 5.5832\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8993 - mse: 5.0159\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8854 - mse: 4.7459\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9514 - mse: 5.4003\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9031 - mse: 6.2146\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8547 - mse: 4.3167\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9596 - mse: 6.0984\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8587 - mse: 4.9521\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9017 - mse: 4.7352\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9147 - mse: 4.1879\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8970 - mse: 4.9843\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8483 - mse: 4.4057\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8608 - mse: 4.1419\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8276 - mse: 5.1549\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8208 - mse: 4.2880\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8255 - mse: 4.7645\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7707 - mse: 3.6362\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8161 - mse: 4.6604\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7263 - mse: 3.6863\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8249 - mse: 5.5654\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8309 - mse: 4.8493\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8084 - mse: 5.2979\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7909 - mse: 5.4410\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7725 - mse: 4.0640\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7568 - mse: 3.5742\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6816 - mse: 2.5909\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7810 - mse: 4.5047\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6872 - mse: 3.1025\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8243 - mse: 5.1258\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7506 - mse: 4.3420\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6860 - mse: 3.7977\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7525 - mse: 4.0653\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6588 - mse: 2.9017\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6545 - mse: 2.9705\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7452 - mse: 3.3900\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6571 - mse: 2.6547\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6503 - mse: 3.1747\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6367 - mse: 2.6396\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7075 - mse: 4.4019\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7001 - mse: 3.9174\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6275 - mse: 3.3933\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6056 - mse: 1.8718\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6447 - mse: 2.8576\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5922 - mse: 2.4674\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6731 - mse: 3.6098\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5653 - mse: 2.7208\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6211 - mse: 2.5593\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5884 - mse: 2.1722\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6493 - mse: 3.9863\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6659 - mse: 3.4698\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6317 - mse: 3.4339\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6672 - mse: 3.2182\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6057 - mse: 2.3414\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5799 - mse: 1.8070\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5725 - mse: 1.7800\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5953 - mse: 2.4297\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5642 - mse: 2.1056\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6322 - mse: 2.3416\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6660 - mse: 3.1761\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6366 - mse: 2.6210\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5820 - mse: 2.6195\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5467 - mse: 2.0548\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5823 - mse: 2.1590\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5669 - mse: 1.8302\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5875 - mse: 2.1175\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5686 - mse: 2.0604\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5842 - mse: 2.4408\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5559 - mse: 2.5146\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5242 - mse: 1.6244\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4983 - mse: 1.2845\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6239 - mse: 2.2907\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5267 - mse: 1.4385\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5989 - mse: 3.1726\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5542 - mse: 1.6368\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5384 - mse: 2.2795\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5569 - mse: 2.6596\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5697 - mse: 2.1798\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5209 - mse: 1.6716\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5668 - mse: 1.9604\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6055 - mse: 2.5377\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5030 - mse: 1.7277\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5336 - mse: 1.3272\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5484 - mse: 2.7879\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5278 - mse: 2.1554\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5284 - mse: 1.8962\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5185 - mse: 2.0047\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 1.4694\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5372 - mse: 1.4260\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5075 - mse: 1.7655\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5116 - mse: 1.6793\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5203 - mse: 2.1786\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5428 - mse: 1.8918\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5086 - mse: 2.0515\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 1.6001\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4920 - mse: 1.5219\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4608 - mse: 1.1348\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5270 - mse: 2.0404\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5072 - mse: 1.1328\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5973 - mse: 2.6455\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5159 - mse: 1.8070\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5029 - mse: 1.7795\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4794 - mse: 1.6020\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4317 - mse: 0.8122\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4864 - mse: 1.8563\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5141 - mse: 2.4767\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4548 - mse: 1.7700\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4624 - mse: 1.5382\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4638 - mse: 1.2497\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4213 - mse: 1.2333\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5160 - mse: 1.3587\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5512 - mse: 2.4473\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5216 - mse: 2.2196\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4229 - mse: 1.3607\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4780 - mse: 1.2864\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5013 - mse: 2.3438\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4687 - mse: 1.1187\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5003 - mse: 1.8684\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4279 - mse: 1.2247\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4835 - mse: 1.4146\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4576 - mse: 1.2950\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4318 - mse: 1.1941\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4649 - mse: 1.2497\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4279 - mse: 0.7150\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4477 - mse: 1.2353\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4712 - mse: 1.1293\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4548 - mse: 1.3747\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3940 - mse: 1.0146\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5240 - mse: 1.8852\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4351 - mse: 1.4990\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4745 - mse: 1.2759\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4494 - mse: 1.1212\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4575 - mse: 1.4506\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4209 - mse: 0.9481\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4425 - mse: 1.9119\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4451 - mse: 1.1401\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4380 - mse: 1.1622\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4143 - mse: 1.2394\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4805 - mse: 1.3408\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4322 - mse: 1.1193\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4172 - mse: 1.0399\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4425 - mse: 1.6571\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4159 - mse: 0.7562\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4948 - mse: 2.0461\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4023 - mse: 0.7957\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4098 - mse: 1.3730\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4219 - mse: 1.4491\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4413 - mse: 1.5000\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3878 - mse: 0.8291\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4325 - mse: 1.3854\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4760 - mse: 1.3365\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4312 - mse: 1.4314\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3742 - mse: 0.7462\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4577 - mse: 1.6423\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4132 - mse: 0.8614\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3803 - mse: 0.8710\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4468 - mse: 1.2667\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.3706 - mse: 6.5811\n",
            "Mean Absolute Error, Mean Squared Error  [1.3706049919128418, 6.581136226654053]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error  [1.102107286453247, 3.9629263877868652]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfJPMNCvPd-"
      },
      "source": [
        "### Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqPWYgY_1kET",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc9ee43b-614e-44f4-cce1-8f29087b7c4c"
      },
      "source": [
        "# ELMO\n",
        "cnn = train_cnn(elmo_embeddings_hair, train_labels)\n",
        "results = cnn.evaluate(elmo_embeddings_hair_test, test_labels)\n",
        "print(\"Mean Absolute Error, Mean Squared Error: \", results)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error:  [1.0965903997421265, 4.228530406951904]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 1s 4ms/step - loss: 3.0894 - mse: 18.7270\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.6560 - mse: 15.5926\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.3380 - mse: 14.2014\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 2.2299 - mse: 14.1784\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.9779 - mse: 11.0737\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.7767 - mse: 9.1741\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.8177 - mse: 10.6047\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.7639 - mse: 11.1267\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.7517 - mse: 11.4102\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6960 - mse: 10.9168\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5534 - mse: 8.3668\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6480 - mse: 10.2042\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.7291 - mse: 12.1915\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5329 - mse: 9.2481\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6807 - mse: 11.3216\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5691 - mse: 9.7304\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6111 - mse: 11.2280\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5872 - mse: 10.8362\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5442 - mse: 9.7682\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5943 - mse: 11.7374\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5967 - mse: 10.4984\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6138 - mse: 10.8318\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4205 - mse: 8.7532\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5418 - mse: 11.7754\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4792 - mse: 10.0799\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4212 - mse: 8.4567\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5165 - mse: 9.7898\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4643 - mse: 9.7344\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4846 - mse: 8.5751\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4075 - mse: 8.3428\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5209 - mse: 10.8882\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4876 - mse: 9.2699\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4196 - mse: 8.4481\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3592 - mse: 8.2434\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4840 - mse: 9.9817\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3924 - mse: 8.5519\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5339 - mse: 10.9617\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3803 - mse: 8.5424\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4427 - mse: 8.8464\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5296 - mse: 11.5304\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5580 - mse: 12.0589\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4564 - mse: 9.8690\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3695 - mse: 8.0816\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4505 - mse: 9.0797\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5047 - mse: 9.8092\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4251 - mse: 8.8588\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4098 - mse: 9.0019\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4763 - mse: 10.1471\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5176 - mse: 10.8994\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4491 - mse: 9.1907\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4452 - mse: 9.7300\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3635 - mse: 8.0914\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5016 - mse: 10.6148\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5800 - mse: 12.4756\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4177 - mse: 7.8811\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3296 - mse: 7.1516\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4681 - mse: 10.0786\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4598 - mse: 9.8210\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4446 - mse: 10.3651\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3963 - mse: 8.6458\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4341 - mse: 9.1828\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5090 - mse: 10.9910\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4271 - mse: 8.0285\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4185 - mse: 8.5941\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4384 - mse: 9.1243\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4207 - mse: 9.1565\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3700 - mse: 7.9538\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3383 - mse: 7.4686\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5042 - mse: 11.2934\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4660 - mse: 9.9046\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5351 - mse: 11.8217\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4446 - mse: 9.2655\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4170 - mse: 8.4836\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3897 - mse: 8.3090\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3398 - mse: 8.1651\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4070 - mse: 8.2661\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4556 - mse: 9.0254\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4645 - mse: 9.7929\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3932 - mse: 8.4217\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4392 - mse: 9.4082\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5412 - mse: 11.8567\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4398 - mse: 9.9171\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4836 - mse: 10.8730\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4695 - mse: 9.8083\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5134 - mse: 10.2967\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4587 - mse: 10.2184\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4513 - mse: 8.9145\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4330 - mse: 8.4246\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.2603 - mse: 6.4862\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4792 - mse: 9.2357\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4335 - mse: 8.9667\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4173 - mse: 7.8847\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4401 - mse: 9.1977\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4411 - mse: 9.0260\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4131 - mse: 8.9975\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5227 - mse: 9.8551\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4925 - mse: 9.2341\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4472 - mse: 8.8496\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3515 - mse: 7.5669\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4892 - mse: 11.0882\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.3389 - mse: 6.5594\n",
            "Mean Absolute Error, Mean Squared Error:  [1.3388919830322266, 6.559425354003906]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error:  [1.0965903997421265, 4.228530406951904]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXTKxFUf1kEU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc26a45e-3f3f-4f85-8d2b-3d9d5a741101"
      },
      "source": [
        "# DistilBERT\n",
        "cnn = train_cnn(distilbert_embeddings_hair, train_labels)\n",
        "results = cnn.evaluate(distilbert_embeddings_hair_test, test_labels)\n",
        "print(\"Mean Absolute Error, Mean Squared Error: \", results)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error:  [1.1034969091415405, 4.20483922958374]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 1s 4ms/step - loss: 4.7458 - mse: 38.9410\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 4.0884 - mse: 30.5212\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 3.3034 - mse: 20.6200\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.7051 - mse: 17.4467\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.4662 - mse: 14.7212\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.1286 - mse: 12.5889\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.9129 - mse: 10.8416\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.8160 - mse: 11.2455\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6211 - mse: 9.1382\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6570 - mse: 10.5760\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6440 - mse: 10.4807\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6161 - mse: 9.7619\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6391 - mse: 10.2517\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6087 - mse: 10.4891\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5332 - mse: 10.1409\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6310 - mse: 10.8430\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4403 - mse: 8.4701\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5487 - mse: 10.9294\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5711 - mse: 11.0039\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5306 - mse: 9.7176\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4603 - mse: 9.8006\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4317 - mse: 9.0357\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4522 - mse: 9.3487\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5977 - mse: 11.1891\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4233 - mse: 8.0971\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5803 - mse: 11.6951\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4834 - mse: 10.1756\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5154 - mse: 10.8562\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5240 - mse: 9.0800\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4406 - mse: 8.8961\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3898 - mse: 7.9224\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4276 - mse: 8.6803\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4655 - mse: 10.1733\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4932 - mse: 10.8546\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5098 - mse: 9.7017\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4278 - mse: 8.9638\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4610 - mse: 9.5077\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4816 - mse: 10.3628\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4326 - mse: 8.9484\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4170 - mse: 8.3881\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3805 - mse: 7.8977\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4228 - mse: 8.8131\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4215 - mse: 8.2270\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4955 - mse: 10.6267\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3666 - mse: 8.1104\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4405 - mse: 8.9432\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4609 - mse: 8.9041\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4565 - mse: 9.0177\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4388 - mse: 9.2529\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5681 - mse: 12.0565\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4731 - mse: 9.6423\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3817 - mse: 8.5147\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4421 - mse: 9.4151\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3733 - mse: 7.9245\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5306 - mse: 11.2027\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4678 - mse: 10.1547\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4567 - mse: 9.5401\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4161 - mse: 9.3351\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5015 - mse: 9.3213\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4165 - mse: 8.8977\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4676 - mse: 9.4529\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4259 - mse: 9.2409\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4546 - mse: 9.4882\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4021 - mse: 8.5596\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5318 - mse: 11.3457\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5600 - mse: 11.1397\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5010 - mse: 10.5527\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4314 - mse: 9.0502\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4805 - mse: 10.1210\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3710 - mse: 8.5218\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3868 - mse: 8.1492\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4908 - mse: 10.5755\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4992 - mse: 10.2898\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3901 - mse: 8.2457\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4619 - mse: 9.7224\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4535 - mse: 9.4598\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4279 - mse: 9.0569\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5580 - mse: 12.1597\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4034 - mse: 8.9116\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4826 - mse: 9.9877\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4045 - mse: 8.9233\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4795 - mse: 10.1094\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5058 - mse: 9.6957\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3863 - mse: 8.5310\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5637 - mse: 11.7913\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4624 - mse: 9.5656\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4013 - mse: 8.2000\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4663 - mse: 9.5759\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4481 - mse: 9.4357\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4622 - mse: 9.8518\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4853 - mse: 10.7188\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4037 - mse: 8.4466\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5359 - mse: 11.7731\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4271 - mse: 9.2156\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3872 - mse: 7.4612\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3538 - mse: 7.5091\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4143 - mse: 8.5423\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3804 - mse: 7.9153\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4593 - mse: 9.9759\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4603 - mse: 9.6858\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.3370 - mse: 6.5678\n",
            "Mean Absolute Error, Mean Squared Error:  [1.3370306491851807, 6.567843914031982]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error:  [1.1034969091415405, 4.20483922958374]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8EPbw-t1kEU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1bce540d-d5e3-47a2-e662-52b3f886fb85"
      },
      "source": [
        "# RoBERTa\n",
        "cnn = train_cnn(roberta_embeddings_hair, train_labels)\n",
        "results = cnn.evaluate(roberta_embeddings_hair_test, test_labels)\n",
        "print(\"Mean Absolute Error, Mean Squared Error: \", results)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error:  [1.096874475479126, 4.2369842529296875]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 1s 3ms/step - loss: 10.3868 - mse: 183.9371\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 8.4864 - mse: 122.9593\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 6.9309 - mse: 79.4580\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 5.6594 - mse: 55.2301\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 4.3554 - mse: 33.7093\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 3.3545 - mse: 21.7102\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.8056 - mse: 16.7771\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.4768 - mse: 14.9388\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.0583 - mse: 10.5381\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.9477 - mse: 11.6070\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.8076 - mse: 10.8112\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6345 - mse: 9.4751\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.7102 - mse: 10.8541\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6224 - mse: 10.6888\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6231 - mse: 9.9235\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6416 - mse: 11.8055\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4873 - mse: 9.1063\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4868 - mse: 8.3095\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5088 - mse: 8.7812\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4703 - mse: 8.7123\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5617 - mse: 10.6295\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5551 - mse: 10.4874\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4891 - mse: 9.4338\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3767 - mse: 7.2350\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4516 - mse: 9.5855\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3682 - mse: 8.0167\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4282 - mse: 8.3608\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4802 - mse: 9.2491\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5556 - mse: 11.0155\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4295 - mse: 9.6066\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4364 - mse: 8.4776\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4372 - mse: 8.3982\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3761 - mse: 7.7007\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4470 - mse: 9.3338\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5108 - mse: 10.3020\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4433 - mse: 9.2125\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4038 - mse: 7.9281\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4356 - mse: 10.1364\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4648 - mse: 9.4292\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4148 - mse: 8.2568\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3210 - mse: 6.7295\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4573 - mse: 9.1352\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5305 - mse: 10.1642\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4935 - mse: 10.2371\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4784 - mse: 9.3208\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4884 - mse: 10.1888\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4767 - mse: 10.3221\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6034 - mse: 11.8348\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5131 - mse: 10.0054\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4793 - mse: 9.1256\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4939 - mse: 9.9237\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3774 - mse: 7.9349\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4380 - mse: 9.7562\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4188 - mse: 8.1852\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3613 - mse: 8.1828\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4984 - mse: 10.3798\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4335 - mse: 9.2242\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4746 - mse: 9.2735\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4465 - mse: 8.7503\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4138 - mse: 8.8604\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4971 - mse: 9.6585\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4140 - mse: 8.2817\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4963 - mse: 10.4695\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4610 - mse: 9.4717\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4687 - mse: 10.2911\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3859 - mse: 8.0415\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4158 - mse: 8.0795\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4166 - mse: 9.0501\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4886 - mse: 9.2077\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3316 - mse: 7.5602\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4082 - mse: 9.0625\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3643 - mse: 7.8652\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4510 - mse: 9.2412\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3739 - mse: 8.2939\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3971 - mse: 9.5558\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4398 - mse: 8.5580\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5366 - mse: 11.2611\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4568 - mse: 9.5352\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4845 - mse: 8.7118\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4467 - mse: 9.1021\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5057 - mse: 11.1628\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4840 - mse: 9.6209\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4546 - mse: 8.8500\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4722 - mse: 9.8436\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3476 - mse: 7.8897\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4206 - mse: 8.5034\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.2907 - mse: 6.6113\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3677 - mse: 7.9370\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3887 - mse: 8.5831\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4112 - mse: 9.0306\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4230 - mse: 9.2094\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3757 - mse: 7.0821\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4663 - mse: 10.0583\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4404 - mse: 8.8256\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4311 - mse: 8.8925\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4675 - mse: 9.1293\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4825 - mse: 9.6494\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4374 - mse: 8.6081\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4214 - mse: 9.5165\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6071 - mse: 12.2564\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3385 - mse: 6.5864\n",
            "Mean Absolute Error, Mean Squared Error:  [1.338531255722046, 6.586434364318848]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error:  [1.096874475479126, 4.2369842529296875]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGPALrPL1kEU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83eb5fcf-66dc-4711-b97c-28ba6ea21aac"
      },
      "source": [
        "# BERT\n",
        "cnn = train_cnn(bert_embeddings_hair, train_labels)\n",
        "results = cnn.evaluate(bert_embeddings_hair_test, test_labels)\n",
        "print(\"Mean Absolute Error, Mean Squared Error: \", results)\n",
        "\"\"\"\n",
        "Mean Absolute Error, Mean Squared Error:  [1.0969746112823486, 4.288430213928223]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 1s 4ms/step - loss: 4.9289 - mse: 44.2796\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 3.8069 - mse: 25.8879\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 3.1898 - mse: 21.7969\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.6972 - mse: 15.8188\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.2985 - mse: 13.0689\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.9688 - mse: 10.1394\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.9911 - mse: 11.9988\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.7148 - mse: 10.4861\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6843 - mse: 9.7888\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5878 - mse: 8.3759\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.7574 - mse: 12.0339\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6155 - mse: 11.2959\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6809 - mse: 11.2043\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.7095 - mse: 13.2237\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5225 - mse: 9.4402\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5267 - mse: 9.7842\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.6305 - mse: 11.3881\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5709 - mse: 10.0976\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3711 - mse: 6.9289\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4638 - mse: 9.0739\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4206 - mse: 8.4189\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5537 - mse: 9.7349\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4596 - mse: 8.9858\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4979 - mse: 10.0531\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6066 - mse: 10.8437\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4508 - mse: 9.3249\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4527 - mse: 9.6639\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3954 - mse: 7.5041\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4519 - mse: 7.9681\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5496 - mse: 11.3016\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4517 - mse: 10.2212\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4133 - mse: 8.6259\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4310 - mse: 9.3545\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3601 - mse: 7.2025\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.6055 - mse: 11.4347\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5803 - mse: 11.5529\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4177 - mse: 8.5065\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3898 - mse: 8.6748\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4487 - mse: 8.8147\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4071 - mse: 8.2279\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4942 - mse: 10.0554\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3620 - mse: 7.8184\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5381 - mse: 11.0175\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4118 - mse: 9.4272\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3792 - mse: 8.1908\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4320 - mse: 10.1129\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4443 - mse: 9.4678\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4149 - mse: 7.8270\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4607 - mse: 10.1942\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3588 - mse: 7.5841\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4287 - mse: 8.9086\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5635 - mse: 10.6354\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5992 - mse: 12.8024\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.5198 - mse: 10.1898\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4674 - mse: 10.2919\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4383 - mse: 9.0535\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5039 - mse: 9.9703\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4908 - mse: 10.8181\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4974 - mse: 10.3870\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3919 - mse: 8.5300\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4619 - mse: 10.1892\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4973 - mse: 10.6497\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3992 - mse: 8.7753\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4231 - mse: 8.9924\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5543 - mse: 11.9178\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4163 - mse: 8.2921\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4275 - mse: 9.1298\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4978 - mse: 9.9119\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4425 - mse: 8.9236\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4484 - mse: 9.7536\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.4260 - mse: 8.3860\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4341 - mse: 8.8293\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4033 - mse: 8.3503\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3171 - mse: 7.4669\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4414 - mse: 8.7933\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4300 - mse: 8.6737\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4173 - mse: 8.5143\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4335 - mse: 8.8904\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4745 - mse: 9.8824\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3660 - mse: 7.9522\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4833 - mse: 10.4510\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3805 - mse: 8.1495\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5388 - mse: 10.7021\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5117 - mse: 10.0132\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4205 - mse: 8.9038\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4277 - mse: 8.8369\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4812 - mse: 9.9215\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4436 - mse: 8.5813\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4330 - mse: 8.1499\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3554 - mse: 7.8424\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4104 - mse: 9.4610\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.5209 - mse: 10.3816\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4069 - mse: 8.1133\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4350 - mse: 8.9509\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.3552 - mse: 8.1673\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.3397 - mse: 8.2294\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4257 - mse: 9.1573\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4671 - mse: 9.6648\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4340 - mse: 9.0366\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.4449 - mse: 9.3567\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3399 - mse: 6.5629\n",
            "Mean Absolute Error, Mean Squared Error:  [1.33991277217865, 6.562948703765869]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMean Absolute Error, Mean Squared Error:  [1.0969746112823486, 4.288430213928223]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    }
  ]
}