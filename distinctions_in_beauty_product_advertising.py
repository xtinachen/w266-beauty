# -*- coding: utf-8 -*-
"""DistinctionsInBeautyProductAdvertising.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CY8bmyC_XtV4CywyEOR3gdAqAAUoHwyc

# Distinctions in Beauty Product Advertising

# Libraries
"""

# All of the data is in a shared Google Drive folder. Mount drive for access. 
from google.colab import drive
drive.mount('/content/gdrive')

# Install textstat to analyze text complexity.
!pip install textstat

# Install Transformers for Embeddings.
!pip install transformers

# General Loading and Processing.
from google.colab import drive
import pandas as pd 
import spacy
import numpy as np
import glob
import random
from collections import defaultdict
import matplotlib.pyplot as plt
import itertools
from scipy.stats import zscore
from scipy import stats
import random

# Text Processing.
import textstat
import nltk
from nltk.corpus import wordnet as wn
from nltk import word_tokenize
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('averaged_perceptron_tagger')
nltk.download('vader_lexicon')
nltk.download('punkt')  
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from collections import Counter
from collections import OrderedDict

# Modeling.
import tensorflow_hub as hub
import tensorflow as tf
from sklearn import preprocessing
import transformers as transformer
import torch
import gensim
import gensim.downloader
from tqdm import tqdm
import keras

"""# Load Data"""

# Truncate size of output.
pd.set_option('max_colwidth', 400)

"""### Lotion Products"""

lotions_path = '/content/gdrive/MyDrive/266/final/data/lotion_scraped_latest_cl.csv'
lotion_df = pd.read_csv(lotions_path)

"""### Hair Products"""

hair_path = '/content/gdrive/MyDrive/266/final/data/hair_scraped_latest_cl.csv'
hair_df = pd.read_csv(hair_path)

"""### All Junglescout Products"""

raw_junglescout_path = '/content/gdrive/MyDrive/266/final/data/junglescout/all_raw_junglescout_categorized.csv'
all_products_df = pd.read_csv(raw_junglescout_path)

"""## Build a Corpus of Marketing Terms

For extracting marketing terms, we tokenized titles and tagged them with their parts of speech using the nltk word_tokenize and pos_tag functions. After extracting all adjectives, we filtered out what we felt were purely marketing terms.
"""

adjective_tags = ["JJ", "JJR", "JJS"] 
adjectives = set()

# Tokenize data and extract adjectives. 
for product_title in lotion_df["product_title_cl"]:
  words_and_tags = nltk.pos_tag(word_tokenize(product_title))
  for word, tag in words_and_tags:
    if tag in adjective_tags:
      adjectives.add(word)

"""# Lotion Analysis

## Data Manipulation
"""

# Reduce dataset to only columns we're interested in. 
lotion_df = lotion_df[["product_id", "product_title_cl", 'title_cl', 'about_cl', 'description_cl', "price_cl", 'price_unit', 'ingredients', "category", "full_text", "title"]]
# Remove all products where the unit price is NAN
lotion_df = lotion_df[lotion_df['price_unit'].notna()]
# Reset the index to ensure linearity. 
lotion_df = lotion_df.reset_index()

lotion_df.shape

"""Remove Outliers: [Source](https://www.kite.com/python/answers/how-to-remove-outliers-from-a-pandas-dataframe-in-python)"""

# Remove Outliers 
z_scores = stats.zscore(lotion_df["price_unit"].values)
abs_z_scores = np.abs(z_scores).reshape(-1,1)
filtered_entries = (abs_z_scores < 1.7).all(axis=1)
lotion_df = lotion_df[filtered_entries]

# Number of unique product_ids.  
len(lotion_df.product_id.unique())

# Examine the sub-categories in the lotion dataset. 
lotion_df["category"].value_counts()

# Examine the distribution of price per unit (ounces).
plt.hist(lotion_df['price_unit'], range=[0,40])
plt.show()

lotion_df["price_unit"].describe()

# Value to split products into lower and higher priced lotion products
cutoff_lotion = lotion_df["price_unit"].median()
cutoff_lotion

"""## Freedman and Jurafsky Analysis (Scaled)

### Falutin

Flesch Reading Score and Grade Level
"""

# The flesch kincaid grade indicates the grade that a student would be able to read
# the text. For example, a 9.3 means a ninth grader could read the text. 
lotion_df["flesch_kincaid_grade"] = lotion_df["full_text"].apply(textstat.flesch_kincaid_grade)

# The flesch read ease score has no lower bound, but has an upper bound of 121.22. 
# The lower the score is, the harder the text is to read. 
lotion_df["reading_ease"] = lotion_df["full_text"].apply(textstat.flesch_reading_ease)

"""Number of Words"""

# Count the number of words present in the text (does not include punctuation by default).
lotion_df["lexicon_count"] = lotion_df["full_text"].apply(textstat.lexicon_count)

"""Word Commonality"""

# Raw marketing terms (hand curated).
marketing_term_corpus = ["absorbing","active","advanced","all natural","aloe","aluminum","anti aging",
     "antioxidant","antiwrinkle","attractant","authentic","best","botanic","bpa",
     "certified","classic","clean","cleanser","clear","clinically","coconut",
     "collagen","comedogenic","cosmeceutical","creamy","curvy","deep","dermatologist",
     "double","dry","dual","elite","essential","essential oil","esthetic","extra",
     "facial","fair","fda","finish","fragrance","fragrant","free","fullest","gentle",
     "glow","gluten","glycolic","green","healthiest","healthy","herbal","hygienic",
     "hypoallergenic","iluminate","incredible","intense","intensive","keeper","kosher",
     "lavish","light","liquid","lucious","lux","maximum","microbiotic","miracle",
     "multifunctional","natural","neutral","nongreasy","norwegian","odorfree","oilcontrol",
     "oilfree","olive","organic","original","pabafree","paraben","parabenfree",
     "phytoactive","poppy","premium","protective","pure","purest","purpose","rapid",
     "raw","resistant","restorative","rich","royale","safe","select","sensitive",
     "shave","shea","sheamoisture","silicon","silicone","silk","skin","smart","soft",
     "solar","soothe","soothing","spotlight","stress","sublime","suntan","tan",
     "texturized","therapy","touch","transformative","tropical","ultimate","unisex",
     "unprocessed","uplift","urban","uv","uva","uvb","vegan","virgin","vitae",
     "vitamin","youthful","zen"]

# Lemmatized.
marketing_term_corpus_cl = ['absorb', 'activ', 'advanc', 'natur', 'alo', 'aluminum', 
                            'anti age', 'antioxid', 'antiwrinkl', 'attract', 'authent', 
                            'best', 'botan', 'bpa', 'certifi', 'classic', 'clean', 'cleanser', 
                            'clear', 'clinic', 'coconut', 'collagen', 'comedogen', 'cosmeceut', 
                            'creami', 'curvi', 'deep', 'dermatologist', 'doubl', 'dri', 'dual', 
                            'elit', 'essenti', 'essenti oil', 'esthet', 'extra', 'facial', 'fair', 
                            'fda', 'finish', 'fragranc', 'fragrant', 'free', 'fullest', 'gentl', 
                            'glow', 'gluten', 'glycol', 'green', 'healthiest', 'healthi', 'herbal', 
                            'hygien', 'hypoallergen', 'ilumin', 'incred', 'intens', 'intens', 
                            'keeper', 'kosher', 'lavish', 'light', 'liquid', 'luciou', 'lux', 
                            'maximum', 'microbiot', 'miracl', 'multifunct', 'natur', 'neutral', 
                            'nongreasi', 'norwegian', 'odorfre', 'oilcontrol', 'oilfre', 'oliv', 
                            'organ', 'origin', 'pabafre', 'paraben', 'parabenfre', 'phytoact', 
                            'poppi', 'premium', 'protect', 'pure', 'purest', 'purpos', 'rapid', 
                            'raw', 'resist', 'restor', 'rich', 'royal', 'safe', 'select', 
                            'sensit', 'shave', 'shea', 'sheamoistur', 'silicon', 'silicon', 
                            'silk', 'skin', 'smart', 'soft', 'solar', 'sooth', 'sooth', 
                            'spotlight', 'stress', 'sublim', 'suntan', 'tan', 'textur', 'therapi', 
                            'touch', 'transform', 'tropic', 'ultim', 'unisex', 'unprocess', 'uplift', 
                            'urban', 'uv', 'uva', 'uvb', 'vegan', 'virgin', 'vita', 'vitamin', 'youth', 'zen']

# Count the occurances of marketing terms in the text. 
def count_marketing_terms(text):
  count = 0
  for word in text.split(" "):
    if word in marketing_term_corpus_cl: 
      count += 1 
  return count

# Raw count of marketing terms present. 
lotion_df["marketing_term_count_raw"] = lotion_df["full_text"].apply(count_marketing_terms)

# Examine falutin variables (as a whole).
lotion_df[["flesch_kincaid_grade","reading_ease", "lexicon_count","marketing_term_count_raw" ]].describe()

# Split dataset based on the median of the price. 
lower_priced_products = lotion_df[lotion_df['price_unit'] <= cutoff_lotion]
higher_priced_products = lotion_df[lotion_df['price_unit'] > cutoff_lotion]

# Examine distribution of falutin variables for lower priced products. 
lower_priced_products[["flesch_kincaid_grade","reading_ease", "lexicon_count","marketing_term_count_raw" ]].describe()

# Examine distribution of falutin variabels for higher priced products. 
higher_priced_products[["flesch_kincaid_grade","reading_ease", "lexicon_count","marketing_term_count_raw" ]].describe()

"""### Distinction

#### Unique and Comparator Words: Baseline

As a baseline anlaysis, we hand-curated a list of words that indicate "uniqueness". This feature will just indicate the number of unique words present in the title.
"""

unique_and_comparator_words = ["unique", "different", "most", "distinct", "special", "single", "only", "new", 'premium', 'ultra',"more", "less", "most", "better", "lower", "higher", "far", "farther", "little", "simpler", "least"]
# most and only got removed due to stop words, add back in? 
unique_and_comparator_words_cl = ['uniqu', 'differ', 'distinct', 'special', 'singl', 'new', 'premium', 'ultra','less', 'better', 'lower', 'higher', 'far', 'farther', 'littl', 'simpler', 'least'] 
def count_unique_and_comparator_words(text):
  count = 0
  for word in unique_and_comparator_words_cl:
    if word in text.lower().split(' '):
      count += 1
  return count 

# lotion_df["unique_word_count_basic"] = lotion_df["product_title_cl"].apply(count_unique_and_comparator_words)
lotion_df["unique_word_count_basic"] = lotion_df["full_text"].apply(count_unique_and_comparator_words)
lotion_df["unique_word_count_basic"].describe()

"""This method provided next to no signal.

#### Unique and Comparator Words: Improvement

We tried two different things to improve this signal. 

**1. Unigrams, Bigrams, Trigrams Using a Custom Word2Vec Model**

First, we extracted unigrams, bigrams and trigrams. We combined the bigrams and trigrams by underscores to create unique unigrams capturing the two or three words. Then we created our own embeddings using a custom Word2Vec model.
"""

# Get unigrams, bigrams and trigrams of product title.
def connect_by_underscore(ngram_list):
  new_list = []
  for ngram in ngram_list:
    new_list.append("_".join(ngram))
  return new_list
    
# Extract unigrams, bigrams and trigrams. 
# Think we should remove prod from unigram/bigram/trigram.
lotion_df["unigrams"] = lotion_df["full_text"].apply(lambda x: np.array(str.split(x)))
lotion_df["bigrams"] = lotion_df["unigrams"].apply(nltk.bigrams).apply(list).apply(connect_by_underscore)
lotion_df["trigrams"] = lotion_df["unigrams"].apply(nltk.trigrams).apply(list).apply(connect_by_underscore)

# Examine Unigrams
lotion_df["unigrams"][:5]

# Create a single vocabulary out of all unigrams, bigrams and trigrams.
vocab_unigrams_bigrams_trigrams = list(itertools.chain.from_iterable(list([lotion_df["unigrams"].values,lotion_df["bigrams"].values,lotion_df["trigrams"].values])))
vocab_unigrams_bigrams_trigrams = list(itertools.chain.from_iterable(vocab_unigrams_bigrams_trigrams))
print(vocab_unigrams_bigrams_trigrams[-5:])

# Create word vectors for the vocabulary using Word2Vec.
custom_model = gensim.models.Word2Vec([vocab_unigrams_bigrams_trigrams])
custom_model.save('custom.embedding')
custom_model = gensim.models.Word2Vec.load('custom.embedding')

# base_word_list: terms to match to (ex. unigue_words_cl).
# model: Word2Vec model.
def extract_similar_words(base_word_list, model):
  counts = {product_id: 0 for product_id in lotion_df["product_id"]}
  other_unique_and_comparator_words = set()
  # Guarantee base words are in the models vocabulary.
  official_base_word_list = [word for word in base_word_list if word in model.wv.vocab]
  # For each word in the base_word_list, get the word vector.
  for base_word in official_base_word_list:
    # For each product_title.
    for product_title, product_id in zip(lotion_df["full_text"], lotion_df["product_id"]): 
      # For each word.
      for word in str(product_title).split(" "):
        # Check if its in the dictionary (to avoid key errors).
        if word in model.wv.vocab:
          # If word in vocab and word_vector similarity above x, add word to list 
          # and increase count of similar words.
          if model.similarity(base_word,word) >= 0.5:
            other_unique_and_comparator_words.add(word)
            counts[product_id] += 1


  return counts, other_unique_and_comparator_words

# Find the list and count of marketing terms in the product text that has 
# a high cosine similarity to the unique/custom word list. 
counts_custom, other_unique_and_comparator_words_custom = extract_similar_words(unique_and_comparator_words_cl, custom_model)
plt.hist(counts_custom.values())
plt.show()

# Merge the counts back into the main df using a join on product_id.
lotion_df = lotion_df.merge(pd.DataFrame(list(counts_custom.items()),columns = ['product_id','mktg_embedding_counts_custom']), how="inner")

# Examine the distribution of the number of marketing terms present 
# (based on cosine similarity).
lotion_df['mktg_embedding_counts_custom'].describe()

# Examine words in the text that had a high cosine similarity to the unique/comparator word list.
list(other_unique_and_comparator_words_custom)[:min(len(other_unique_and_comparator_words_custom), 25)]

# Split products based on the median price to analyze any significant differences 
# in the distribution of the number marketing terms present. 
lower_priced_products = lotion_df[lotion_df['price_unit'] <= cutoff_lotion]
higher_priced_products = lotion_df[lotion_df['price_unit'] > cutoff_lotion]

# Examine distribution of the number of marketing terms in the text for lower 
# priced products.
lower_priced_products["mktg_embedding_counts_custom"].describe()

plt.hist(lower_priced_products["mktg_embedding_counts_custom"])
plt.show()

# Examine distribution of the number of marketing terms in the text for higher 
# priced products. 
higher_priced_products["mktg_embedding_counts_custom"].describe()

plt.hist(higher_priced_products["mktg_embedding_counts_custom"])
plt.show()

"""**2. Pre-trained Glove Embeddings on Unigrams**

The next thing we tried to do to improve on the baseline was to use only unigrams and pre-trained word vectors.
"""

# Only use unigrams, since bigrams and trigrams joined on '_' won't be in vocab. 
vocab_unigrams = list(itertools.chain.from_iterable(lotion_df["unigrams"]))
print(vocab_unigrams[:10])

# Download pre-trained word vectors.
glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')

# Extract words and the count of those words in each text using cosine similarity to
# the hand curated unique/comparator word list. 
counts_glove, other_unique_and_comparator_words_glove = extract_similar_words(unique_and_comparator_words_cl, glove_vectors)

# Merge the counts back into the main df using a join on product_id.
lotion_df = lotion_df.merge(pd.DataFrame(list(counts_glove.items()),columns = ['product_id','mktg_embedding_counts_glov']), how="inner")

# Examine the distribution of the number of marketing terms present 
# (based on cosine similarity).
lotion_df['mktg_embedding_counts_glov'].describe()

# Examine words in the text that had a high cosine similarity to the unique/comparator word list.
list(other_unique_and_comparator_words_glove)[:min(len(other_unique_and_comparator_words_glove), 25)]

# Split products based on the median price to analyze any significant differences 
# in the distribution of the number marketing terms present. 
lower_priced_products = lotion_df[lotion_df['price_unit'] <= cutoff_lotion]
higher_priced_products = lotion_df[lotion_df['price_unit'] > cutoff_lotion]

# Examine distribution of the number of marketing terms in the text for lower 
# priced products.
lower_priced_products["mktg_embedding_counts_glov"].describe()

plt.hist(lower_priced_products["mktg_embedding_counts_glov"])
plt.show()

# Examine distribution of the number of marketing terms in the text for higher 
# priced products. 
higher_priced_products["mktg_embedding_counts_glov"].describe()

plt.hist(higher_priced_products["mktg_embedding_counts_glov"])
plt.show()

"""#### Sentiment Analysis"""

# Extract sentiment scores for each text using nltk SentimentIntensityAnalyzer.
sentiment_analyzer = SentimentIntensityAnalyzer()
sentiment_df = lotion_df["full_text"].apply(sentiment_analyzer.polarity_scores).to_frame()
sentiment_df  = sentiment_df["full_text"].apply(pd.Series)
# Add results to the main dataframe. 
lotion_df["pos"] = sentiment_df["pos"]
lotion_df["neg"] = sentiment_df["neg"]
lotion_df["neu"] = sentiment_df["neu"]
lotion_df["compound"] = sentiment_df["compound"]
# Examine values
lotion_df[["pos","neg","neu","compound"]].head()

# Split products based on price. 
lower_priced_products = lotion_df[lotion_df["price_unit"] <= cutoff_lotion] 
higher_priced_products = lotion_df[lotion_df["price_unit"] > cutoff_lotion]

# Examine sentiment scores for the lower priced products.
lower_priced_products[["pos","neg","neu","compound"]].describe()

# Examine sentiment scores for the higher priced products. 
higher_priced_products[["pos","neg","neu","compound"]].describe()

"""### Authenticity

#### Term Frequency - Inverse Document Frequency  
TFIDF gives a score for each token/word in 1 document of documents cominbed into one. For authenticy, we want the words and its tfidf over the whole lower price "document" and higher price "document" (lower_priced_products, higher_priced_products)
"""

# Create TFIDF function
def tfidf(column, col_name):
  tfIdfVectorizer=TfidfVectorizer(use_idf=True)
  column = column.dropna().tolist()
  text = [' '.join(column)] # makes into a list with one item
    #np.unique(np.hstack(column)).tolist()
  print(text)
  tfIdf = tfIdfVectorizer.fit_transform(text)
  # tfIdf[#] makes it only look at that row - need to fix
  df_name = pd.DataFrame(tfIdf.T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[col_name])
  df_name = df_name.sort_values(col_name, ascending=False)
  return df_name

#  Create TFIDF data for lower priced lotions: titles, about, and description
lower_title_tf = tfidf(lower_priced_products.title_cl, col_name='l_title_tfidf')
lower_about_tf = tfidf(lower_priced_products.about_cl, col_name='l_about_tfidf')
lower_desc_tf = tfidf(lower_priced_products.description_cl, col_name='l_desc_tfidf')

#  Create TFIDF data for higher priced lotions: titles, about, and description
higher_title_tf = tfidf(higher_priced_products.title_cl, col_name='h_title_tfidf')
higher_about_tf = tfidf(higher_priced_products.about_cl, col_name='h_about_tfidf')
higher_desc_tf = tfidf(higher_priced_products.description_cl, col_name='h_desc_tfidf')

# Create TFIDF dataframe for lower priced products
lower_tf = lower_title_tf.join(lower_about_tf, how='outer')
lower_tf = lower_tf.join(lower_desc_tf, how='outer')
lower_tf['word'] = lower_tf.index
lower_tf.sort_values(by=['l_desc_tfidf'], ascending=False).head(30)

# Create TFIDF dataframe for higher priced products
higher_tf = higher_title_tf.join(higher_about_tf, how='outer')
higher_tf = higher_tf.join(higher_desc_tf, how='outer')
higher_tf['word'] = higher_tf.index
higher_tf.sort_values(by=['h_desc_tfidf'], ascending=False).head(30)

# Create joined TFIDF dataframe
tfidf_joined = lower_tf.merge(higher_tf, how='outer', on ='word')
tfidf_joined = tfidf_joined[['word', 'l_title_tfidf', 'h_title_tfidf',	'l_about_tfidf', 'h_about_tfidf', 'l_desc_tfidf', 'h_desc_tfidf']]
tfidf_joined.sort_values(by='l_title_tfidf', ascending=False) # sort to see, change by accordingly

"""### Health


*   Ingredients from Amazon Products 
*   Toxicity Levels from California Chemicals in Cosmetics Database 


"""

# Check if ingredient exists in our data
lotion_df[lotion_df['ingredients'].str.contains('retinol', na=False)][["product_title_cl","ingredients"]]

lotion_df.ingredients.unique()[0:3]

toxic_chem_li = ['titanium dioxide',
 'distillates',
 'estragole',
 'cocamide diethanolamine',
 'toluene',
 'chromium',
 'retinol',
 'retinol/retinyl esters',
 'vitamin a',
 'vitamin a palmitate',
 'butylated hydroxyanisole',
 'coffea arabica extract',
 'lauramide diethanolamine',
 'silica',
 'carbon black',
 'genistein',
 'progesterone',
 '2,4-hexadienal',
 'methyleugenol',
 'carbon-black extracts',
 'retinyl palmitate',
 'o-phenylphenol',
 'acrylamide',
 'formaldehyde',
 'ginkgo biloba extract',
 'mica',
 'ethylene glycol',
 'acetic acid',
 'ethyl acrylate',
 'trade secret',
 'methanol',
 'mineral oils',
 'diethanolamine',
 'tea-lauryl sulfate',
 'retinyl acetate',
 'lead acetate',
 'talc',
 'triethanolamine',
 'o-phenylenediamine and its salts',
 'safrole',
 'styrene',
 'acetaldehyde',
 'cocamide dea',
 '1,4-dioxane',
 'arsenic',
 'dichloroacetic acid',
 'ethylene oxide',
 'lead',
 'dichloromethane',
 'benzene',
 'benzyl chloride',
 'n-nitrosodimethylamine',
 'propylene oxide',
 'methyl chloride',
 'cadmium and cadmium compounds',
 'n-methylpyrrolidone',
 'di-n-butyl phthalate',
 'coal tars',
 'all-trans retinoic acid',
 'quinoline and its strong acid salts',
 'methylene glycol',
 'benzophenone',
 'cocamide',
 'lauramide dea',
 'aloe vera',
 'musk xylene',
 'aspirin',
 'coal tar',
 'benzophenone-3',
 'quartz',
 'talc containing asbestiform fibers',
 'sodium bromate',
 'phenacetin',
 'mercury and mercury compounds',
 'p-aminodiphenylamine',
 'permethrin',
 'acetylsalicylic acid',
 'coal tar extract',
 'selenium sulfide',
 'oil orange ss',
 'spironolactone',
 'nickel',
 'caffeic acid',
 'cocamide mea',
 'cosmetic talc',
 'c.i. acid red 114',
 'caffeine',
 'benzophenone-4',
 'ethanol in alcoholic beverages',
 'coffee extract',
 'retinol palmitate',
 'coffee bean extract',
 'propylene glycol mono-t-butyl ether',
 'avobenzone',
 'coal tar solution',
 'pulegone',
 'beta-myrcene',
 '2,2-bis(bromomethyl)-1,3-propanediol',
 'benzo[a]pyrene',
 'benz[a]anthracene',
 'extract of coffee bean',
 'goldenseal root powder',
 'isopropyl alcohol manufacture using strong acids',
 '2-propyleneacrolein',
 'n,n-dimethyl-p-toluidine',
 'formaldehyde solution',
 'n-nitrosodiethanolamine',
 'benzophenone-2',
 'vinyl acetate',
 'trichloroacetic acid',
 'phenacemide',
 'polygeenan',
 'diethanolamides of the fatty acids of coconut oil',
 'bisphenol a',
 'hydrous magnesium silicate']

# If ingredients contain toxic ingredient, label how many, else 0. 
def check_toxic(ing):
  counter = 0
  for i in toxic_chem_li:
    if i in ing.lower().split(', '): 
      counter += 1
  
  return counter

# If a product  contain toxic ingredients, create a list of them joined on |.
def check_toxic_ingredient(ing):
  li = []
  for i in toxic_chem_li:
    if i in ing.lower().split(', '): 
      li.append(i)
  return '|'.join(li)

# Retrieve count of toxic ingredients.
lotion_df['toxic_count'] = lotion_df['ingredients'].apply(lambda x: check_toxic(x) if pd.notnull(x) else None)

# Retrieve list of toxic ingredients. 
lotion_df['toxic_ing'] = lotion_df['ingredients'].apply(lambda x: check_toxic_ingredient(x) if pd.notnull(x) else None)

# Split products based on price. 
lower_priced_products = lotion_df[lotion_df["price_unit"] <= cutoff_lotion] 
higher_priced_products = lotion_df[lotion_df["price_unit"] > cutoff_lotion]

# Examine distribution of toxic ingredients for lower priced products. 
lower_priced_products["toxic_count"].describe()

# Examine distribution of toxic ingredients for higher  priced products.
higher_priced_products["toxic_count"].describe()

"""## Modeling"""

# Truncate text to a max length of 100.
def truncate_or_fill(text):
  if len(text.split()) >= 100:
    return ' '.join(text.split()[:100])
  return text

# Truncate all text to a max length of 100
lotion_df["full_text_trunc"] = lotion_df["full_text"].apply(truncate_or_fill)

"""#### Neural Network (Definition) 

Used for the baseline and the improvement.[link text](https://)
"""

# Run a 3 layer Dense Neural Network with Dropout and Relu activation.
# Examine results interms of Mean Squared Error. 
def run_model(input_dims, train_features, train_labels, test_features, test_labels, epochs_):
  model = tf.keras.models.Sequential()
  model.add( tf.keras.layers.Dense(64, activation='relu',input_shape=input_dims))
  model.add( tf.keras.layers.Dropout(0.1))
  model.add(tf.keras.layers.Dense(64, activation='relu'))
  model.add( tf.keras.layers.Dense(1))

  loss_fn = tf.keras.losses.MeanAbsoluteError()

  model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['mse'])  
  
  history = model.fit(train_features,train_labels, epochs=epochs_)
  results = model.evaluate(test_features, test_labels)
  print("Mean Absolute Error, Mean Squared Error ", results)

"""### Train Test Split"""

# Split data into train and test.
random_df = pd.DataFrame(np.random.randn(lotion_df["full_text_trunc"].shape[0]))
mask = np.random.rand(len(random_df)) < 0.8
train_features = lotion_df[mask]
train_labels = lotion_df["price_unit"][mask].fillna(0)
test_features = lotion_df[~mask]
test_labels = lotion_df["price_unit"][~mask].fillna(0)

print(train_features.shape)
print(train_labels.shape)
print(test_features.shape)
print(test_labels.shape)

"""### Baseline
Run a Neural Network with features computed during the Freedman and Jurafsky Analysis
"""

# Scale a dataframe column
def scale_data(data):
  min_max_scaler = preprocessing.StandardScaler()
  data_scaled = min_max_scaler.fit_transform(data)
  return pd.DataFrame(data_scaled)

lotion_df["marketing_term_count_scaled"] = scale_data((lotion_df["marketing_term_count_raw"]+1).values.reshape(-1,1))
lotion_df["flesch_kincaid_grade_scaled"] = scale_data((lotion_df["flesch_kincaid_grade"]).values.reshape(-1,1))
lotion_df["reading_ease_scaled"] = scale_data((lotion_df["reading_ease"]).values.reshape(-1,1))
lotion_df["lexicon_count_scaled"] = scale_data((lotion_df["lexicon_count"]).values.reshape(-1,1))
lotion_df["mktg_embedding_counts_glov_scaled"] = scale_data((lotion_df["mktg_embedding_counts_glov"]).values.reshape(-1,1))
lotion_df["compound_scaled"] = scale_data((lotion_df["compound"]).values.reshape(-1,1))
lotion_df["toxic_count_scaled"] = scale_data((lotion_df["toxic_count"]).values.reshape(-1,1))

# Scaled Data
# Mean Absolute Error, Mean Squared Error  [6.7269392013549805, 275.2937316894531]
train = lotion_df[["marketing_term_count_scaled" , "flesch_kincaid_grade_scaled" , "reading_ease_scaled" , "lexicon_count_scaled" , "mktg_embedding_counts_glov_scaled" , "compound_scaled","toxic_count_scaled"]][mask]
test = lotion_df[["marketing_term_count_scaled" , "flesch_kincaid_grade_scaled" , "reading_ease_scaled" , "lexicon_count_scaled" , "mktg_embedding_counts_glov_scaled" , "compound_scaled","toxic_count_scaled"]][~mask]

# Results of using Unscaled Features. 
# Mean Absolute Error, Mean Squared Error  [6.72703218460083, 275.4627685546875]
train = lotion_df[["marketing_term_count_raw" , "flesch_kincaid_grade" , "reading_ease" , "lexicon_count" , "mktg_embedding_counts_glov" , "compound","toxic_count"]][mask]
test = lotion_df[["marketing_term_count_raw" , "flesch_kincaid_grade" , "reading_ease" , "lexicon_count" , "mktg_embedding_counts_glov" , "compound","toxic_count"]][~mask]

# Scaled and neu instead of compount
# Mean Absolute Error, Mean Squared Error  [6.727107048034668, 275.5451965332031]
train = lotion_df[[ "marketing_term_count_scaled" ,"flesch_kincaid_grade_scaled" , "reading_ease_scaled" , "lexicon_count_scaled" , "mktg_embedding_counts_glov_scaled","toxic_count_scaled" , "neu"]][mask]
test = lotion_df[["marketing_term_count_scaled" ,"flesch_kincaid_grade_scaled" , "reading_ease_scaled" , "lexicon_count_scaled" , "mktg_embedding_counts_glov_scaled" , "neu","toxic_count_scaled"]][~mask]

run_model((train.shape[1],), train, train_labels, test, test_labels, 200)

"""### Improvement 
Use Embeddings and pass them through a Neural Net as well as a Convolutional Neural Net.

#### ELMO - Contextualized Word Embeddings
"""

elmo_embeddings_train_path_lotion = '/content/gdrive/MyDrive/266/final/data/elmo_embeddings_train_lotion.npy'
  elmo_embedding_test_path_lotion = '/content/gdrive/MyDrive/266/final/data/elmo_embeddings_test_lotion.npy'

train_features.values

def create_and_save_elmo_embeddings(elmo_embeddings_train_path_, elmo_embedding_test_path_):
  # Load pre-trained ELMO model
  elmo = hub.load("https://tfhub.dev/google/elmo/3")

  # NO BATCH - crashes
  # tf_constants = tf.constant(train_features, dtype = tf.string) 
  # elmo_embeddings = elmo.signatures["default"](tf_constants)["default"]

  # BATCH
  # Training Embeddings
  tf_constants = tf.constant(train_features["full_text_trunc"], dtype = tf.string) 
  batch_size = 200
  embeddings = []
  for index in (np.arange(0, len(tf_constants), batch_size)):
    batch_embeddings = elmo.signatures["default"](tf_constants[index:index+batch_size])["elmo"]
    embeddings.append(batch_embeddings)

  elmo_embeddings = tf.concat(embeddings, axis=0)

  # Test Embeddings 
  tf_constants_test = tf.constant(test_features["full_text_trunc"], dtype = tf.string)
  embeddings_test = []
  for index in (np.arange(0, len(tf_constants_test), batch_size)):
    batch_embeddings = elmo.signatures["default"](tf_constants_test[index:index+batch_size])["elmo"]
    embeddings_test.append(batch_embeddings)

  elmo_embeddings_test = tf.concat(embeddings_test, axis=0)

  # Store embeddings so its faster to load and use them later
  np.save(elmo_embeddings_train_path_, elmo_embeddings)
  np.save(elmo_embedding_test_path_, elmo_embeddings_test)


create_and_save_elmo_embeddings(elmo_embeddings_train_path_lotion,elmo_embedding_test_path_lotion)

# Load pre-saved elmo word embeddings 
elmo_embeddings_lotion = np.load(elmo_embeddings_train_path_lotion)
elmo_embeddings_lotion_test = np.load(elmo_embedding_test_path_lotion)

print("elmo_embeddings_lotion shape: ", elmo_embeddings_lotion.shape)
print("elmo_embeddings_lotion_test shape: ", elmo_embeddings_lotion_test.shape)

"""#### BERT (Variations) Embeddings
Semi-Adapted from [Source](https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)
"""

def create_and_save_bert_embeddings(text, embeddings_path, bert_type="distilbert", batch_size=100,
                                    max_len=None):

  # Load Pre-Trained BERT Model 
  if bert_type == "distilbert":
    model_class, tokenizer_class, pretrained_weights = (transformer.DistilBertModel, transformer.DistilBertTokenizer, 'distilbert-base-uncased')
  elif bert_type == "roberta":
    model_class, tokenizer_class, pretrained_weights = (transformer.RobertaModel, transformer.RobertaTokenizer, 'roberta-base')
  else:
    model_class, tokenizer_class, pretrained_weights = (transformer.BertModel, transformer.BertTokenizer, 'bert-base-uncased')

  # Load pretrained model/tokenizer
  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
  model = model_class.from_pretrained(pretrained_weights)

  # Extract tokens in batches
  all_tokenized = []
  for index in tqdm(np.arange(0, len(text["full_text_trunc"]), batch_size), 
                    "Creating embeddings"):
    batch = text["full_text_trunc"][index:index+batch_size]
    tokenized = batch.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))
    all_tokenized.append(tokenized)

  # Get max len over all batches
  if max_len is None:
    max_len = 0
    for tokenized in all_tokenized:
      for i in tokenized.values:
        if len(i) > max_len:
            max_len = len(i)
  
  embeddings = []
  for tokenized in tqdm(all_tokenized, "Running model"):
    # Add padding so all inputs are the same size 

    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])
    # Apply attention mask to ignore padding 
    attention_mask = np.where(padded != 0, 1, 0)

    input_ids = torch.tensor(padded)  
    attention_mask = torch.tensor(attention_mask)

    with torch.no_grad():
      last_hidden_states = model(input_ids, attention_mask=attention_mask)

    # Retrieve word tokens
    embeddings.append(last_hidden_states[0][:,1:,:].numpy())

  # Since we did batches, concatenate into a single embeeddings list 
  embeddings = np.concatenate(embeddings, 0)  # N, 768
  # Save to file 
  np.save(embeddings_path,embeddings)
  print(embeddings.shape)
  return max_len

""" **DistilBERT**"""

distilbert_embeddings_train_path_lotion = "/content/gdrive/MyDrive/266/final/data/distilbert_embeddings_train_lotion.npy"
distilbert_embeddings_test_path_lotion =  "/content/gdrive/MyDrive/266/final/data/distilbert_embeddings_test_lotion.npy"

max_len_train = create_and_save_bert_embeddings(train_features, distilbert_embeddings_train_path_lotion  , bert_type="distilbert")
create_and_save_bert_embeddings(test_features, distilbert_embeddings_test_path_lotion, bert_type="distilbert", max_len=max_len_train)

# Load pre-saved DistilBERT word embeddings 
distilbert_embeddings_lotion = np.load(distilbert_embeddings_train_path_lotion)
distilbert_embeddings_lotion_test = np.load(distilbert_embeddings_test_path_lotion)

print("distilbert_embeddings_lotion shape: ", distilbert_embeddings_lotion.shape)
print("distilbert_embeddings_lotion_test shape: ", distilbert_embeddings_lotion_test.shape)

""" **RoBERTa**"""

roberta_embeddings_train_path_lotion = "/content/gdrive/MyDrive/266/final/data/roberta_embeddings_train_lotion.npy"
roberta_embeddings_test_path_lotion =  "/content/gdrive/MyDrive/266/final/data/roberta_embeddings_test_lotion.npy"

max_len_train = create_and_save_bert_embeddings(train_features, roberta_embeddings_train_path_lotion  , bert_type="roberta")
create_and_save_bert_embeddings(test_features, roberta_embeddings_test_path_lotion, bert_type="roberta", max_len=max_len_train)

# Load pre-saved RoBERTa word embeddings 
roberta_embeddings_lotion = np.load(roberta_embeddings_train_path_lotion)
roberta_embeddings_lotion_test = np.load(roberta_embeddings_test_path_lotion)

print("roberta_embeddings_lotion shape: ", roberta_embeddings_lotion.shape)
print("roberta_embeddings_lotion_test shape: ", roberta_embeddings_lotion_test.shape)

"""**BERT (original)**"""

bert_embeddings_train_path_lotion = "/content/gdrive/MyDrive/266/final/data/bert_embeddings_train_lotion.npy"
bert_embeddings_test_path_lotion =  "/content/gdrive/MyDrive/266/final/data/bert_embeddings_test_lotion.npy"

max_len_train = create_and_save_bert_embeddings(train_features, bert_embeddings_train_path_lotion  , bert_type="bert")
create_and_save_bert_embeddings(test_features, bert_embeddings_test_path_lotion, bert_type="bert", max_len=max_len_train)

# Load pre-saved RoBERTa word embeddings 
bert_embeddings_lotion = np.load(bert_embeddings_train_path_lotion)
bert_embeddings_lotion_test = np.load(bert_embeddings_test_path_lotion)

print("bert_embeddings_lotion shape: ", bert_embeddings_lotion.shape)
print("bert_embeddings_lotion_test shape: ", bert_embeddings_lotion_test.shape)

"""### Neural Network - Run for Improvement"""

# ELMO
run_model((1024,), elmo_embeddings_lotion, train_labels, elmo_embeddings_lotion_test, test_labels, 200)
"""
Mean Absolute Error, Mean Squared Error  [11.123788833618164, 4236.31884765625]
"""

# DistilBERT
run_model((768,), distilbert_embeddings_lotion, train_labels, distilbert_embeddings_lotion_test, test_labels, 200)
"""
Mean Absolute Error, Mean Squared Error  [6.725437641143799, 338.0096740722656]
"""

# RoBERTa
run_model((768,), roberta_embeddings_lotion, train_labels, roberta_embeddings_lotion_test, test_labels, 200)
"""
Mean Absolute Error, Mean Squared Error  [6.730209827423096, 325.2977600097656]
"""

# BERT
run_model((768,), bert_embeddings_lotion, train_labels, bert_embeddings_lotion_test, test_labels, 200)
"""
Mean Absolute Error, Mean Squared Error  [5.8421783447265625, 309.8191223144531]
"""

"""### Convolutional Neural Network"""

def train_cnn(embeddings_train, train_labels, epochs=100):    
  # Specify model hyperparameters.
    num_filters = [4, 4, 4, 4]
    kernel_sizes= [4, 2, 3, 4]
    dense_layer_dims = [64,64]
    dropout_rate = 0.85
    print(embeddings_train.shape)
    # Input is a special "layer".  It defines a placeholder that will be overwritten by the training data.
    word_embeddings = keras.layers.Input(shape=(embeddings_train.shape[1],embeddings_train.shape[2]))

    # Construct "filters" randomly initialized filters with dimension "kernel_size" for each size of filter we want.
    conv_layers_for_all_kernel_sizes = []
    for kernel_size, filters in zip(kernel_sizes, num_filters):
        conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(word_embeddings)
        conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)
        conv_layers_for_all_kernel_sizes.append(conv_layer)

    # Concat the feature maps from each different size.
    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)

    # Add dropout to improve generalization
    h = keras.layers.Dropout(rate=dropout_rate)(h)

    # A fully connected layer for each dense layer dimension in dense_layer_dims.
    for dense_layer_dimension in dense_layer_dims:
        keras.layers.Dense(dense_layer_dimension, activation='relu')

    prediction = keras.layers.Dense(1)(h)

    model = keras.Model(inputs=word_embeddings, outputs=prediction)
    model.compile(optimizer='adam',
                  loss='mean_absolute_error',  
                  metrics=['mse'])        
    model.reset_states()
    model.fit(embeddings_train, train_labels, epochs=epochs)
    return model

# Explore Hyperparameters 
def explore_hyper_parameters():
  dropout_rates_list = []
  num_filters_list = []
  kernel_sizes_list = []
  results_list = []
  for i in range(10):
      print("Run: ", i)
      dropout_rate = random.uniform(0, 1)
      num = np.random.randint(3,5)
      num_filters = [np.random.randint(3,8)]*num
      kernel_sizes = []
      for i in range(num):
        kernel_sizes.append(np.random.randint(2,5))
      print(num, num_filters, kernel_sizes)
      model = train_cnn(bert_embeddings_hair,train_labels, dropout_rate, kernel_sizes, num_filters)
      results = model.evaluate(bert_embeddings_hair_test, test_labels)
      dropout_rates_list.append(dropout_rate)
      num_filters_list.append(num_filters)
      kernel_sizes_list.append(kernel_sizes)
      results_list.append(results)

      
  for i in range(30):
    print(dropout_rates_list[i],num_filters_list[i],kernel_sizes_list[i],results_list[i])

type(train_labels)

# ELMO
cnn = train_cnn(elmo_embeddings_lotion, np.array(train_labels))
results = cnn.evaluate(elmo_embeddings_lotion_test, test_labels)
print("Mean Absolute Error, Mean Squared Error: ", results)
"""
Mean Absolute Error, Mean Squared Error:  [1.4698753356933594, 9.138339042663574]
"""

# DistilBERT
cnn = train_cnn(distilbert_embeddings_lotion, train_labels)
results = cnn.evaluate(distilbert_embeddings_lotion_test, test_labels)
print("Mean Absolute Error, Mean Squared Error: ", results)
"""
Mean Absolute Error, Mean Squared Error:  [1.4674744606018066, 9.114583015441895]
"""

# RoBERTa
cnn = train_cnn(roberta_embeddings_lotion, train_labels)
results = cnn.evaluate(roberta_embeddings_lotion_test, test_labels)
print("Mean Absolute Error, Mean Squared Error: ", results)
"""
Mean Absolute Error, Mean Squared Error:  [1.471461534500122, 9.153165817260742]
"""

# BERT
cnn = train_cnn(bert_embeddings_lotion, train_labels)
results = cnn.evaluate(bert_embeddings_lotion_test, test_labels)
print("Mean Absolute Error, Mean Squared Error: ", results)
"""
Mean Absolute Error, Mean Squared Error:  [1.47007417678833, 9.10583782196045]
"""

"""# Hair Product Analysis

## Data Manipulation
"""

# Reduce dataset to only columns we're interested in. 
hair_df = hair_df[["product_id", "product_title_cl", 'title_cl', 'about_cl', 'description_cl', "price_cl", 'price_unit', 'ingredients', "category", "full_text", "title"]]
# Remove all products where the unit price is NAN
hair_df = hair_df[hair_df['price_unit'].notna()]
# Reset the index to ensure linearity. 
hair_df = hair_df.reset_index()

hair_df.shape

"""Remove Outliers: [Source](https://www.kite.com/python/answers/how-to-remove-outliers-from-a-pandas-dataframe-in-python)"""

# Remove Outliers 
z_scores = stats.zscore(hair_df["price_unit"].values)
abs_z_scores = np.abs(z_scores).reshape(-1,1)
filtered_entries = (abs_z_scores < 1.7).all(axis=1)
hair_df = hair_df[filtered_entries]

# Number of unique product_ids.  
len(hair_df.product_id.unique())

# Examine the sub-categories in the hair dataset. 
hair_df["category"].value_counts()

# Examine the distribution of price per unit (ounces).
plt.hist(hair_df['price_unit'], range=[0,40])
plt.show()

hair_df["price_unit"].describe()

# Value to split products into lower and higher priced hair products
cutoff_hair = hair_df["price_unit"].median()
cutoff_hair

"""## Freedman and Jurafsky Analysis (Scaled)

### Falutin

Flesch Reading Score and Grade Level
"""

# The flesch kincaid grade indicates the grade that a student would be able to read
# the text. For example, a 9.3 means a ninth grader could read the text. 
hair_df["flesch_kincaid_grade"] = hair_df["full_text"].apply(textstat.flesch_kincaid_grade)

# The flesch read ease score has no lower bound, but has an upper bound of 121.22. 
# The lower the score is, the harder the text is to read. 
hair_df["reading_ease"] = hair_df["full_text"].apply(textstat.flesch_reading_ease)

"""Number of Words"""

# Count the number of words present in the text (does not include punctuation by default).
hair_df["lexicon_count"] = hair_df["full_text"].apply(textstat.lexicon_count)

"""Word Commonality"""

# Raw count of marketing terms present. 
hair_df["marketing_term_count_raw"] = hair_df["full_text"].apply(count_marketing_terms)

# Examine falutin variables (as a whole).
hair_df[["flesch_kincaid_grade","reading_ease", "lexicon_count","marketing_term_count_raw" ]].describe()

# Split dataset based on the median of the price. 
lower_priced_products = hair_df[hair_df['price_unit'] <= cutoff_hair]
higher_priced_products = hair_df[hair_df['price_unit'] > cutoff_hair]

# Examine distribution of falutin variables for lower priced products. 
lower_priced_products[["flesch_kincaid_grade","reading_ease", "lexicon_count","marketing_term_count_raw" ]].describe()

# Examine distribution of falutin variabels for higher priced products. 
higher_priced_products[["flesch_kincaid_grade","reading_ease", "lexicon_count","marketing_term_count_raw" ]].describe()

"""### Distinction

#### Unique and Comparator Words: Baseline

As a baseline anlaysis, we hand-curated a list of words that indicate "uniqueness". This feature will just indicate the number of unique words present in the title.
"""

hair_df["unique_word_count_basic"] = hair_df["full_text"].apply(count_unique_and_comparator_words)
hair_df["unique_word_count_basic"].describe()

"""This method provided next to no signal.

#### Unique and Comparator Words: Improvement

We tried two different things to improve this signal. 

**1. Unigrams, Bigrams, Trigrams Using a Custom Word2Vec Model**

First, we extracted unigrams, bigrams and trigrams. We combined the bigrams and trigrams by underscores to create unique unigrams capturing the two or three words. Then we created our own embeddings using a custom Word2Vec model.
"""

# Extract unigrams, bigrams and trigrams. 
# Think we should remove prod from unigram/bigram/trigram.
hair_df["unigrams"] = hair_df["full_text"].apply(lambda x: np.array(str.split(x)))
hair_df["bigrams"] = hair_df["unigrams"].apply(nltk.bigrams).apply(list).apply(connect_by_underscore)
hair_df["trigrams"] = hair_df["unigrams"].apply(nltk.trigrams).apply(list).apply(connect_by_underscore)

# Examine Unigrams
hair_df["unigrams"][:5]

# Create a single vocabulary out of all unigrams, bigrams and trigrams.
vocab_unigrams_bigrams_trigrams = list(itertools.chain.from_iterable(list([hair_df["unigrams"].values,hair_df["bigrams"].values,hair_df["trigrams"].values])))
vocab_unigrams_bigrams_trigrams = list(itertools.chain.from_iterable(vocab_unigrams_bigrams_trigrams))
print(vocab_unigrams_bigrams_trigrams[-5:])

# Create word vectors for the vocabulary using Word2Vec.
custom_model = gensim.models.Word2Vec([vocab_unigrams_bigrams_trigrams])
custom_model.save('custom.embedding')
custom_model = gensim.models.Word2Vec.load('custom.embedding')

# Find the list and count of marketing terms in the product text that has 
# a high cosine similarity to the unique/custom word list. 
counts_custom, other_unique_and_comparator_words_custom = extract_similar_words(unique_and_comparator_words_cl, custom_model)
plt.hist(counts_custom.values())
plt.show()

# Merge the counts back into the main df using a join on product_id.
hair_df = hair_df.merge(pd.DataFrame(list(counts_custom.items()),columns = ['product_id','mktg_embedding_counts_custom']), how="inner")

# Examine the distribution of the number of marketing terms present 
# (based on cosine similarity).
hair_df['mktg_embedding_counts_custom'].describe()

# Examine words in the text that had a high cosine similarity to the unique/comparator word list.
list(other_unique_and_comparator_words_custom)[:min(len(other_unique_and_comparator_words_custom), 25)]

# Split products based on the median price to analyze any significant differences 
# in the distribution of the number marketing terms present. 
lower_priced_products = hair_df[hair_df['price_unit'] <= cutoff_hair]
higher_priced_products = hair_df[hair_df['price_unit'] > cutoff_hair]

# Examine distribution of the number of marketing terms in the text for lower 
# priced products.
lower_priced_products["mktg_embedding_counts_custom"].describe()

plt.hist(lower_priced_products["mktg_embedding_counts_custom"])
plt.show()

# Examine distribution of the number of marketing terms in the text for higher 
# priced products. 
higher_priced_products["mktg_embedding_counts_custom"].describe()

plt.hist(higher_priced_products["mktg_embedding_counts_custom"])
plt.show()

"""**2. Pre-trained Glove Embeddings on Unigrams**

The next thing we tried to do to improve on the baseline was to use only unigrams and pre-trained word vectors.
"""

# Only use unigrams, since bigrams and trigrams joined on '_' won't be in vocab. 
vocab_unigrams = list(itertools.chain.from_iterable(hair_df["unigrams"]))
print(vocab_unigrams[:10])

# Extract words and the count of those words in each text using cosine similarity to
# the hand curated unique/comparator word list. 
counts_glove, other_unique_and_comparator_words_glove = extract_similar_words(unique_and_comparator_words_cl, glove_vectors)

# Merge the counts back into the main df using a join on product_id.
hair_df = hair_df.merge(pd.DataFrame(list(counts_glove.items()),columns = ['product_id','mktg_embedding_counts_glov']), how="inner")

# Examine the distribution of the number of marketing terms present 
# (based on cosine similarity).
hair_df['mktg_embedding_counts_glov'].describe()

# Examine words in the text that had a high cosine similarity to the unique/comparator word list.
list(other_unique_and_comparator_words_glove)[:min(len(other_unique_and_comparator_words_glove), 25)]

# Split products based on the median price to analyze any significant differences 
# in the distribution of the number marketing terms present. 
lower_priced_products = hair_df[hair_df['price_unit'] <= cutoff_hair]
higher_priced_products = hair_df[hair_df['price_unit'] > cutoff_hair]

# Examine distribution of the number of marketing terms in the text for lower 
# priced products.
lower_priced_products["mktg_embedding_counts_glov"].describe()

plt.hist(lower_priced_products["mktg_embedding_counts_glov"])
plt.show()

# Examine distribution of the number of marketing terms in the text for higher 
# priced products. 
higher_priced_products["mktg_embedding_counts_glov"].describe()

plt.hist(higher_priced_products["mktg_embedding_counts_glov"])
plt.show()

"""#### Sentiment Analysis"""

# Extract sentiment scores for each text using nltk SentimentIntensityAnalyzer.
sentiment_analyzer = SentimentIntensityAnalyzer()
sentiment_df = hair_df["full_text"].apply(sentiment_analyzer.polarity_scores).to_frame()
sentiment_df  = sentiment_df["full_text"].apply(pd.Series)
# Add results to the main dataframe. 
hair_df["pos"] = sentiment_df["pos"]
hair_df["neg"] = sentiment_df["neg"]
hair_df["neu"] = sentiment_df["neu"]
hair_df["compound"] = sentiment_df["compound"]
# Examine values
hair_df[["pos","neg","neu","compound"]].head()

# Split products based on price. 
lower_priced_products = hair_df[hair_df["price_unit"] <= cutoff_hair] 
higher_priced_products = hair_df[hair_df["price_unit"] > cutoff_hair]

# Examine sentiment scores for the lower priced products.
lower_priced_products[["pos","neg","neu","compound"]].describe()

# Examine sentiment scores for the higher priced products. 
higher_priced_products[["pos","neg","neu","compound"]].describe()

"""### Authenticity

#### Term Frequency - Inverse Document Frequency  
TFIDF gives a score for each token/word in 1 document of documents cominbed into one. For authenticy, we want the words and its tfidf over the whole lower price "document" and higher price "document" (lower_priced_products, higher_priced_products)
"""

#  Create TFIDF data for lower priced hairs: titles, about, and description
lower_title_tf = tfidf(lower_priced_products.title_cl, col_name='l_title_tfidf')
lower_about_tf = tfidf(lower_priced_products.about_cl, col_name='l_about_tfidf')
lower_desc_tf = tfidf(lower_priced_products.description_cl, col_name='l_desc_tfidf')

#  Create TFIDF data for higher priced hairs: titles, about, and description
higher_title_tf = tfidf(higher_priced_products.title_cl, col_name='h_title_tfidf')
higher_about_tf = tfidf(higher_priced_products.about_cl, col_name='h_about_tfidf')
higher_desc_tf = tfidf(higher_priced_products.description_cl, col_name='h_desc_tfidf')

# Create TFIDF dataframe for lower priced products
lower_tf = lower_title_tf.join(lower_about_tf, how='outer')
lower_tf = lower_tf.join(lower_desc_tf, how='outer')
lower_tf['word'] = lower_tf.index
lower_tf.sort_values(by=['l_desc_tfidf'], ascending=False).head(30)

# Create TFIDF dataframe for higher priced products
higher_tf = higher_title_tf.join(higher_about_tf, how='outer')
higher_tf = higher_tf.join(higher_desc_tf, how='outer')
higher_tf['word'] = higher_tf.index
higher_tf.sort_values(by=['h_desc_tfidf'], ascending=False).head(30)

# Create joined TFIDF dataframe
tfidf_joined = lower_tf.merge(higher_tf, how='outer', on ='word')
tfidf_joined = tfidf_joined[['word', 'l_title_tfidf', 'h_title_tfidf',	'l_about_tfidf', 'h_about_tfidf', 'l_desc_tfidf', 'h_desc_tfidf']]
tfidf_joined.sort_values(by='l_title_tfidf', ascending=False) # sort to see, change by accordingly

"""### Health


*   Ingredients from Amazon Products 
*   Toxicity Levels from California Chemicals in Cosmetics Database 


"""

# Check if ingredient exists in our data
hair_df[hair_df['ingredients'].str.contains('retinol', na=False)][["product_title_cl","ingredients"]]

hair_df.ingredients.unique()[0:3]

# Retrieve count of toxic ingredients.
hair_df['toxic_count'] = hair_df['ingredients'].apply(lambda x: check_toxic(x) if pd.notnull(x) else None)

# Retrieve list of toxic ingredients. 
hair_df['toxic_ing'] = hair_df['ingredients'].apply(lambda x: check_toxic_ingredient(x) if pd.notnull(x) else None)

# Split products based on price. 
lower_priced_products = hair_df[hair_df["price_unit"] <= cutoff_hair] 
higher_priced_products = hair_df[hair_df["price_unit"] > cutoff_hair]

# Examine distribution of toxic ingredients for lower priced products. 
lower_priced_products["toxic_count"].describe()

# Examine distribution of toxic ingredients for higher  priced products.
higher_priced_products["toxic_count"].describe()

"""## Modeling"""

# Truncate all text to a max length of 100
hair_df["full_text_trunc"] = hair_df["full_text"].apply(truncate_or_fill)

"""#### Neural Network (Definition) 

Used for the baseline and the improvement.[link text](https://)

### Train Test Split
"""

# Split data into train and test.
random_df = pd.DataFrame(np.random.randn(hair_df["full_text_trunc"].shape[0]))
mask = np.random.rand(len(random_df)) < 0.8
train_features = hair_df[mask]
train_labels = hair_df["price_unit"][mask].fillna(0)
test_features = hair_df[~mask]
test_labels = hair_df["price_unit"][~mask].fillna(0)

print(train_features.shape)
print(train_labels.shape)
print(test_features.shape)
print(test_labels.shape)

"""### Baseline
Run a Neural Network with features computed during the Freedman and Jurafsky Analysis
"""

# Scale a dataframe column
def scale_data(data):
  min_max_scaler = preprocessing.StandardScaler()
  data_scaled = min_max_scaler.fit_transform(data)
  return pd.DataFrame(data_scaled)

hair_df["marketing_term_count_scaled"] = scale_data((hair_df["marketing_term_count_raw"]+1).values.reshape(-1,1))
hair_df["flesch_kincaid_grade_scaled"] = scale_data((hair_df["flesch_kincaid_grade"]).values.reshape(-1,1))
hair_df["reading_ease_scaled"] = scale_data((hair_df["reading_ease"]).values.reshape(-1,1))
hair_df["lexicon_count_scaled"] = scale_data((hair_df["lexicon_count"]).values.reshape(-1,1))
hair_df["mktg_embedding_counts_glov_scaled"] = scale_data((hair_df["mktg_embedding_counts_glov"]).values.reshape(-1,1))
hair_df["compound_scaled"] = scale_data((hair_df["compound"]).values.reshape(-1,1))
hair_df["toxic_count_scaled"] = scale_data((hair_df["toxic_count"]).values.reshape(-1,1))

"""Experiments"""

# Scaled Data
# Mean Absolute Error, Mean Squared Error  [1.1124241352081299, 4.136312961578369]
train = hair_df[["marketing_term_count_scaled" , "flesch_kincaid_grade_scaled" , "reading_ease_scaled" , "lexicon_count_scaled" , "mktg_embedding_counts_glov_scaled" , "compound_scaled","toxic_count_scaled"]][mask]
test = hair_df[["marketing_term_count_scaled" , "flesch_kincaid_grade_scaled" , "reading_ease_scaled" , "lexicon_count_scaled" , "mktg_embedding_counts_glov_scaled" , "compound_scaled","toxic_count_scaled"]][~mask]

# Results of using Unscaled Features. 
# Mean Absolute Error, Mean Squared Error  [1.065545678138733, 4.275307655334473]
train = hair_df[["marketing_term_count_raw" , "flesch_kincaid_grade" , "reading_ease" , "lexicon_count" , "mktg_embedding_counts_glov" , "compound","toxic_count"]][mask]
test = hair_df[["marketing_term_count_raw" , "flesch_kincaid_grade" , "reading_ease" , "lexicon_count" , "mktg_embedding_counts_glov" , "compound","toxic_count"]][~mask]

# Scaled and neu instead of compount
# Mean Absolute Error, Mean Squared Error  [1.0878902673721313, 4.176847457885742]
train = hair_df[["marketing_term_count_scaled" ,"flesch_kincaid_grade_scaled" , "reading_ease_scaled" , "lexicon_count_scaled" , "mktg_embedding_counts_glov_scaled" , "neu","toxic_count_scaled"]][mask]
test = hair_df[["marketing_term_count_scaled" ,"flesch_kincaid_grade_scaled" , "reading_ease_scaled" , "lexicon_count_scaled" , "mktg_embedding_counts_glov_scaled" , "neu","toxic_count_scaled"]][~mask]

run_model((train.shape[1],), train, train_labels, test, test_labels, 200)

"""### Improvement 
Use Embeddings and pass them through a Neural Net as well as a Convolutional Neural Net.

#### ELMO - Contextualized Word Embeddings
"""

elmo_embeddings_train_path_hair = '/content/gdrive/MyDrive/266/final/data/elmo_embeddings_train_hair.npy'
  elmo_embedding_test_path_hair = '/content/gdrive/MyDrive/266/final/data/elmo_embeddings_test_hair.npy'

create_and_save_elmo_embeddings(elmo_embeddings_train_path_hair,elmo_embedding_test_path_hair)

# Load pre-saved elmo word embeddings 
elmo_embeddings_hair = np.load(elmo_embeddings_train_path_hair)
elmo_embeddings_hair_test = np.load(elmo_embedding_test_path_hair)

print("elmo_embeddings_hair shape: ", elmo_embeddings_hair.shape)
print("elmo_embeddings_hair_test shape: ", elmo_embeddings_hair_test.shape)

"""#### BERT (Variations) Embeddings
Semi-Adapted from [Source](https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)

**DistilBERT**
"""

distilbert_embeddings_train_path_hair = "/content/gdrive/MyDrive/266/final/data/distilbert_embeddings_train_hair.npy"
distilbert_embeddings_test_path_hair =  "/content/gdrive/MyDrive/266/final/data/distilbert_embeddings_test_hair.npy"

train_max_len = create_and_save_bert_embeddings(train_features, distilbert_embeddings_train_path_hair  , bert_type="distilbert")
create_and_save_bert_embeddings(test_features, distilbert_embeddings_test_path_hair, bert_type="distilbert",max_len=train_max_len)

# Load pre-saved DistilBERT word embeddings 
distilbert_embeddings_hair = np.load(distilbert_embeddings_train_path_hair)
distilbert_embeddings_hair_test = np.load(distilbert_embeddings_test_path_hair)

print("distilbert_embeddings_hair shape: ", distilbert_embeddings_hair.shape)
print("distilbert_embeddings_hair_test shape: ", distilbert_embeddings_hair_test.shape)

""" **RoBERTa**"""

roberta_embeddings_train_path_hair = "/content/gdrive/MyDrive/266/final/data/roberta_embeddings_train_hair.npy"
roberta_embeddings_test_path_hair =  "/content/gdrive/MyDrive/266/final/data/roberta_embeddings_test_hair.npy"

train_max_len = create_and_save_bert_embeddings(train_features, roberta_embeddings_train_path_hair  , bert_type="roberta")
create_and_save_bert_embeddings(test_features, roberta_embeddings_test_path_hair, bert_type="roberta",max_len=train_max_len)

# Load pre-saved RoBERTa word embeddings 
roberta_embeddings_hair = np.load(roberta_embeddings_train_path_hair)
roberta_embeddings_hair_test = np.load(roberta_embeddings_test_path_hair)

print("roberta_embeddings_hair shape: ", roberta_embeddings_hair.shape)
print("roberta_embeddings_hair_test shape: ", roberta_embeddings_hair_test.shape)

"""**BERT (original)**"""

bert_embeddings_train_path_hair = "/content/gdrive/MyDrive/266/final/data/bert_embeddings_train_hair.npy"
bert_embeddings_test_path_hair =  "/content/gdrive/MyDrive/266/final/data/bert_embeddings_test_hair.npy"

train_max_len = create_and_save_bert_embeddings(train_features, bert_embeddings_train_path_hair  , bert_type="bert")
create_and_save_bert_embeddings(test_features, bert_embeddings_test_path_hair, bert_type="bert",max_len=train_max_len)

# Load pre-saved RoBERTa word embeddings 
bert_embeddings_hair = np.load(bert_embeddings_train_path_hair)
bert_embeddings_hair_test = np.load(bert_embeddings_test_path_hair)

print("bert_embeddings shape: ", bert_embeddings_hair.shape)
print("bert_embeddings_test shape: ", bert_embeddings_hair_test.shape)

"""### Neural Network - Run for Improvement"""

# ELMO
run_model((1024,), elmo_embeddings_hair, train_labels, elmo_embeddings_hair_test, test_labels, 200)
"""
Mean Absolute Error, Mean Squared Error  [1.1777405738830566, 4.052495002746582]
"""

# DistilBERT
run_model((768,), distilbert_embeddings_hair, train_labels, distilbert_embeddings_hair_test, test_labels, 200)
"""
Mean Absolute Error, Mean Squared Error  [1.0419797897338867, 3.6801388263702393]
"""

# RoBERTa
run_model((768,), roberta_embeddings_hair, train_labels, roberta_embeddings_hair_test, test_labels, 200)
"""
Mean Absolute Error, Mean Squared Error  [1.0948551893234253, 3.6444075107574463]
"""

# BERT
run_model((768,), bert_embeddings_hair, train_labels, bert_embeddings_hair_test, test_labels, 200)
"""
Mean Absolute Error, Mean Squared Error  [1.102107286453247, 3.9629263877868652]
"""

"""### Convolutional Neural Network"""

# ELMO
cnn = train_cnn(elmo_embeddings_hair, train_labels)
results = cnn.evaluate(elmo_embeddings_hair_test, test_labels)
print("Mean Absolute Error, Mean Squared Error: ", results)
"""
Mean Absolute Error, Mean Squared Error:  [1.0965903997421265, 4.228530406951904]
"""

# DistilBERT
cnn = train_cnn(distilbert_embeddings_hair, train_labels)
results = cnn.evaluate(distilbert_embeddings_hair_test, test_labels)
print("Mean Absolute Error, Mean Squared Error: ", results)
"""
Mean Absolute Error, Mean Squared Error:  [1.1034969091415405, 4.20483922958374]
"""

# RoBERTa
cnn = train_cnn(roberta_embeddings_hair, train_labels)
results = cnn.evaluate(roberta_embeddings_hair_test, test_labels)
print("Mean Absolute Error, Mean Squared Error: ", results)
"""
Mean Absolute Error, Mean Squared Error:  [1.096874475479126, 4.2369842529296875]
"""

# BERT
cnn = train_cnn(bert_embeddings_hair, train_labels)
results = cnn.evaluate(bert_embeddings_hair_test, test_labels)
print("Mean Absolute Error, Mean Squared Error: ", results)
"""
Mean Absolute Error, Mean Squared Error:  [1.0969746112823486, 4.288430213928223]
"""

